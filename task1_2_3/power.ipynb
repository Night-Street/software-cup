{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SeyoFjbwUYS5","executionInfo":{"status":"ok","timestamp":1657193734903,"user_tz":-480,"elapsed":17243,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"e9805127-0b61-4051-f754-cc52ab5f1eb8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/Othercomputers/陈成的thinkpad/software-cup\")\n","!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3D8C5HUzUWGg","executionInfo":{"status":"ok","timestamp":1657193932804,"user_tz":-480,"elapsed":128735,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"2edff693-58d8-467b-b802-57c8bfb71fa5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting catboost==1.0.6\n","  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n","\u001b[K     |████████████████████████████████| 76.6 MB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied: lightgbm==2.2.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.2.3)\n","Collecting matplotlib==2.2.3\n","  Downloading matplotlib-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (12.6 MB)\n","\u001b[K     |████████████████████████████████| 12.6 MB 19.0 MB/s \n","\u001b[?25hRequirement already satisfied: mlxtend==0.14.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.14.0)\n","Collecting numpy==1.20.3\n","  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n","\u001b[K     |████████████████████████████████| 15.3 MB 415 kB/s \n","\u001b[?25hCollecting paddlepaddle_gpu==2.0.2\n","  Downloading paddlepaddle_gpu-2.0.2-cp37-cp37m-manylinux1_x86_64.whl (711.8 MB)\n","\u001b[K     |████████████████████████████████| 711.8 MB 5.4 kB/s \n","\u001b[?25hCollecting pandas==1.1.5\n","  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n","\u001b[K     |████████████████████████████████| 9.5 MB 46.4 MB/s \n","\u001b[?25hCollecting scikit_learn==0.20.0\n","  Downloading scikit_learn-0.20.0-cp37-cp37m-manylinux1_x86_64.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 49.4 MB/s \n","\u001b[?25hCollecting seaborn==0.10.0\n","  Downloading seaborn-0.10.0-py3-none-any.whl (215 kB)\n","\u001b[K     |████████████████████████████████| 215 kB 56.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm==4.64.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (4.64.0)\n","Requirement already satisfied: xgboost==0.90 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.90)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.6->-r requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.6->-r requirements.txt (line 1)) (0.10.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.6->-r requirements.txt (line 1)) (5.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost==1.0.6->-r requirements.txt (line 1)) (1.4.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.3->-r requirements.txt (line 3)) (2022.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.3->-r requirements.txt (line 3)) (1.4.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.3->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.3->-r requirements.txt (line 3)) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.3->-r requirements.txt (line 3)) (3.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend==0.14.0->-r requirements.txt (line 4)) (57.4.0)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (2.23.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (7.1.2)\n","Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (3.17.3)\n","Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (0.8.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (4.4.2)\n","Requirement already satisfied: gast>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (0.5.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==2.2.3->-r requirements.txt (line 3)) (4.1.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->paddlepaddle_gpu==2.0.2->-r requirements.txt (line 6)) (2022.6.15)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost==1.0.6->-r requirements.txt (line 1)) (8.0.1)\n","Installing collected packages: numpy, scikit-learn, pandas, matplotlib, seaborn, paddlepaddle-gpu, catboost\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.2\n","    Uninstalling scikit-learn-1.0.2:\n","      Successfully uninstalled scikit-learn-1.0.2\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.3.5\n","    Uninstalling pandas-1.3.5:\n","      Successfully uninstalled pandas-1.3.5\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","  Attempting uninstall: seaborn\n","    Found existing installation: seaborn 0.11.2\n","    Uninstalling seaborn-0.11.2:\n","      Successfully uninstalled seaborn-0.11.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.20.0 which is incompatible.\n","xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.20.3 which is incompatible.\n","plotnine 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 2.2.3 which is incompatible.\n","mizani 0.6.0 requires matplotlib>=3.1.1, but you have matplotlib 2.2.3 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.20.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","arviz 0.12.1 requires matplotlib>=3.0, but you have matplotlib 2.2.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed catboost-1.0.6 matplotlib-2.2.3 numpy-1.20.3 paddlepaddle-gpu-2.0.2 pandas-1.1.5 scikit-learn-0.20.0 seaborn-0.10.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits","numpy"]}}},"metadata":{}}]},{"cell_type":"code","source":["os.chdir(\"/content/drive/Othercomputers/陈成的thinkpad/software-cup/task1_2_3\")"],"metadata":{"id":"r_ezgt4jVJc3","executionInfo":{"status":"ok","timestamp":1657193932805,"user_tz":-480,"elapsed":16,"user":{"displayName":"陈成","userId":"07346209167866063288"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  from ._gradient_boosting import predict_stages\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  from ._gradient_boosting import predict_stages\n"]}],"source":["import gc\n","\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","from catboost import CatBoostClassifier\n","from lightgbm import LGBMClassifier\n","from mlxtend.classifier import EnsembleVoteClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from tqdm import tqdm\n","from xgboost import XGBClassifier"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"D6xFnFOX-PCw","executionInfo":{"status":"ok","timestamp":1657193934386,"user_tz":-480,"elapsed":1595,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"831d7369-938c-47cb-8c7f-868efbc4c05c"}},{"cell_type":"code","execution_count":5,"outputs":[],"source":["def saved(df, file):\n","    df.to_csv(file, header=False, index=False)\n","    print('saved')"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"M08SFEqx-PCw","executionInfo":{"status":"ok","timestamp":1657193934387,"user_tz":-480,"elapsed":8,"user":{"displayName":"陈成","userId":"07346209167866063288"}}}},{"cell_type":"markdown","source":["# 任务一，求每个用户的年平均缴费次数和平均金额"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"SGK8P95f-PCx"}},{"cell_type":"code","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额\n","0   1000000001      4.0    493.5  246.750000\n","1   1000000002      3.5    245.0  140.000000\n","2   1000000003      3.5    590.0  337.142857\n","3   1000000004      4.0    310.5  155.250000\n","4   1000000005      3.5    750.0  428.571429\n","..         ...      ...      ...         ...\n","95  1000000096      3.5    458.0  261.714286\n","96  1000000097      3.0    324.5  216.333333\n","97  1000000098      3.0    367.5  245.000000\n","98  1000000099      3.0    426.5  284.333333\n","99  1000000100      3.5    471.5  269.428571\n","\n","[100 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-6752bde5-5de9-4b58-98ad-a17244767271\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>用户编码</th>\n","      <th>年平均缴费次数</th>\n","      <th>年平均缴费金额</th>\n","      <th>平均每次缴费金额</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000000001</td>\n","      <td>4.0</td>\n","      <td>493.5</td>\n","      <td>246.750000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1000000002</td>\n","      <td>3.5</td>\n","      <td>245.0</td>\n","      <td>140.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1000000003</td>\n","      <td>3.5</td>\n","      <td>590.0</td>\n","      <td>337.142857</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000000004</td>\n","      <td>4.0</td>\n","      <td>310.5</td>\n","      <td>155.250000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000000005</td>\n","      <td>3.5</td>\n","      <td>750.0</td>\n","      <td>428.571429</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>1000000096</td>\n","      <td>3.5</td>\n","      <td>458.0</td>\n","      <td>261.714286</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>1000000097</td>\n","      <td>3.0</td>\n","      <td>324.5</td>\n","      <td>216.333333</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>1000000098</td>\n","      <td>3.0</td>\n","      <td>367.5</td>\n","      <td>245.000000</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>1000000099</td>\n","      <td>3.0</td>\n","      <td>426.5</td>\n","      <td>284.333333</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>1000000100</td>\n","      <td>3.5</td>\n","      <td>471.5</td>\n","      <td>269.428571</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6752bde5-5de9-4b58-98ad-a17244767271')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6752bde5-5de9-4b58-98ad-a17244767271 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6752bde5-5de9-4b58-98ad-a17244767271');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["power = pd.read_excel(\"../dataset/cph.xlsx\")\n","power[\"缴费日期\"] = pd.to_datetime(power[\"缴费日期\"])\n","\n","def getYear(df, date):\n","    year = []\n","    for i in df[date]:\n","        year.append(i.year)\n","    return year\n","\n","\n","power[\"year\"] = getYear(power, \"缴费日期\")\n","power\n","power_user = power.groupby(\"用户编号\")\n","user_list = list(power_user)\n","user_id = []\n","user_chargeDate_mean = []\n","user_chargeMoney_mean = []\n","user_charge_Money_Date = []\n","for i in user_list:\n","    user = pd.DataFrame(i[1])\n","    id = user.用户编号.unique()[0]\n","    len = user[\"year\"].nunique()\n","    charge_Date_mean = user.缴费日期.nunique() / len\n","    charge_Moeny_mean = user[\"缴费金额（元）\"].sum() / len\n","    charge_Money_Date = user[\"缴费金额（元）\"].sum() / charge_Date_mean\n","    user_id.append(id)\n","    user_chargeDate_mean.append(charge_Date_mean)\n","    user_chargeMoney_mean.append(charge_Moeny_mean)\n","    user_charge_Money_Date.append(charge_Money_Date)\n","\n","power_mean = pd.DataFrame()\n","power_mean[\"用户编码\"] = user_id\n","power_mean[\"年平均缴费次数\"] = user_chargeDate_mean\n","power_mean[\"年平均缴费金额\"] = user_chargeMoney_mean\n","power_mean[\"平均每次缴费金额\"] = user_charge_Money_Date\n","#平均每次缴费金额\n","# saved(power_mean,'Task1.csv')\n","power_mean"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"k3NrKA2w-PCx","executionInfo":{"status":"ok","timestamp":1657193934893,"user_tz":-480,"elapsed":513,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"b77215ea-54a4-4d4a-93af-817969a1fedc"}},{"cell_type":"code","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.5"]},"metadata":{},"execution_count":7}],"source":["charge_Date_mean"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"h3K0Y-jS-PCx","executionInfo":{"status":"ok","timestamp":1657193935293,"user_tz":-480,"elapsed":402,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"0bab2ed6-0eac-4cef-a203-7c75b99223d6"}},{"cell_type":"markdown","source":["# 任务二 客户分类"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"j6Sd_dcd-PCx"}},{"cell_type":"code","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["高价值型客户：\n","          用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额\n","0   1000000001      4.0    493.5  246.750000\n","1   1000000003      3.5    590.0  337.142857\n","2   1000000005      3.5    750.0  428.571429\n","3   1000000006      3.5    505.0  288.571429\n","4   1000000007      3.5    553.0  316.000000\n","5   1000000008      3.5    537.0  306.857143\n","6   1000000009      3.5    485.0  277.142857\n","7   1000000010      4.0    386.5  193.250000\n","8   1000000011      3.5    374.0  213.714286\n","9   1000000012      3.5    455.5  260.285714\n","10  1000000013      3.5    417.5  238.571429\n","11  1000000014      3.5    374.5  214.000000\n","12  1000000016      4.0    392.0  196.000000\n","13  1000000017      3.5    390.0  222.857143\n","14  1000000020      3.5    449.5  256.857143\n","15  1000000021      3.5    378.5  216.285714\n","16  1000000024      3.5    452.0  258.285714\n","17  1000000026      3.5    388.5  222.000000\n","18  1000000034      3.5    360.5  206.000000\n","19  1000000035      3.5    384.5  219.714286\n","20  1000000039      3.5    375.0  214.285714\n","21  1000000052      3.5    383.0  218.857143\n","22  1000000055      3.5    394.0  225.142857\n","23  1000000059      4.0    370.0  185.000000\n","24  1000000060      3.5    365.0  208.571429\n","25  1000000062      3.5    386.5  220.857143\n","26  1000000065      3.5    367.0  209.714286\n","27  1000000071      3.5    371.0  212.000000\n","28  1000000072      3.5    399.0  228.000000\n","29  1000000076      3.5    362.5  207.142857\n","30  1000000077      3.5    360.0  205.714286\n","31  1000000083      3.5    374.0  213.714286\n","32  1000000084      3.5    644.5  368.285714\n","33  1000000089      5.0    388.0   77.600000\n","34  1000000091      3.5    380.5  217.428571\n","35  1000000093      5.0    463.0   92.600000\n","36  1000000095      3.5    499.5  285.428571\n","37  1000000096      3.5    458.0  261.714286\n","38  1000000100      3.5    471.5  269.428571；\n","低价值型客户：\n","          用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额\n","0   1000000022      3.0    267.0  178.000000\n","1   1000000023      2.5    246.5  197.200000\n","2   1000000028      3.0    285.5  190.333333\n","3   1000000032      3.0    259.5  173.000000\n","4   1000000033      3.0    272.5  181.666667\n","5   1000000036      3.0    322.5  215.000000\n","6   1000000038      3.0    267.0  178.000000\n","7   1000000041      3.0    341.5  227.666667\n","8   1000000044      3.0    300.5  200.333333\n","9   1000000046      3.0    355.5  237.000000\n","10  1000000048      3.0    339.0  226.000000\n","11  1000000051      3.0    298.5  199.000000\n","12  1000000054      3.0    229.5  153.000000\n","13  1000000064      3.0    260.0  173.333333\n","14  1000000066      3.0    349.5  233.000000\n","15  1000000069      3.0    274.0  182.666667\n","16  1000000073      2.0    202.0  202.000000\n","17  1000000078      2.5    296.0  236.800000\n","18  1000000079      3.0    298.5  199.000000\n","19  1000000082      2.5    254.0  203.200000\n","20  1000000090      3.0    306.0  204.000000\n","21  1000000092      3.0    254.0   84.666667\n","22  1000000094      3.0    313.0  208.666667\n","23  1000000097      3.0    324.5  216.333333；\n","大众型客户：\n","          用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额\n","0   1000000002      3.5    245.0  140.000000\n","1   1000000004      4.0    310.5  155.250000\n","2   1000000015      3.5    350.0  200.000000\n","3   1000000018      3.5    320.5  183.142857\n","4   1000000025      3.5    325.5  186.000000\n","5   1000000027      3.5    315.5  180.285714\n","6   1000000029      3.5    345.5  197.428571\n","7   1000000030      3.5    335.5  191.714286\n","8   1000000031      3.5    327.5  187.142857\n","9   1000000037      3.5    313.5  179.142857\n","10  1000000040      3.5    328.0  187.428571\n","11  1000000042      3.5    306.0  174.857143\n","12  1000000043      3.5    320.0  182.857143\n","13  1000000045      3.5    355.5  203.142857\n","14  1000000047      3.5    327.0  186.857143\n","15  1000000049      3.5    294.5  168.285714\n","16  1000000050      3.5    284.0  162.285714\n","17  1000000053      3.5    347.0  198.285714\n","18  1000000056      3.5    357.0  204.000000\n","19  1000000057      3.5    355.5  203.142857\n","20  1000000058      3.5    342.5  195.714286\n","21  1000000061      3.5    341.5  195.142857\n","22  1000000063      3.5    314.0  179.428571\n","23  1000000067      3.5    344.5  196.857143\n","24  1000000068      3.5    355.5  203.142857\n","25  1000000070      4.0    333.5  166.750000\n","26  1000000074      3.5    318.0  181.714286\n","27  1000000075      3.5    314.5  179.714286\n","28  1000000080      3.5    301.5  172.285714\n","29  1000000081      3.5    340.0  194.285714\n","30  1000000085      3.5    284.5  162.571429\n","31  1000000086      4.0    325.0   81.250000\n","32  1000000087      3.5    325.5  186.000000\n","33  1000000088      3.5    269.0  153.714286；\n","潜力型客户：\n","         用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额\n","0  1000000019      3.0    364.5  243.000000\n","1  1000000098      3.0    367.5  245.000000\n","2  1000000099      3.0    426.5  284.333333\n","saved\n","saved\n","saved\n","saved\n"]},{"output_type":"execute_result","data":{"text/plain":["          用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额 是否为高价值\n","0   1000000001      4.0    493.5  246.750000      1\n","1   1000000002      3.5    245.0  140.000000      0\n","2   1000000003      3.5    590.0  337.142857      1\n","3   1000000004      4.0    310.5  155.250000      0\n","4   1000000005      3.5    750.0  428.571429      1\n","..         ...      ...      ...         ...    ...\n","95  1000000096      3.5    458.0  261.714286      1\n","96  1000000097      3.0    324.5  216.333333      0\n","97  1000000098      3.0    367.5  245.000000      0\n","98  1000000099      3.0    426.5  284.333333      0\n","99  1000000100      3.5    471.5  269.428571      1\n","\n","[100 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-7c97374d-f467-45b3-84b9-c58314c438bc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>用户编码</th>\n","      <th>年平均缴费次数</th>\n","      <th>年平均缴费金额</th>\n","      <th>平均每次缴费金额</th>\n","      <th>是否为高价值</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000000001</td>\n","      <td>4.0</td>\n","      <td>493.5</td>\n","      <td>246.750000</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1000000002</td>\n","      <td>3.5</td>\n","      <td>245.0</td>\n","      <td>140.000000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1000000003</td>\n","      <td>3.5</td>\n","      <td>590.0</td>\n","      <td>337.142857</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000000004</td>\n","      <td>4.0</td>\n","      <td>310.5</td>\n","      <td>155.250000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000000005</td>\n","      <td>3.5</td>\n","      <td>750.0</td>\n","      <td>428.571429</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>1000000096</td>\n","      <td>3.5</td>\n","      <td>458.0</td>\n","      <td>261.714286</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>1000000097</td>\n","      <td>3.0</td>\n","      <td>324.5</td>\n","      <td>216.333333</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>1000000098</td>\n","      <td>3.0</td>\n","      <td>367.5</td>\n","      <td>245.000000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>1000000099</td>\n","      <td>3.0</td>\n","      <td>426.5</td>\n","      <td>284.333333</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>1000000100</td>\n","      <td>3.5</td>\n","      <td>471.5</td>\n","      <td>269.428571</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c97374d-f467-45b3-84b9-c58314c438bc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7c97374d-f467-45b3-84b9-c58314c438bc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7c97374d-f467-45b3-84b9-c58314c438bc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["count = power_mean.年平均缴费次数.mean()\n","money = power_mean.年平均缴费金额.mean()\n","\n","high_value = pd.DataFrame(columns=['用户编码', '年平均缴费次数', '年平均缴费金额', '平均每次缴费金额'])\n","low_value = pd.DataFrame(columns=['用户编码', '年平均缴费次数', '年平均缴费金额', '平均每次缴费金额'])\n","normal = pd.DataFrame(columns=['用户编码', '年平均缴费次数', '年平均缴费金额', '平均每次缴费金额'])\n","potency = pd.DataFrame(columns=['用户编码', '年平均缴费次数', '年平均缴费金额', '平均每次缴费金额'])\n","\n","client = pd.DataFrame(columns=['用户编码', '年平均缴费次数', '年平均缴费金额', '平均每次缴费金额', '是否为高价值'])\n","\n","for row in power_mean.iterrows():\n","    count_mean = row[1][\"年平均缴费次数\"]\n","    money_mean = row[1][\"年平均缴费金额\"]\n","    money_count = row[1][\"平均每次缴费金额\"]\n","    id = str(int(row[1][\"用户编码\"]))\n","\n","    if (count_mean > count and money_mean > money):\n","        high_value = high_value.append(\n","            {'用户编码': id, '年平均缴费次数': count_mean, '年平均缴费金额': money_mean, '平均每次缴费金额': money_count}, ignore_index=True)\n","\n","        client = client.append(\n","            {'用户编码': id, '年平均缴费次数': count_mean, '年平均缴费金额': money_mean, '平均每次缴费金额': money_count, '是否为高价值': 1},\n","            ignore_index=True)\n","    elif (count_mean <= count and money_mean <= money):\n","        low_value = low_value.append(\n","            {'用户编码': id, '年平均缴费次数': count_mean, '年平均缴费金额': money_mean, '平均每次缴费金额': money_count}, ignore_index=True)\n","\n","        client = client.append(\n","            {'用户编码': id, '年平均缴费次数': count_mean, '年平均缴费金额': money_mean, '平均每次缴费金额': money_count, '是否为高价值': 0},\n","            ignore_index=True)\n","    elif (count_mean > count and money_mean <= money):\n","        normal = normal.append({'用户编码': id, '年平均缴费次数': count_mean, '年平均缴费金额': money_mean, '平均每次缴费金额': money_count},\n","                               ignore_index=True)\n","\n","        client = client.append(\n","            {'用户编码': id, '年平均缴费次数': count_mean, '年平均缴费金额': money_mean, '平均每次缴费金额': money_count, '是否为高价值': 0},\n","            ignore_index=True)\n","    elif (count_mean <= count and money_mean > money):\n","        potency = potency.append({'用户编码': id, '年平均缴费次数': count_mean, '年平均缴费金额': money_mean, '平均每次缴费金额': money_count},\n","                                 ignore_index=True)\n","\n","        client = client.append(\n","            {'用户编码': id, '年平均缴费次数': count_mean, '年平均缴费金额': money_mean, '平均每次缴费金额': money_count, '是否为高价值': 0},\n","            ignore_index=True)\n","\n","print(\"高价值型客户：\\n{}；\\n低价值型客户：\\n{}；\\n大众型客户：\\n{}；\\n潜力型客户：\\n{}\".format(high_value, low_value, normal, potency))\n","saved(high_value, 'Task2_high.csv')\n","saved(low_value, 'Task2_low.csv')\n","saved(normal, 'Task2_normal.csv')\n","saved(potency, 'Task2_potency.csv')\n","client\n","\n","\n","\n","#38 23 33 2"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2tIIKoM2-PCx","executionInfo":{"status":"ok","timestamp":1657193936642,"user_tz":-480,"elapsed":1353,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"3ffc1f06-1cba-4281-cf4a-7070774897f6"}},{"cell_type":"code","execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(          用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额\n"," 0   1000000001      4.0    493.5  246.750000\n"," 1   1000000003      3.5    590.0  337.142857\n"," 2   1000000005      3.5    750.0  428.571429\n"," 3   1000000006      3.5    505.0  288.571429\n"," 4   1000000007      3.5    553.0  316.000000\n"," 5   1000000008      3.5    537.0  306.857143\n"," 6   1000000009      3.5    485.0  277.142857\n"," 7   1000000010      4.0    386.5  193.250000\n"," 8   1000000011      3.5    374.0  213.714286\n"," 9   1000000012      3.5    455.5  260.285714\n"," 10  1000000013      3.5    417.5  238.571429\n"," 11  1000000014      3.5    374.5  214.000000\n"," 12  1000000016      4.0    392.0  196.000000\n"," 13  1000000017      3.5    390.0  222.857143\n"," 14  1000000020      3.5    449.5  256.857143\n"," 15  1000000021      3.5    378.5  216.285714\n"," 16  1000000024      3.5    452.0  258.285714\n"," 17  1000000026      3.5    388.5  222.000000\n"," 18  1000000034      3.5    360.5  206.000000\n"," 19  1000000035      3.5    384.5  219.714286\n"," 20  1000000039      3.5    375.0  214.285714\n"," 21  1000000052      3.5    383.0  218.857143\n"," 22  1000000055      3.5    394.0  225.142857\n"," 23  1000000059      4.0    370.0  185.000000\n"," 24  1000000060      3.5    365.0  208.571429\n"," 25  1000000062      3.5    386.5  220.857143\n"," 26  1000000065      3.5    367.0  209.714286\n"," 27  1000000071      3.5    371.0  212.000000\n"," 28  1000000072      3.5    399.0  228.000000\n"," 29  1000000076      3.5    362.5  207.142857\n"," 30  1000000077      3.5    360.0  205.714286\n"," 31  1000000083      3.5    374.0  213.714286\n"," 32  1000000084      3.5    644.5  368.285714\n"," 33  1000000089      5.0    388.0   77.600000\n"," 34  1000000091      3.5    380.5  217.428571\n"," 35  1000000093      5.0    463.0   92.600000\n"," 36  1000000095      3.5    499.5  285.428571\n"," 37  1000000096      3.5    458.0  261.714286\n"," 38  1000000100      3.5    471.5  269.428571,\n","           用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额\n"," 0   1000000022      3.0    267.0  178.000000\n"," 1   1000000023      2.5    246.5  197.200000\n"," 2   1000000028      3.0    285.5  190.333333\n"," 3   1000000032      3.0    259.5  173.000000\n"," 4   1000000033      3.0    272.5  181.666667\n"," 5   1000000036      3.0    322.5  215.000000\n"," 6   1000000038      3.0    267.0  178.000000\n"," 7   1000000041      3.0    341.5  227.666667\n"," 8   1000000044      3.0    300.5  200.333333\n"," 9   1000000046      3.0    355.5  237.000000\n"," 10  1000000048      3.0    339.0  226.000000\n"," 11  1000000051      3.0    298.5  199.000000\n"," 12  1000000054      3.0    229.5  153.000000\n"," 13  1000000064      3.0    260.0  173.333333\n"," 14  1000000066      3.0    349.5  233.000000\n"," 15  1000000069      3.0    274.0  182.666667\n"," 16  1000000073      2.0    202.0  202.000000\n"," 17  1000000078      2.5    296.0  236.800000\n"," 18  1000000079      3.0    298.5  199.000000\n"," 19  1000000082      2.5    254.0  203.200000\n"," 20  1000000090      3.0    306.0  204.000000\n"," 21  1000000092      3.0    254.0   84.666667\n"," 22  1000000094      3.0    313.0  208.666667\n"," 23  1000000097      3.0    324.5  216.333333,\n","           用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额\n"," 0   1000000002      3.5    245.0  140.000000\n"," 1   1000000004      4.0    310.5  155.250000\n"," 2   1000000015      3.5    350.0  200.000000\n"," 3   1000000018      3.5    320.5  183.142857\n"," 4   1000000025      3.5    325.5  186.000000\n"," 5   1000000027      3.5    315.5  180.285714\n"," 6   1000000029      3.5    345.5  197.428571\n"," 7   1000000030      3.5    335.5  191.714286\n"," 8   1000000031      3.5    327.5  187.142857\n"," 9   1000000037      3.5    313.5  179.142857\n"," 10  1000000040      3.5    328.0  187.428571\n"," 11  1000000042      3.5    306.0  174.857143\n"," 12  1000000043      3.5    320.0  182.857143\n"," 13  1000000045      3.5    355.5  203.142857\n"," 14  1000000047      3.5    327.0  186.857143\n"," 15  1000000049      3.5    294.5  168.285714\n"," 16  1000000050      3.5    284.0  162.285714\n"," 17  1000000053      3.5    347.0  198.285714\n"," 18  1000000056      3.5    357.0  204.000000\n"," 19  1000000057      3.5    355.5  203.142857\n"," 20  1000000058      3.5    342.5  195.714286\n"," 21  1000000061      3.5    341.5  195.142857\n"," 22  1000000063      3.5    314.0  179.428571\n"," 23  1000000067      3.5    344.5  196.857143\n"," 24  1000000068      3.5    355.5  203.142857\n"," 25  1000000070      4.0    333.5  166.750000\n"," 26  1000000074      3.5    318.0  181.714286\n"," 27  1000000075      3.5    314.5  179.714286\n"," 28  1000000080      3.5    301.5  172.285714\n"," 29  1000000081      3.5    340.0  194.285714\n"," 30  1000000085      3.5    284.5  162.571429\n"," 31  1000000086      4.0    325.0   81.250000\n"," 32  1000000087      3.5    325.5  186.000000\n"," 33  1000000088      3.5    269.0  153.714286,\n","          用户编码  年平均缴费次数  年平均缴费金额    平均每次缴费金额\n"," 0  1000000019      3.0    364.5  243.000000\n"," 1  1000000098      3.0    367.5  245.000000\n"," 2  1000000099      3.0    426.5  284.333333)"]},"metadata":{},"execution_count":9}],"source":["high_value, low_value, normal, potency"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"V5vlNtnR-PCx","executionInfo":{"status":"ok","timestamp":1657193936643,"user_tz":-480,"elapsed":14,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"87dc70b1-efda-43b9-e3af-c163ed6d8edf"}},{"cell_type":"markdown","source":["# 任务三，预测高价值top5"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"hp6x2INn-PCx"}},{"cell_type":"code","execution_count":10,"outputs":[],"source":["from sklearn import model_selection\n","\n","# 将数据集拆分为训练集和测试集\n","X = client[['年平均缴费次数', '年平均缴费金额', '平均每次缴费金额']]\n","y = client[['用户编码', '是否为高价值']]\n","X, X_test, y, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n","X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.1)\n","Y_test = y_test\n","y = y['是否为高价值']\n","y_train = y_train['是否为高价值']\n","y_valid = y_valid['是否为高价值']\n","y_test = y_test['是否为高价值']"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"CTQpXIVH-PCy","executionInfo":{"status":"ok","timestamp":1657193937139,"user_tz":-480,"elapsed":503,"user":{"displayName":"陈成","userId":"07346209167866063288"}}}},{"cell_type":"code","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0.]\n"]}],"source":["y_valid = y_valid.to_numpy().astype(np.float64)\n","y_test = y_test.to_numpy().astype(np.float64)\n","print(y_test)"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"GYF-9eI2-PCy","executionInfo":{"status":"ok","timestamp":1657193937140,"user_tz":-480,"elapsed":12,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"d95bd6a0-29f2-4fd4-d67a-a1acc6b7ee19"}},{"cell_type":"code","execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1302: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  dtype=np.object)\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1489: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  sample_mask = np.ones((n_samples, ), dtype=np.bool)\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n"]},{"output_type":"stream","name":"stdout","text":["Learning rate set to 0.00335\n","0:\tlearn: 0.6879725\ttotal: 47.2ms\tremaining: 47.2s\n","1:\tlearn: 0.6833458\ttotal: 47.8ms\tremaining: 23.9s\n","2:\tlearn: 0.6781754\ttotal: 48.9ms\tremaining: 16.3s\n","3:\tlearn: 0.6743283\ttotal: 49.9ms\tremaining: 12.4s\n","4:\tlearn: 0.6688156\ttotal: 50.7ms\tremaining: 10.1s\n","5:\tlearn: 0.6647604\ttotal: 51.6ms\tremaining: 8.54s\n","6:\tlearn: 0.6593466\ttotal: 52.4ms\tremaining: 7.44s\n","7:\tlearn: 0.6549589\ttotal: 53.2ms\tremaining: 6.6s\n","8:\tlearn: 0.6492052\ttotal: 53.9ms\tremaining: 5.93s\n","9:\tlearn: 0.6446711\ttotal: 54.6ms\tremaining: 5.41s\n","10:\tlearn: 0.6401237\ttotal: 55.3ms\tremaining: 4.97s\n","11:\tlearn: 0.6349967\ttotal: 56.1ms\tremaining: 4.62s\n","12:\tlearn: 0.6301427\ttotal: 57.1ms\tremaining: 4.33s\n","13:\tlearn: 0.6250087\ttotal: 58.1ms\tremaining: 4.09s\n","14:\tlearn: 0.6197657\ttotal: 58.8ms\tremaining: 3.86s\n","15:\tlearn: 0.6153346\ttotal: 59.5ms\tremaining: 3.66s\n","16:\tlearn: 0.6112474\ttotal: 60.7ms\tremaining: 3.51s\n","17:\tlearn: 0.6067846\ttotal: 61.1ms\tremaining: 3.33s\n","18:\tlearn: 0.6019988\ttotal: 61.5ms\tremaining: 3.18s\n","19:\tlearn: 0.5976939\ttotal: 62ms\tremaining: 3.04s\n","20:\tlearn: 0.5940894\ttotal: 62.4ms\tremaining: 2.91s\n","21:\tlearn: 0.5905097\ttotal: 62.7ms\tremaining: 2.79s\n","22:\tlearn: 0.5861195\ttotal: 63.1ms\tremaining: 2.68s\n","23:\tlearn: 0.5820666\ttotal: 63.5ms\tremaining: 2.58s\n","24:\tlearn: 0.5777563\ttotal: 63.9ms\tremaining: 2.49s\n","25:\tlearn: 0.5734460\ttotal: 64.3ms\tremaining: 2.41s\n","26:\tlearn: 0.5693422\ttotal: 64.7ms\tremaining: 2.33s\n","27:\tlearn: 0.5659410\ttotal: 65.1ms\tremaining: 2.26s\n","28:\tlearn: 0.5620148\ttotal: 65.5ms\tremaining: 2.19s\n","29:\tlearn: 0.5581312\ttotal: 65.8ms\tremaining: 2.13s\n","30:\tlearn: 0.5543914\ttotal: 66.2ms\tremaining: 2.07s\n","31:\tlearn: 0.5505516\ttotal: 66.6ms\tremaining: 2.02s\n","32:\tlearn: 0.5464686\ttotal: 66.8ms\tremaining: 1.96s\n","33:\tlearn: 0.5421670\ttotal: 67.3ms\tremaining: 1.91s\n","34:\tlearn: 0.5387869\ttotal: 67.7ms\tremaining: 1.86s\n","35:\tlearn: 0.5353129\ttotal: 68ms\tremaining: 1.82s\n","36:\tlearn: 0.5315025\ttotal: 68.5ms\tremaining: 1.78s\n","37:\tlearn: 0.5274269\ttotal: 68.8ms\tremaining: 1.74s\n","38:\tlearn: 0.5232851\ttotal: 69.2ms\tremaining: 1.71s\n","39:\tlearn: 0.5202992\ttotal: 69.4ms\tremaining: 1.67s\n","40:\tlearn: 0.5163776\ttotal: 69.8ms\tremaining: 1.63s\n","41:\tlearn: 0.5124835\ttotal: 70.2ms\tremaining: 1.6s\n","42:\tlearn: 0.5087183\ttotal: 70.7ms\tremaining: 1.57s\n","43:\tlearn: 0.5045286\ttotal: 70.9ms\tremaining: 1.54s\n","44:\tlearn: 0.5009682\ttotal: 71.3ms\tremaining: 1.51s\n","45:\tlearn: 0.4974241\ttotal: 71.7ms\tremaining: 1.49s\n","46:\tlearn: 0.4942377\ttotal: 72.1ms\tremaining: 1.46s\n","47:\tlearn: 0.4904749\ttotal: 72.5ms\tremaining: 1.44s\n","48:\tlearn: 0.4871908\ttotal: 72.8ms\tremaining: 1.41s\n","49:\tlearn: 0.4837616\ttotal: 73.2ms\tremaining: 1.39s\n","50:\tlearn: 0.4804521\ttotal: 73.6ms\tremaining: 1.37s\n","51:\tlearn: 0.4770861\ttotal: 73.9ms\tremaining: 1.35s\n","52:\tlearn: 0.4731899\ttotal: 74.3ms\tremaining: 1.33s\n","53:\tlearn: 0.4694982\ttotal: 74.6ms\tremaining: 1.31s\n","54:\tlearn: 0.4662622\ttotal: 75.5ms\tremaining: 1.3s\n","55:\tlearn: 0.4631456\ttotal: 75.9ms\tremaining: 1.28s\n","56:\tlearn: 0.4604680\ttotal: 76.4ms\tremaining: 1.26s\n","57:\tlearn: 0.4573834\ttotal: 76.8ms\tremaining: 1.25s\n","58:\tlearn: 0.4545751\ttotal: 77.2ms\tremaining: 1.23s\n","59:\tlearn: 0.4517868\ttotal: 77.5ms\tremaining: 1.21s\n","60:\tlearn: 0.4495515\ttotal: 77.8ms\tremaining: 1.2s\n","61:\tlearn: 0.4457468\ttotal: 78.1ms\tremaining: 1.18s\n","62:\tlearn: 0.4431371\ttotal: 78.5ms\tremaining: 1.17s\n","63:\tlearn: 0.4399934\ttotal: 78.9ms\tremaining: 1.15s\n","64:\tlearn: 0.4367048\ttotal: 79.2ms\tremaining: 1.14s\n","65:\tlearn: 0.4340680\ttotal: 79.6ms\tremaining: 1.13s\n","66:\tlearn: 0.4315809\ttotal: 80ms\tremaining: 1.11s\n","67:\tlearn: 0.4290948\ttotal: 80.4ms\tremaining: 1.1s\n","68:\tlearn: 0.4264262\ttotal: 80.7ms\tremaining: 1.09s\n","69:\tlearn: 0.4231533\ttotal: 81.1ms\tremaining: 1.08s\n","70:\tlearn: 0.4194424\ttotal: 81.3ms\tremaining: 1.06s\n","71:\tlearn: 0.4164176\ttotal: 81.7ms\tremaining: 1.05s\n","72:\tlearn: 0.4140817\ttotal: 82.1ms\tremaining: 1.04s\n","73:\tlearn: 0.4109329\ttotal: 82.3ms\tremaining: 1.03s\n","74:\tlearn: 0.4080599\ttotal: 82.7ms\tremaining: 1.02s\n","75:\tlearn: 0.4052449\ttotal: 83ms\tremaining: 1.01s\n","76:\tlearn: 0.4023872\ttotal: 83.4ms\tremaining: 999ms\n","77:\tlearn: 0.3995936\ttotal: 83.8ms\tremaining: 991ms\n","78:\tlearn: 0.3965045\ttotal: 84ms\tremaining: 980ms\n","79:\tlearn: 0.3940857\ttotal: 84.4ms\tremaining: 971ms\n","80:\tlearn: 0.3916965\ttotal: 84.8ms\tremaining: 962ms\n","81:\tlearn: 0.3896337\ttotal: 85.2ms\tremaining: 954ms\n","82:\tlearn: 0.3866853\ttotal: 85.6ms\tremaining: 946ms\n","83:\tlearn: 0.3840732\ttotal: 86.3ms\tremaining: 941ms\n","84:\tlearn: 0.3815475\ttotal: 86.7ms\tremaining: 933ms\n","85:\tlearn: 0.3792932\ttotal: 87.1ms\tremaining: 926ms\n","86:\tlearn: 0.3770343\ttotal: 87.5ms\tremaining: 918ms\n","87:\tlearn: 0.3750520\ttotal: 87.9ms\tremaining: 911ms\n","88:\tlearn: 0.3726404\ttotal: 88.2ms\tremaining: 903ms\n","89:\tlearn: 0.3702537\ttotal: 88.6ms\tremaining: 896ms\n","90:\tlearn: 0.3669084\ttotal: 88.8ms\tremaining: 888ms\n","91:\tlearn: 0.3641730\ttotal: 89.1ms\tremaining: 880ms\n","92:\tlearn: 0.3615208\ttotal: 89.6ms\tremaining: 874ms\n","93:\tlearn: 0.3593511\ttotal: 90ms\tremaining: 867ms\n","94:\tlearn: 0.3565231\ttotal: 90.2ms\tremaining: 860ms\n","95:\tlearn: 0.3546065\ttotal: 90.5ms\tremaining: 852ms\n","96:\tlearn: 0.3525364\ttotal: 90.9ms\tremaining: 846ms\n","97:\tlearn: 0.3498325\ttotal: 91.1ms\tremaining: 839ms\n","98:\tlearn: 0.3477684\ttotal: 91.5ms\tremaining: 833ms\n","99:\tlearn: 0.3454206\ttotal: 92.1ms\tremaining: 829ms\n","100:\tlearn: 0.3432389\ttotal: 92.5ms\tremaining: 823ms\n","101:\tlearn: 0.3412599\ttotal: 92.9ms\tremaining: 818ms\n","102:\tlearn: 0.3387254\ttotal: 93.3ms\tremaining: 812ms\n","103:\tlearn: 0.3364170\ttotal: 93.7ms\tremaining: 807ms\n","104:\tlearn: 0.3343366\ttotal: 94ms\tremaining: 802ms\n","105:\tlearn: 0.3328331\ttotal: 94.4ms\tremaining: 797ms\n","106:\tlearn: 0.3308585\ttotal: 94.8ms\tremaining: 791ms\n","107:\tlearn: 0.3284801\ttotal: 95.1ms\tremaining: 786ms\n","108:\tlearn: 0.3266177\ttotal: 95.5ms\tremaining: 781ms\n","109:\tlearn: 0.3242777\ttotal: 96.1ms\tremaining: 777ms\n","110:\tlearn: 0.3224976\ttotal: 96.4ms\tremaining: 772ms\n","111:\tlearn: 0.3202976\ttotal: 96.9ms\tremaining: 768ms\n","112:\tlearn: 0.3182914\ttotal: 97.2ms\tremaining: 763ms\n","113:\tlearn: 0.3162971\ttotal: 97.6ms\tremaining: 758ms\n","114:\tlearn: 0.3142874\ttotal: 98ms\tremaining: 754ms\n","115:\tlearn: 0.3118336\ttotal: 98.3ms\tremaining: 749ms\n","116:\tlearn: 0.3100761\ttotal: 98.7ms\tremaining: 745ms\n","117:\tlearn: 0.3082096\ttotal: 99.1ms\tremaining: 741ms\n","118:\tlearn: 0.3063496\ttotal: 99.5ms\tremaining: 736ms\n","119:\tlearn: 0.3037290\ttotal: 99.7ms\tremaining: 731ms\n","120:\tlearn: 0.3018150\ttotal: 100ms\tremaining: 727ms\n","121:\tlearn: 0.3002000\ttotal: 101ms\tremaining: 723ms\n","122:\tlearn: 0.2987714\ttotal: 101ms\tremaining: 720ms\n","123:\tlearn: 0.2971494\ttotal: 101ms\tremaining: 716ms\n","124:\tlearn: 0.2952110\ttotal: 102ms\tremaining: 717ms\n","125:\tlearn: 0.2933965\ttotal: 103ms\tremaining: 715ms\n","126:\tlearn: 0.2914921\ttotal: 103ms\tremaining: 711ms\n","127:\tlearn: 0.2898651\ttotal: 104ms\tremaining: 707ms\n","128:\tlearn: 0.2883174\ttotal: 104ms\tremaining: 705ms\n","129:\tlearn: 0.2865975\ttotal: 105ms\tremaining: 701ms\n","130:\tlearn: 0.2847432\ttotal: 105ms\tremaining: 698ms\n","131:\tlearn: 0.2831465\ttotal: 106ms\tremaining: 694ms\n","132:\tlearn: 0.2814263\ttotal: 106ms\tremaining: 690ms\n","133:\tlearn: 0.2799546\ttotal: 106ms\tremaining: 688ms\n","134:\tlearn: 0.2780503\ttotal: 107ms\tremaining: 686ms\n","135:\tlearn: 0.2766419\ttotal: 107ms\tremaining: 682ms\n","136:\tlearn: 0.2753576\ttotal: 108ms\tremaining: 680ms\n","137:\tlearn: 0.2734893\ttotal: 108ms\tremaining: 677ms\n","138:\tlearn: 0.2719337\ttotal: 109ms\tremaining: 674ms\n","139:\tlearn: 0.2697604\ttotal: 109ms\tremaining: 670ms\n","140:\tlearn: 0.2685909\ttotal: 109ms\tremaining: 667ms\n","141:\tlearn: 0.2674032\ttotal: 110ms\tremaining: 664ms\n","142:\tlearn: 0.2658058\ttotal: 110ms\tremaining: 661ms\n","143:\tlearn: 0.2640771\ttotal: 111ms\tremaining: 657ms\n","144:\tlearn: 0.2622998\ttotal: 111ms\tremaining: 654ms\n","145:\tlearn: 0.2607684\ttotal: 111ms\tremaining: 651ms\n","146:\tlearn: 0.2593853\ttotal: 112ms\tremaining: 648ms\n","147:\tlearn: 0.2580739\ttotal: 112ms\tremaining: 646ms\n","148:\tlearn: 0.2562804\ttotal: 113ms\tremaining: 643ms\n","149:\tlearn: 0.2547870\ttotal: 113ms\tremaining: 641ms\n","150:\tlearn: 0.2534401\ttotal: 114ms\tremaining: 639ms\n","151:\tlearn: 0.2515776\ttotal: 114ms\tremaining: 636ms\n","152:\tlearn: 0.2500386\ttotal: 114ms\tremaining: 633ms\n","153:\tlearn: 0.2481808\ttotal: 115ms\tremaining: 630ms\n","154:\tlearn: 0.2464338\ttotal: 115ms\tremaining: 627ms\n","155:\tlearn: 0.2448849\ttotal: 115ms\tremaining: 624ms\n","156:\tlearn: 0.2437832\ttotal: 116ms\tremaining: 622ms\n","157:\tlearn: 0.2422582\ttotal: 116ms\tremaining: 619ms\n","158:\tlearn: 0.2406494\ttotal: 117ms\tremaining: 617ms\n","159:\tlearn: 0.2394794\ttotal: 117ms\tremaining: 615ms\n","160:\tlearn: 0.2383272\ttotal: 118ms\tremaining: 612ms\n","161:\tlearn: 0.2370484\ttotal: 118ms\tremaining: 610ms\n","162:\tlearn: 0.2357283\ttotal: 118ms\tremaining: 607ms\n","163:\tlearn: 0.2348272\ttotal: 119ms\tremaining: 605ms\n","164:\tlearn: 0.2336723\ttotal: 119ms\tremaining: 603ms\n","165:\tlearn: 0.2325764\ttotal: 120ms\tremaining: 600ms\n","166:\tlearn: 0.2313036\ttotal: 120ms\tremaining: 598ms\n","167:\tlearn: 0.2296919\ttotal: 120ms\tremaining: 595ms\n","168:\tlearn: 0.2282795\ttotal: 121ms\tremaining: 593ms\n","169:\tlearn: 0.2271876\ttotal: 121ms\tremaining: 591ms\n","170:\tlearn: 0.2259497\ttotal: 121ms\tremaining: 589ms\n","171:\tlearn: 0.2244515\ttotal: 122ms\tremaining: 586ms\n","172:\tlearn: 0.2234429\ttotal: 122ms\tremaining: 584ms\n","173:\tlearn: 0.2219743\ttotal: 123ms\tremaining: 582ms\n","174:\tlearn: 0.2206316\ttotal: 123ms\tremaining: 579ms\n","175:\tlearn: 0.2196078\ttotal: 124ms\tremaining: 579ms\n","176:\tlearn: 0.2180365\ttotal: 124ms\tremaining: 578ms\n","177:\tlearn: 0.2165645\ttotal: 125ms\tremaining: 576ms\n","178:\tlearn: 0.2154001\ttotal: 125ms\tremaining: 575ms\n","179:\tlearn: 0.2140698\ttotal: 126ms\tremaining: 573ms\n","180:\tlearn: 0.2131046\ttotal: 126ms\tremaining: 572ms\n","181:\tlearn: 0.2116901\ttotal: 127ms\tremaining: 570ms\n","182:\tlearn: 0.2106391\ttotal: 127ms\tremaining: 568ms\n","183:\tlearn: 0.2095372\ttotal: 127ms\tremaining: 565ms\n","184:\tlearn: 0.2083463\ttotal: 128ms\tremaining: 563ms\n","185:\tlearn: 0.2073632\ttotal: 128ms\tremaining: 562ms\n","186:\tlearn: 0.2063016\ttotal: 129ms\tremaining: 559ms\n","187:\tlearn: 0.2050259\ttotal: 129ms\tremaining: 558ms\n","188:\tlearn: 0.2042054\ttotal: 130ms\tremaining: 556ms\n","189:\tlearn: 0.2032507\ttotal: 130ms\tremaining: 554ms\n","190:\tlearn: 0.2016508\ttotal: 130ms\tremaining: 551ms\n","191:\tlearn: 0.2003040\ttotal: 130ms\tremaining: 549ms\n","192:\tlearn: 0.1991617\ttotal: 131ms\tremaining: 547ms\n","193:\tlearn: 0.1981947\ttotal: 131ms\tremaining: 545ms\n","194:\tlearn: 0.1971649\ttotal: 131ms\tremaining: 543ms\n","195:\tlearn: 0.1960962\ttotal: 132ms\tremaining: 541ms\n","196:\tlearn: 0.1950798\ttotal: 132ms\tremaining: 539ms\n","197:\tlearn: 0.1941208\ttotal: 133ms\tremaining: 537ms\n","198:\tlearn: 0.1932140\ttotal: 133ms\tremaining: 536ms\n","199:\tlearn: 0.1923171\ttotal: 133ms\tremaining: 534ms\n","200:\tlearn: 0.1912952\ttotal: 134ms\tremaining: 532ms\n","201:\tlearn: 0.1902276\ttotal: 134ms\tremaining: 531ms\n","202:\tlearn: 0.1893987\ttotal: 135ms\tremaining: 530ms\n","203:\tlearn: 0.1883438\ttotal: 135ms\tremaining: 528ms\n","204:\tlearn: 0.1873275\ttotal: 136ms\tremaining: 527ms\n","205:\tlearn: 0.1864309\ttotal: 136ms\tremaining: 525ms\n","206:\tlearn: 0.1852666\ttotal: 137ms\tremaining: 523ms\n","207:\tlearn: 0.1842521\ttotal: 137ms\tremaining: 521ms\n","208:\tlearn: 0.1833650\ttotal: 137ms\tremaining: 520ms\n","209:\tlearn: 0.1825293\ttotal: 138ms\tremaining: 518ms\n","210:\tlearn: 0.1815062\ttotal: 138ms\tremaining: 516ms\n","211:\tlearn: 0.1808992\ttotal: 138ms\tremaining: 514ms\n","212:\tlearn: 0.1800459\ttotal: 139ms\tremaining: 512ms\n","213:\tlearn: 0.1791247\ttotal: 139ms\tremaining: 512ms\n","214:\tlearn: 0.1782507\ttotal: 140ms\tremaining: 510ms\n","215:\tlearn: 0.1772593\ttotal: 140ms\tremaining: 509ms\n","216:\tlearn: 0.1763910\ttotal: 140ms\tremaining: 507ms\n","217:\tlearn: 0.1755523\ttotal: 141ms\tremaining: 505ms\n","218:\tlearn: 0.1749138\ttotal: 141ms\tremaining: 504ms\n","219:\tlearn: 0.1739752\ttotal: 142ms\tremaining: 502ms\n","220:\tlearn: 0.1730555\ttotal: 142ms\tremaining: 501ms\n","221:\tlearn: 0.1720865\ttotal: 142ms\tremaining: 499ms\n","222:\tlearn: 0.1712345\ttotal: 143ms\tremaining: 498ms\n","223:\tlearn: 0.1700289\ttotal: 143ms\tremaining: 496ms\n","224:\tlearn: 0.1691243\ttotal: 144ms\tremaining: 494ms\n","225:\tlearn: 0.1682657\ttotal: 144ms\tremaining: 493ms\n","226:\tlearn: 0.1673758\ttotal: 145ms\tremaining: 492ms\n","227:\tlearn: 0.1666978\ttotal: 145ms\tremaining: 491ms\n","228:\tlearn: 0.1659422\ttotal: 145ms\tremaining: 489ms\n","229:\tlearn: 0.1653774\ttotal: 146ms\tremaining: 488ms\n","230:\tlearn: 0.1644198\ttotal: 146ms\tremaining: 486ms\n","231:\tlearn: 0.1635741\ttotal: 146ms\tremaining: 485ms\n","232:\tlearn: 0.1630853\ttotal: 147ms\tremaining: 484ms\n","233:\tlearn: 0.1624467\ttotal: 147ms\tremaining: 482ms\n","234:\tlearn: 0.1615835\ttotal: 148ms\tremaining: 481ms\n","235:\tlearn: 0.1608297\ttotal: 148ms\tremaining: 479ms\n","236:\tlearn: 0.1600591\ttotal: 149ms\tremaining: 478ms\n","237:\tlearn: 0.1592640\ttotal: 149ms\tremaining: 476ms\n","238:\tlearn: 0.1584816\ttotal: 149ms\tremaining: 475ms\n","239:\tlearn: 0.1578337\ttotal: 149ms\tremaining: 473ms\n","240:\tlearn: 0.1571597\ttotal: 150ms\tremaining: 472ms\n","241:\tlearn: 0.1566061\ttotal: 150ms\tremaining: 470ms\n","242:\tlearn: 0.1558508\ttotal: 151ms\tremaining: 469ms\n","243:\tlearn: 0.1548922\ttotal: 151ms\tremaining: 467ms\n","244:\tlearn: 0.1540458\ttotal: 151ms\tremaining: 466ms\n","245:\tlearn: 0.1533956\ttotal: 152ms\tremaining: 465ms\n","246:\tlearn: 0.1527943\ttotal: 152ms\tremaining: 464ms\n","247:\tlearn: 0.1522503\ttotal: 152ms\tremaining: 462ms\n","248:\tlearn: 0.1514413\ttotal: 153ms\tremaining: 461ms\n","249:\tlearn: 0.1505586\ttotal: 153ms\tremaining: 459ms\n","250:\tlearn: 0.1497933\ttotal: 154ms\tremaining: 458ms\n","251:\tlearn: 0.1491101\ttotal: 154ms\tremaining: 457ms\n","252:\tlearn: 0.1482728\ttotal: 155ms\tremaining: 457ms\n","253:\tlearn: 0.1475578\ttotal: 155ms\tremaining: 456ms\n","254:\tlearn: 0.1467448\ttotal: 156ms\tremaining: 455ms\n","255:\tlearn: 0.1459827\ttotal: 156ms\tremaining: 454ms\n","256:\tlearn: 0.1452776\ttotal: 157ms\tremaining: 453ms\n","257:\tlearn: 0.1444871\ttotal: 157ms\tremaining: 452ms\n","258:\tlearn: 0.1439544\ttotal: 157ms\tremaining: 451ms\n","259:\tlearn: 0.1430749\ttotal: 158ms\tremaining: 449ms\n","260:\tlearn: 0.1423797\ttotal: 158ms\tremaining: 447ms\n","261:\tlearn: 0.1416050\ttotal: 158ms\tremaining: 446ms\n","262:\tlearn: 0.1409789\ttotal: 159ms\tremaining: 445ms\n","263:\tlearn: 0.1404416\ttotal: 159ms\tremaining: 443ms\n","264:\tlearn: 0.1396718\ttotal: 159ms\tremaining: 442ms\n","265:\tlearn: 0.1390738\ttotal: 160ms\tremaining: 440ms\n","266:\tlearn: 0.1384362\ttotal: 160ms\tremaining: 439ms\n","267:\tlearn: 0.1378796\ttotal: 160ms\tremaining: 438ms\n","268:\tlearn: 0.1373197\ttotal: 161ms\tremaining: 437ms\n","269:\tlearn: 0.1366380\ttotal: 161ms\tremaining: 436ms\n","270:\tlearn: 0.1362233\ttotal: 162ms\tremaining: 435ms\n","271:\tlearn: 0.1356154\ttotal: 162ms\tremaining: 434ms\n","272:\tlearn: 0.1349078\ttotal: 163ms\tremaining: 434ms\n","273:\tlearn: 0.1340960\ttotal: 163ms\tremaining: 432ms\n","274:\tlearn: 0.1334215\ttotal: 164ms\tremaining: 432ms\n","275:\tlearn: 0.1327395\ttotal: 164ms\tremaining: 431ms\n","276:\tlearn: 0.1320479\ttotal: 165ms\tremaining: 430ms\n","277:\tlearn: 0.1315326\ttotal: 165ms\tremaining: 429ms\n","278:\tlearn: 0.1309498\ttotal: 166ms\tremaining: 428ms\n","279:\tlearn: 0.1303347\ttotal: 166ms\tremaining: 427ms\n","280:\tlearn: 0.1298478\ttotal: 166ms\tremaining: 426ms\n","281:\tlearn: 0.1292457\ttotal: 167ms\tremaining: 425ms\n","282:\tlearn: 0.1287202\ttotal: 167ms\tremaining: 423ms\n","283:\tlearn: 0.1281179\ttotal: 168ms\tremaining: 423ms\n","284:\tlearn: 0.1276093\ttotal: 168ms\tremaining: 422ms\n","285:\tlearn: 0.1269825\ttotal: 168ms\tremaining: 421ms\n","286:\tlearn: 0.1265534\ttotal: 169ms\tremaining: 419ms\n","287:\tlearn: 0.1261585\ttotal: 169ms\tremaining: 418ms\n","288:\tlearn: 0.1255831\ttotal: 170ms\tremaining: 417ms\n","289:\tlearn: 0.1250556\ttotal: 170ms\tremaining: 416ms\n","290:\tlearn: 0.1245999\ttotal: 170ms\tremaining: 415ms\n","291:\tlearn: 0.1239896\ttotal: 171ms\tremaining: 414ms\n","292:\tlearn: 0.1235703\ttotal: 171ms\tremaining: 412ms\n","293:\tlearn: 0.1231217\ttotal: 171ms\tremaining: 411ms\n","294:\tlearn: 0.1223695\ttotal: 172ms\tremaining: 410ms\n","295:\tlearn: 0.1218676\ttotal: 172ms\tremaining: 409ms\n","296:\tlearn: 0.1213915\ttotal: 173ms\tremaining: 408ms\n","297:\tlearn: 0.1206909\ttotal: 173ms\tremaining: 407ms\n","298:\tlearn: 0.1201206\ttotal: 173ms\tremaining: 406ms\n","299:\tlearn: 0.1195424\ttotal: 174ms\tremaining: 406ms\n","300:\tlearn: 0.1190820\ttotal: 174ms\tremaining: 405ms\n","301:\tlearn: 0.1186636\ttotal: 175ms\tremaining: 404ms\n","302:\tlearn: 0.1181827\ttotal: 175ms\tremaining: 403ms\n","303:\tlearn: 0.1177859\ttotal: 176ms\tremaining: 402ms\n","304:\tlearn: 0.1170120\ttotal: 176ms\tremaining: 401ms\n","305:\tlearn: 0.1164656\ttotal: 176ms\tremaining: 399ms\n","306:\tlearn: 0.1161807\ttotal: 176ms\tremaining: 398ms\n","307:\tlearn: 0.1157351\ttotal: 177ms\tremaining: 397ms\n","308:\tlearn: 0.1152259\ttotal: 177ms\tremaining: 396ms\n","309:\tlearn: 0.1147463\ttotal: 178ms\tremaining: 396ms\n","310:\tlearn: 0.1142578\ttotal: 178ms\tremaining: 394ms\n","311:\tlearn: 0.1137800\ttotal: 178ms\tremaining: 394ms\n","312:\tlearn: 0.1134736\ttotal: 179ms\tremaining: 393ms\n","313:\tlearn: 0.1130278\ttotal: 179ms\tremaining: 392ms\n","314:\tlearn: 0.1125819\ttotal: 180ms\tremaining: 391ms\n","315:\tlearn: 0.1121344\ttotal: 180ms\tremaining: 390ms\n","316:\tlearn: 0.1117334\ttotal: 181ms\tremaining: 389ms\n","317:\tlearn: 0.1113153\ttotal: 181ms\tremaining: 388ms\n","318:\tlearn: 0.1109332\ttotal: 181ms\tremaining: 387ms\n","319:\tlearn: 0.1104808\ttotal: 182ms\tremaining: 386ms\n","320:\tlearn: 0.1101166\ttotal: 182ms\tremaining: 385ms\n","321:\tlearn: 0.1095603\ttotal: 182ms\tremaining: 384ms\n","322:\tlearn: 0.1091012\ttotal: 183ms\tremaining: 383ms\n","323:\tlearn: 0.1087394\ttotal: 183ms\tremaining: 382ms\n","324:\tlearn: 0.1082719\ttotal: 184ms\tremaining: 381ms\n","325:\tlearn: 0.1079200\ttotal: 184ms\tremaining: 381ms\n","326:\tlearn: 0.1075569\ttotal: 185ms\tremaining: 380ms\n","327:\tlearn: 0.1072382\ttotal: 185ms\tremaining: 379ms\n","328:\tlearn: 0.1067102\ttotal: 186ms\tremaining: 379ms\n","329:\tlearn: 0.1063482\ttotal: 186ms\tremaining: 378ms\n","330:\tlearn: 0.1058910\ttotal: 187ms\tremaining: 378ms\n","331:\tlearn: 0.1054240\ttotal: 187ms\tremaining: 377ms\n","332:\tlearn: 0.1050513\ttotal: 188ms\tremaining: 376ms\n","333:\tlearn: 0.1047800\ttotal: 188ms\tremaining: 376ms\n","334:\tlearn: 0.1043665\ttotal: 189ms\tremaining: 375ms\n","335:\tlearn: 0.1040083\ttotal: 190ms\tremaining: 375ms\n","336:\tlearn: 0.1036088\ttotal: 190ms\tremaining: 374ms\n","337:\tlearn: 0.1031064\ttotal: 191ms\tremaining: 373ms\n","338:\tlearn: 0.1028046\ttotal: 191ms\tremaining: 373ms\n","339:\tlearn: 0.1024552\ttotal: 192ms\tremaining: 372ms\n","340:\tlearn: 0.1021021\ttotal: 193ms\tremaining: 373ms\n","341:\tlearn: 0.1016645\ttotal: 195ms\tremaining: 375ms\n","342:\tlearn: 0.1013646\ttotal: 196ms\tremaining: 375ms\n","343:\tlearn: 0.1010822\ttotal: 197ms\tremaining: 376ms\n","344:\tlearn: 0.1006606\ttotal: 198ms\tremaining: 377ms\n","345:\tlearn: 0.1001836\ttotal: 200ms\tremaining: 377ms\n","346:\tlearn: 0.0997743\ttotal: 201ms\tremaining: 378ms\n","347:\tlearn: 0.0994366\ttotal: 202ms\tremaining: 379ms\n","348:\tlearn: 0.0990864\ttotal: 204ms\tremaining: 380ms\n","349:\tlearn: 0.0986319\ttotal: 204ms\tremaining: 379ms\n","350:\tlearn: 0.0983103\ttotal: 205ms\tremaining: 379ms\n","351:\tlearn: 0.0979656\ttotal: 206ms\tremaining: 379ms\n","352:\tlearn: 0.0975623\ttotal: 207ms\tremaining: 379ms\n","353:\tlearn: 0.0970475\ttotal: 208ms\tremaining: 380ms\n","354:\tlearn: 0.0966025\ttotal: 209ms\tremaining: 379ms\n","355:\tlearn: 0.0961735\ttotal: 210ms\tremaining: 379ms\n","356:\tlearn: 0.0957500\ttotal: 211ms\tremaining: 380ms\n","357:\tlearn: 0.0953126\ttotal: 212ms\tremaining: 380ms\n","358:\tlearn: 0.0948764\ttotal: 213ms\tremaining: 380ms\n","359:\tlearn: 0.0945128\ttotal: 214ms\tremaining: 380ms\n","360:\tlearn: 0.0939919\ttotal: 215ms\tremaining: 381ms\n","361:\tlearn: 0.0936220\ttotal: 216ms\tremaining: 381ms\n","362:\tlearn: 0.0931781\ttotal: 217ms\tremaining: 381ms\n","363:\tlearn: 0.0928790\ttotal: 218ms\tremaining: 382ms\n","364:\tlearn: 0.0926339\ttotal: 220ms\tremaining: 382ms\n","365:\tlearn: 0.0922973\ttotal: 221ms\tremaining: 383ms\n","366:\tlearn: 0.0919396\ttotal: 222ms\tremaining: 383ms\n","367:\tlearn: 0.0915338\ttotal: 223ms\tremaining: 384ms\n","368:\tlearn: 0.0911721\ttotal: 225ms\tremaining: 384ms\n","369:\tlearn: 0.0908423\ttotal: 225ms\tremaining: 383ms\n","370:\tlearn: 0.0904663\ttotal: 226ms\tremaining: 383ms\n","371:\tlearn: 0.0901668\ttotal: 227ms\tremaining: 383ms\n","372:\tlearn: 0.0899314\ttotal: 228ms\tremaining: 384ms\n","373:\tlearn: 0.0895166\ttotal: 229ms\tremaining: 384ms\n","374:\tlearn: 0.0891617\ttotal: 230ms\tremaining: 383ms\n","375:\tlearn: 0.0887660\ttotal: 231ms\tremaining: 384ms\n","376:\tlearn: 0.0882597\ttotal: 232ms\tremaining: 384ms\n","377:\tlearn: 0.0879874\ttotal: 233ms\tremaining: 383ms\n","378:\tlearn: 0.0877269\ttotal: 234ms\tremaining: 384ms\n","379:\tlearn: 0.0873581\ttotal: 235ms\tremaining: 383ms\n","380:\tlearn: 0.0870033\ttotal: 236ms\tremaining: 383ms\n","381:\tlearn: 0.0866860\ttotal: 237ms\tremaining: 384ms\n","382:\tlearn: 0.0864261\ttotal: 239ms\tremaining: 384ms\n","383:\tlearn: 0.0861215\ttotal: 240ms\tremaining: 385ms\n","384:\tlearn: 0.0858890\ttotal: 242ms\tremaining: 386ms\n","385:\tlearn: 0.0856445\ttotal: 243ms\tremaining: 387ms\n","386:\tlearn: 0.0852895\ttotal: 244ms\tremaining: 387ms\n","387:\tlearn: 0.0850043\ttotal: 246ms\tremaining: 388ms\n","388:\tlearn: 0.0848157\ttotal: 247ms\tremaining: 388ms\n","389:\tlearn: 0.0846021\ttotal: 248ms\tremaining: 388ms\n","390:\tlearn: 0.0844040\ttotal: 249ms\tremaining: 388ms\n","391:\tlearn: 0.0841889\ttotal: 251ms\tremaining: 389ms\n","392:\tlearn: 0.0838483\ttotal: 256ms\tremaining: 395ms\n","393:\tlearn: 0.0834529\ttotal: 261ms\tremaining: 401ms\n","394:\tlearn: 0.0831381\ttotal: 261ms\tremaining: 400ms\n","395:\tlearn: 0.0828475\ttotal: 262ms\tremaining: 400ms\n","396:\tlearn: 0.0825968\ttotal: 263ms\tremaining: 400ms\n","397:\tlearn: 0.0822927\ttotal: 264ms\tremaining: 400ms\n","398:\tlearn: 0.0819343\ttotal: 266ms\tremaining: 400ms\n","399:\tlearn: 0.0815967\ttotal: 267ms\tremaining: 400ms\n","400:\tlearn: 0.0812382\ttotal: 268ms\tremaining: 400ms\n","401:\tlearn: 0.0809911\ttotal: 269ms\tremaining: 401ms\n","402:\tlearn: 0.0807227\ttotal: 271ms\tremaining: 401ms\n","403:\tlearn: 0.0803589\ttotal: 272ms\tremaining: 401ms\n","404:\tlearn: 0.0800421\ttotal: 273ms\tremaining: 401ms\n","405:\tlearn: 0.0798171\ttotal: 274ms\tremaining: 401ms\n","406:\tlearn: 0.0795334\ttotal: 275ms\tremaining: 401ms\n","407:\tlearn: 0.0792309\ttotal: 277ms\tremaining: 402ms\n","408:\tlearn: 0.0789523\ttotal: 278ms\tremaining: 401ms\n","409:\tlearn: 0.0787551\ttotal: 279ms\tremaining: 401ms\n","410:\tlearn: 0.0785565\ttotal: 280ms\tremaining: 401ms\n","411:\tlearn: 0.0782282\ttotal: 281ms\tremaining: 401ms\n","412:\tlearn: 0.0780200\ttotal: 285ms\tremaining: 406ms\n","413:\tlearn: 0.0778174\ttotal: 287ms\tremaining: 406ms\n","414:\tlearn: 0.0775151\ttotal: 288ms\tremaining: 406ms\n","415:\tlearn: 0.0772510\ttotal: 289ms\tremaining: 406ms\n","416:\tlearn: 0.0770102\ttotal: 294ms\tremaining: 411ms\n","417:\tlearn: 0.0767636\ttotal: 295ms\tremaining: 411ms\n","418:\tlearn: 0.0764870\ttotal: 297ms\tremaining: 411ms\n","419:\tlearn: 0.0762272\ttotal: 298ms\tremaining: 412ms\n","420:\tlearn: 0.0759518\ttotal: 300ms\tremaining: 412ms\n","421:\tlearn: 0.0757126\ttotal: 301ms\tremaining: 412ms\n","422:\tlearn: 0.0754705\ttotal: 302ms\tremaining: 412ms\n","423:\tlearn: 0.0752504\ttotal: 304ms\tremaining: 413ms\n","424:\tlearn: 0.0750295\ttotal: 305ms\tremaining: 413ms\n","425:\tlearn: 0.0747792\ttotal: 307ms\tremaining: 414ms\n","426:\tlearn: 0.0745336\ttotal: 308ms\tremaining: 414ms\n","427:\tlearn: 0.0742472\ttotal: 310ms\tremaining: 414ms\n","428:\tlearn: 0.0740120\ttotal: 311ms\tremaining: 414ms\n","429:\tlearn: 0.0736559\ttotal: 314ms\tremaining: 416ms\n","430:\tlearn: 0.0734025\ttotal: 315ms\tremaining: 415ms\n","431:\tlearn: 0.0731424\ttotal: 316ms\tremaining: 416ms\n","432:\tlearn: 0.0729670\ttotal: 317ms\tremaining: 415ms\n","433:\tlearn: 0.0727680\ttotal: 318ms\tremaining: 415ms\n","434:\tlearn: 0.0726211\ttotal: 320ms\tremaining: 415ms\n","435:\tlearn: 0.0724408\ttotal: 321ms\tremaining: 415ms\n","436:\tlearn: 0.0721501\ttotal: 322ms\tremaining: 415ms\n","437:\tlearn: 0.0718720\ttotal: 324ms\tremaining: 415ms\n","438:\tlearn: 0.0716520\ttotal: 326ms\tremaining: 416ms\n","439:\tlearn: 0.0714920\ttotal: 327ms\tremaining: 417ms\n","440:\tlearn: 0.0712380\ttotal: 329ms\tremaining: 417ms\n","441:\tlearn: 0.0710181\ttotal: 330ms\tremaining: 417ms\n","442:\tlearn: 0.0708674\ttotal: 331ms\tremaining: 417ms\n","443:\tlearn: 0.0706292\ttotal: 333ms\tremaining: 417ms\n","444:\tlearn: 0.0704245\ttotal: 334ms\tremaining: 417ms\n","445:\tlearn: 0.0702471\ttotal: 336ms\tremaining: 417ms\n","446:\tlearn: 0.0700004\ttotal: 336ms\tremaining: 416ms\n","447:\tlearn: 0.0698287\ttotal: 337ms\tremaining: 416ms\n","448:\tlearn: 0.0695866\ttotal: 339ms\tremaining: 416ms\n","449:\tlearn: 0.0693011\ttotal: 340ms\tremaining: 415ms\n","450:\tlearn: 0.0691103\ttotal: 341ms\tremaining: 415ms\n","451:\tlearn: 0.0688800\ttotal: 342ms\tremaining: 415ms\n","452:\tlearn: 0.0686580\ttotal: 344ms\tremaining: 415ms\n","453:\tlearn: 0.0683659\ttotal: 345ms\tremaining: 415ms\n","454:\tlearn: 0.0681367\ttotal: 346ms\tremaining: 414ms\n","455:\tlearn: 0.0679227\ttotal: 347ms\tremaining: 414ms\n","456:\tlearn: 0.0677016\ttotal: 349ms\tremaining: 414ms\n","457:\tlearn: 0.0674135\ttotal: 350ms\tremaining: 414ms\n","458:\tlearn: 0.0671973\ttotal: 352ms\tremaining: 414ms\n","459:\tlearn: 0.0669968\ttotal: 353ms\tremaining: 414ms\n","460:\tlearn: 0.0666816\ttotal: 353ms\tremaining: 413ms\n","461:\tlearn: 0.0664762\ttotal: 354ms\tremaining: 412ms\n","462:\tlearn: 0.0662516\ttotal: 355ms\tremaining: 411ms\n","463:\tlearn: 0.0660289\ttotal: 356ms\tremaining: 411ms\n","464:\tlearn: 0.0658541\ttotal: 357ms\tremaining: 411ms\n","465:\tlearn: 0.0655639\ttotal: 358ms\tremaining: 410ms\n","466:\tlearn: 0.0652957\ttotal: 359ms\tremaining: 409ms\n","467:\tlearn: 0.0650675\ttotal: 360ms\tremaining: 409ms\n","468:\tlearn: 0.0649383\ttotal: 361ms\tremaining: 409ms\n","469:\tlearn: 0.0646750\ttotal: 364ms\tremaining: 410ms\n","470:\tlearn: 0.0644603\ttotal: 365ms\tremaining: 410ms\n","471:\tlearn: 0.0642651\ttotal: 367ms\tremaining: 410ms\n","472:\tlearn: 0.0641065\ttotal: 368ms\tremaining: 410ms\n","473:\tlearn: 0.0638866\ttotal: 369ms\tremaining: 410ms\n","474:\tlearn: 0.0635944\ttotal: 371ms\tremaining: 410ms\n","475:\tlearn: 0.0634694\ttotal: 372ms\tremaining: 409ms\n","476:\tlearn: 0.0632405\ttotal: 373ms\tremaining: 409ms\n","477:\tlearn: 0.0630282\ttotal: 375ms\tremaining: 409ms\n","478:\tlearn: 0.0628087\ttotal: 376ms\tremaining: 409ms\n","479:\tlearn: 0.0625657\ttotal: 377ms\tremaining: 409ms\n","480:\tlearn: 0.0623544\ttotal: 378ms\tremaining: 408ms\n","481:\tlearn: 0.0621488\ttotal: 380ms\tremaining: 408ms\n","482:\tlearn: 0.0618960\ttotal: 381ms\tremaining: 407ms\n","483:\tlearn: 0.0616735\ttotal: 381ms\tremaining: 407ms\n","484:\tlearn: 0.0615257\ttotal: 383ms\tremaining: 406ms\n","485:\tlearn: 0.0613164\ttotal: 384ms\tremaining: 406ms\n","486:\tlearn: 0.0611335\ttotal: 385ms\tremaining: 405ms\n","487:\tlearn: 0.0609547\ttotal: 386ms\tremaining: 405ms\n","488:\tlearn: 0.0607982\ttotal: 387ms\tremaining: 404ms\n","489:\tlearn: 0.0606145\ttotal: 388ms\tremaining: 404ms\n","490:\tlearn: 0.0604821\ttotal: 389ms\tremaining: 404ms\n","491:\tlearn: 0.0602921\ttotal: 391ms\tremaining: 404ms\n","492:\tlearn: 0.0600942\ttotal: 392ms\tremaining: 403ms\n","493:\tlearn: 0.0599029\ttotal: 394ms\tremaining: 403ms\n","494:\tlearn: 0.0597680\ttotal: 395ms\tremaining: 403ms\n","495:\tlearn: 0.0596204\ttotal: 395ms\tremaining: 402ms\n","496:\tlearn: 0.0595027\ttotal: 396ms\tremaining: 401ms\n","497:\tlearn: 0.0593702\ttotal: 397ms\tremaining: 400ms\n","498:\tlearn: 0.0591584\ttotal: 398ms\tremaining: 400ms\n","499:\tlearn: 0.0589701\ttotal: 399ms\tremaining: 399ms\n","500:\tlearn: 0.0587414\ttotal: 400ms\tremaining: 399ms\n","501:\tlearn: 0.0585625\ttotal: 402ms\tremaining: 398ms\n","502:\tlearn: 0.0583923\ttotal: 403ms\tremaining: 398ms\n","503:\tlearn: 0.0582368\ttotal: 404ms\tremaining: 397ms\n","504:\tlearn: 0.0581342\ttotal: 405ms\tremaining: 397ms\n","505:\tlearn: 0.0578794\ttotal: 406ms\tremaining: 396ms\n","506:\tlearn: 0.0577001\ttotal: 407ms\tremaining: 396ms\n","507:\tlearn: 0.0575841\ttotal: 408ms\tremaining: 395ms\n","508:\tlearn: 0.0574660\ttotal: 409ms\tremaining: 395ms\n","509:\tlearn: 0.0572995\ttotal: 411ms\tremaining: 395ms\n","510:\tlearn: 0.0571118\ttotal: 413ms\tremaining: 395ms\n","511:\tlearn: 0.0568854\ttotal: 414ms\tremaining: 395ms\n","512:\tlearn: 0.0566783\ttotal: 416ms\tremaining: 395ms\n","513:\tlearn: 0.0565249\ttotal: 418ms\tremaining: 395ms\n","514:\tlearn: 0.0563727\ttotal: 419ms\tremaining: 395ms\n","515:\tlearn: 0.0562221\ttotal: 420ms\tremaining: 394ms\n","516:\tlearn: 0.0560263\ttotal: 421ms\tremaining: 394ms\n","517:\tlearn: 0.0558335\ttotal: 422ms\tremaining: 393ms\n","518:\tlearn: 0.0556876\ttotal: 423ms\tremaining: 392ms\n","519:\tlearn: 0.0555092\ttotal: 424ms\tremaining: 392ms\n","520:\tlearn: 0.0553376\ttotal: 425ms\tremaining: 391ms\n","521:\tlearn: 0.0551860\ttotal: 426ms\tremaining: 390ms\n","522:\tlearn: 0.0549990\ttotal: 427ms\tremaining: 389ms\n","523:\tlearn: 0.0548048\ttotal: 427ms\tremaining: 388ms\n","524:\tlearn: 0.0546559\ttotal: 428ms\tremaining: 387ms\n","525:\tlearn: 0.0544721\ttotal: 429ms\tremaining: 386ms\n","526:\tlearn: 0.0543727\ttotal: 429ms\tremaining: 385ms\n","527:\tlearn: 0.0542464\ttotal: 430ms\tremaining: 384ms\n","528:\tlearn: 0.0540635\ttotal: 431ms\tremaining: 384ms\n","529:\tlearn: 0.0538089\ttotal: 431ms\tremaining: 382ms\n","530:\tlearn: 0.0536650\ttotal: 432ms\tremaining: 382ms\n","531:\tlearn: 0.0535071\ttotal: 433ms\tremaining: 381ms\n","532:\tlearn: 0.0533964\ttotal: 433ms\tremaining: 380ms\n","533:\tlearn: 0.0532863\ttotal: 434ms\tremaining: 379ms\n","534:\tlearn: 0.0531539\ttotal: 435ms\tremaining: 378ms\n","535:\tlearn: 0.0529470\ttotal: 435ms\tremaining: 377ms\n","536:\tlearn: 0.0528100\ttotal: 436ms\tremaining: 376ms\n","537:\tlearn: 0.0527266\ttotal: 436ms\tremaining: 375ms\n","538:\tlearn: 0.0526044\ttotal: 437ms\tremaining: 374ms\n","539:\tlearn: 0.0524882\ttotal: 438ms\tremaining: 373ms\n","540:\tlearn: 0.0523042\ttotal: 439ms\tremaining: 372ms\n","541:\tlearn: 0.0521195\ttotal: 439ms\tremaining: 371ms\n","542:\tlearn: 0.0520086\ttotal: 441ms\tremaining: 371ms\n","543:\tlearn: 0.0518577\ttotal: 442ms\tremaining: 370ms\n","544:\tlearn: 0.0516814\ttotal: 443ms\tremaining: 370ms\n","545:\tlearn: 0.0515356\ttotal: 444ms\tremaining: 369ms\n","546:\tlearn: 0.0513866\ttotal: 446ms\tremaining: 369ms\n","547:\tlearn: 0.0512652\ttotal: 448ms\tremaining: 369ms\n","548:\tlearn: 0.0511101\ttotal: 449ms\tremaining: 369ms\n","549:\tlearn: 0.0509736\ttotal: 449ms\tremaining: 367ms\n","550:\tlearn: 0.0509113\ttotal: 449ms\tremaining: 366ms\n","551:\tlearn: 0.0507665\ttotal: 450ms\tremaining: 365ms\n","552:\tlearn: 0.0506404\ttotal: 450ms\tremaining: 364ms\n","553:\tlearn: 0.0505256\ttotal: 450ms\tremaining: 363ms\n","554:\tlearn: 0.0503338\ttotal: 451ms\tremaining: 361ms\n","555:\tlearn: 0.0502283\ttotal: 451ms\tremaining: 360ms\n","556:\tlearn: 0.0500889\ttotal: 451ms\tremaining: 359ms\n","557:\tlearn: 0.0499417\ttotal: 455ms\tremaining: 361ms\n","558:\tlearn: 0.0497985\ttotal: 457ms\tremaining: 360ms\n","559:\tlearn: 0.0496506\ttotal: 458ms\tremaining: 360ms\n","560:\tlearn: 0.0495068\ttotal: 460ms\tremaining: 360ms\n","561:\tlearn: 0.0494082\ttotal: 461ms\tremaining: 359ms\n","562:\tlearn: 0.0493082\ttotal: 463ms\tremaining: 359ms\n","563:\tlearn: 0.0491925\ttotal: 464ms\tremaining: 359ms\n","564:\tlearn: 0.0490777\ttotal: 465ms\tremaining: 358ms\n","565:\tlearn: 0.0489170\ttotal: 467ms\tremaining: 358ms\n","566:\tlearn: 0.0487937\ttotal: 468ms\tremaining: 357ms\n","567:\tlearn: 0.0486601\ttotal: 469ms\tremaining: 357ms\n","568:\tlearn: 0.0485332\ttotal: 471ms\tremaining: 357ms\n","569:\tlearn: 0.0484109\ttotal: 472ms\tremaining: 356ms\n","570:\tlearn: 0.0482548\ttotal: 473ms\tremaining: 356ms\n","571:\tlearn: 0.0481559\ttotal: 475ms\tremaining: 355ms\n","572:\tlearn: 0.0480228\ttotal: 475ms\tremaining: 354ms\n","573:\tlearn: 0.0478988\ttotal: 476ms\tremaining: 353ms\n","574:\tlearn: 0.0477772\ttotal: 476ms\tremaining: 352ms\n","575:\tlearn: 0.0476260\ttotal: 476ms\tremaining: 351ms\n","576:\tlearn: 0.0475005\ttotal: 477ms\tremaining: 350ms\n","577:\tlearn: 0.0474047\ttotal: 477ms\tremaining: 348ms\n","578:\tlearn: 0.0472616\ttotal: 481ms\tremaining: 350ms\n","579:\tlearn: 0.0471526\ttotal: 482ms\tremaining: 349ms\n","580:\tlearn: 0.0470154\ttotal: 484ms\tremaining: 349ms\n","581:\tlearn: 0.0468938\ttotal: 485ms\tremaining: 348ms\n","582:\tlearn: 0.0467787\ttotal: 487ms\tremaining: 348ms\n","583:\tlearn: 0.0466602\ttotal: 487ms\tremaining: 347ms\n","584:\tlearn: 0.0465301\ttotal: 488ms\tremaining: 346ms\n","585:\tlearn: 0.0464474\ttotal: 489ms\tremaining: 346ms\n","586:\tlearn: 0.0463251\ttotal: 490ms\tremaining: 345ms\n","587:\tlearn: 0.0462158\ttotal: 491ms\tremaining: 344ms\n","588:\tlearn: 0.0460276\ttotal: 491ms\tremaining: 343ms\n","589:\tlearn: 0.0458499\ttotal: 496ms\tremaining: 345ms\n","590:\tlearn: 0.0457147\ttotal: 497ms\tremaining: 344ms\n","591:\tlearn: 0.0456057\ttotal: 499ms\tremaining: 344ms\n","592:\tlearn: 0.0454812\ttotal: 500ms\tremaining: 343ms\n","593:\tlearn: 0.0453873\ttotal: 501ms\tremaining: 343ms\n","594:\tlearn: 0.0453152\ttotal: 503ms\tremaining: 342ms\n","595:\tlearn: 0.0452115\ttotal: 504ms\tremaining: 341ms\n","596:\tlearn: 0.0450729\ttotal: 505ms\tremaining: 341ms\n","597:\tlearn: 0.0449616\ttotal: 506ms\tremaining: 340ms\n","598:\tlearn: 0.0448582\ttotal: 507ms\tremaining: 340ms\n","599:\tlearn: 0.0447197\ttotal: 508ms\tremaining: 339ms\n","600:\tlearn: 0.0445991\ttotal: 509ms\tremaining: 338ms\n","601:\tlearn: 0.0445117\ttotal: 510ms\tremaining: 337ms\n","602:\tlearn: 0.0443860\ttotal: 512ms\tremaining: 337ms\n","603:\tlearn: 0.0442830\ttotal: 513ms\tremaining: 336ms\n","604:\tlearn: 0.0441200\ttotal: 514ms\tremaining: 336ms\n","605:\tlearn: 0.0440384\ttotal: 516ms\tremaining: 335ms\n","606:\tlearn: 0.0439555\ttotal: 517ms\tremaining: 335ms\n","607:\tlearn: 0.0438553\ttotal: 517ms\tremaining: 334ms\n","608:\tlearn: 0.0437458\ttotal: 518ms\tremaining: 332ms\n","609:\tlearn: 0.0436090\ttotal: 518ms\tremaining: 331ms\n","610:\tlearn: 0.0435062\ttotal: 519ms\tremaining: 330ms\n","611:\tlearn: 0.0434537\ttotal: 519ms\tremaining: 329ms\n","612:\tlearn: 0.0433557\ttotal: 520ms\tremaining: 328ms\n","613:\tlearn: 0.0432543\ttotal: 520ms\tremaining: 327ms\n","614:\tlearn: 0.0431311\ttotal: 520ms\tremaining: 326ms\n","615:\tlearn: 0.0429668\ttotal: 521ms\tremaining: 325ms\n","616:\tlearn: 0.0428073\ttotal: 521ms\tremaining: 324ms\n","617:\tlearn: 0.0427307\ttotal: 522ms\tremaining: 322ms\n","618:\tlearn: 0.0426562\ttotal: 525ms\tremaining: 323ms\n","619:\tlearn: 0.0425415\ttotal: 525ms\tremaining: 322ms\n","620:\tlearn: 0.0424339\ttotal: 526ms\tremaining: 321ms\n","621:\tlearn: 0.0423363\ttotal: 526ms\tremaining: 320ms\n","622:\tlearn: 0.0422231\ttotal: 527ms\tremaining: 319ms\n","623:\tlearn: 0.0421087\ttotal: 527ms\tremaining: 318ms\n","624:\tlearn: 0.0420424\ttotal: 527ms\tremaining: 316ms\n","625:\tlearn: 0.0419599\ttotal: 528ms\tremaining: 315ms\n","626:\tlearn: 0.0418651\ttotal: 528ms\tremaining: 314ms\n","627:\tlearn: 0.0417407\ttotal: 529ms\tremaining: 313ms\n","628:\tlearn: 0.0416709\ttotal: 529ms\tremaining: 312ms\n","629:\tlearn: 0.0416137\ttotal: 529ms\tremaining: 311ms\n","630:\tlearn: 0.0415174\ttotal: 530ms\tremaining: 310ms\n","631:\tlearn: 0.0414116\ttotal: 530ms\tremaining: 309ms\n","632:\tlearn: 0.0413231\ttotal: 531ms\tremaining: 308ms\n","633:\tlearn: 0.0412065\ttotal: 531ms\tremaining: 307ms\n","634:\tlearn: 0.0411276\ttotal: 531ms\tremaining: 305ms\n","635:\tlearn: 0.0410015\ttotal: 532ms\tremaining: 304ms\n","636:\tlearn: 0.0408708\ttotal: 532ms\tremaining: 303ms\n","637:\tlearn: 0.0407237\ttotal: 533ms\tremaining: 302ms\n","638:\tlearn: 0.0406375\ttotal: 533ms\tremaining: 301ms\n","639:\tlearn: 0.0405470\ttotal: 533ms\tremaining: 300ms\n","640:\tlearn: 0.0404547\ttotal: 534ms\tremaining: 299ms\n","641:\tlearn: 0.0403569\ttotal: 534ms\tremaining: 298ms\n","642:\tlearn: 0.0402608\ttotal: 535ms\tremaining: 297ms\n","643:\tlearn: 0.0401774\ttotal: 535ms\tremaining: 296ms\n","644:\tlearn: 0.0400990\ttotal: 535ms\tremaining: 295ms\n","645:\tlearn: 0.0400328\ttotal: 536ms\tremaining: 293ms\n","646:\tlearn: 0.0399609\ttotal: 536ms\tremaining: 292ms\n","647:\tlearn: 0.0398656\ttotal: 536ms\tremaining: 291ms\n","648:\tlearn: 0.0397915\ttotal: 537ms\tremaining: 290ms\n","649:\tlearn: 0.0396961\ttotal: 537ms\tremaining: 289ms\n","650:\tlearn: 0.0396116\ttotal: 538ms\tremaining: 288ms\n","651:\tlearn: 0.0395157\ttotal: 538ms\tremaining: 287ms\n","652:\tlearn: 0.0393892\ttotal: 539ms\tremaining: 286ms\n","653:\tlearn: 0.0393111\ttotal: 539ms\tremaining: 285ms\n","654:\tlearn: 0.0391947\ttotal: 540ms\tremaining: 284ms\n","655:\tlearn: 0.0391179\ttotal: 540ms\tremaining: 283ms\n","656:\tlearn: 0.0390292\ttotal: 541ms\tremaining: 282ms\n","657:\tlearn: 0.0389319\ttotal: 541ms\tremaining: 281ms\n","658:\tlearn: 0.0388331\ttotal: 542ms\tremaining: 280ms\n","659:\tlearn: 0.0387339\ttotal: 542ms\tremaining: 279ms\n","660:\tlearn: 0.0386410\ttotal: 542ms\tremaining: 278ms\n","661:\tlearn: 0.0385478\ttotal: 543ms\tremaining: 277ms\n","662:\tlearn: 0.0384354\ttotal: 543ms\tremaining: 276ms\n","663:\tlearn: 0.0383421\ttotal: 544ms\tremaining: 275ms\n","664:\tlearn: 0.0382776\ttotal: 544ms\tremaining: 274ms\n","665:\tlearn: 0.0382229\ttotal: 544ms\tremaining: 273ms\n","666:\tlearn: 0.0381534\ttotal: 545ms\tremaining: 272ms\n","667:\tlearn: 0.0380032\ttotal: 545ms\tremaining: 271ms\n","668:\tlearn: 0.0379260\ttotal: 546ms\tremaining: 270ms\n","669:\tlearn: 0.0378670\ttotal: 546ms\tremaining: 269ms\n","670:\tlearn: 0.0377834\ttotal: 546ms\tremaining: 268ms\n","671:\tlearn: 0.0377214\ttotal: 547ms\tremaining: 267ms\n","672:\tlearn: 0.0376281\ttotal: 547ms\tremaining: 266ms\n","673:\tlearn: 0.0375254\ttotal: 548ms\tremaining: 265ms\n","674:\tlearn: 0.0374146\ttotal: 548ms\tremaining: 264ms\n","675:\tlearn: 0.0373654\ttotal: 548ms\tremaining: 263ms\n","676:\tlearn: 0.0372478\ttotal: 549ms\tremaining: 262ms\n","677:\tlearn: 0.0371636\ttotal: 549ms\tremaining: 261ms\n","678:\tlearn: 0.0370949\ttotal: 550ms\tremaining: 260ms\n","679:\tlearn: 0.0369862\ttotal: 550ms\tremaining: 259ms\n","680:\tlearn: 0.0369181\ttotal: 550ms\tremaining: 258ms\n","681:\tlearn: 0.0368245\ttotal: 551ms\tremaining: 257ms\n","682:\tlearn: 0.0367641\ttotal: 553ms\tremaining: 257ms\n","683:\tlearn: 0.0366683\ttotal: 554ms\tremaining: 256ms\n","684:\tlearn: 0.0365788\ttotal: 554ms\tremaining: 255ms\n","685:\tlearn: 0.0365078\ttotal: 555ms\tremaining: 254ms\n","686:\tlearn: 0.0364510\ttotal: 555ms\tremaining: 253ms\n","687:\tlearn: 0.0363945\ttotal: 555ms\tremaining: 252ms\n","688:\tlearn: 0.0363012\ttotal: 556ms\tremaining: 251ms\n","689:\tlearn: 0.0362211\ttotal: 556ms\tremaining: 250ms\n","690:\tlearn: 0.0361560\ttotal: 556ms\tremaining: 249ms\n","691:\tlearn: 0.0360522\ttotal: 557ms\tremaining: 248ms\n","692:\tlearn: 0.0359398\ttotal: 557ms\tremaining: 247ms\n","693:\tlearn: 0.0358262\ttotal: 558ms\tremaining: 246ms\n","694:\tlearn: 0.0357642\ttotal: 558ms\tremaining: 245ms\n","695:\tlearn: 0.0356719\ttotal: 558ms\tremaining: 244ms\n","696:\tlearn: 0.0355902\ttotal: 559ms\tremaining: 243ms\n","697:\tlearn: 0.0355069\ttotal: 559ms\tremaining: 242ms\n","698:\tlearn: 0.0354354\ttotal: 559ms\tremaining: 241ms\n","699:\tlearn: 0.0353316\ttotal: 560ms\tremaining: 240ms\n","700:\tlearn: 0.0352614\ttotal: 560ms\tremaining: 239ms\n","701:\tlearn: 0.0351905\ttotal: 561ms\tremaining: 238ms\n","702:\tlearn: 0.0351187\ttotal: 561ms\tremaining: 237ms\n","703:\tlearn: 0.0350571\ttotal: 562ms\tremaining: 236ms\n","704:\tlearn: 0.0349891\ttotal: 562ms\tremaining: 235ms\n","705:\tlearn: 0.0349174\ttotal: 562ms\tremaining: 234ms\n","706:\tlearn: 0.0348538\ttotal: 563ms\tremaining: 233ms\n","707:\tlearn: 0.0347817\ttotal: 563ms\tremaining: 232ms\n","708:\tlearn: 0.0347062\ttotal: 564ms\tremaining: 231ms\n","709:\tlearn: 0.0346308\ttotal: 564ms\tremaining: 230ms\n","710:\tlearn: 0.0345547\ttotal: 565ms\tremaining: 229ms\n","711:\tlearn: 0.0344902\ttotal: 565ms\tremaining: 229ms\n","712:\tlearn: 0.0344200\ttotal: 566ms\tremaining: 228ms\n","713:\tlearn: 0.0343605\ttotal: 566ms\tremaining: 227ms\n","714:\tlearn: 0.0342486\ttotal: 566ms\tremaining: 226ms\n","715:\tlearn: 0.0341740\ttotal: 567ms\tremaining: 225ms\n","716:\tlearn: 0.0341024\ttotal: 567ms\tremaining: 224ms\n","717:\tlearn: 0.0340338\ttotal: 568ms\tremaining: 223ms\n","718:\tlearn: 0.0339759\ttotal: 568ms\tremaining: 222ms\n","719:\tlearn: 0.0339113\ttotal: 568ms\tremaining: 221ms\n","720:\tlearn: 0.0337861\ttotal: 569ms\tremaining: 220ms\n","721:\tlearn: 0.0337222\ttotal: 569ms\tremaining: 219ms\n","722:\tlearn: 0.0336503\ttotal: 569ms\tremaining: 218ms\n","723:\tlearn: 0.0336115\ttotal: 570ms\tremaining: 217ms\n","724:\tlearn: 0.0335464\ttotal: 570ms\tremaining: 216ms\n","725:\tlearn: 0.0334642\ttotal: 571ms\tremaining: 215ms\n","726:\tlearn: 0.0333851\ttotal: 571ms\tremaining: 214ms\n","727:\tlearn: 0.0333085\ttotal: 572ms\tremaining: 214ms\n","728:\tlearn: 0.0332391\ttotal: 572ms\tremaining: 213ms\n","729:\tlearn: 0.0331686\ttotal: 572ms\tremaining: 212ms\n","730:\tlearn: 0.0330469\ttotal: 573ms\tremaining: 211ms\n","731:\tlearn: 0.0329349\ttotal: 573ms\tremaining: 210ms\n","732:\tlearn: 0.0328585\ttotal: 573ms\tremaining: 209ms\n","733:\tlearn: 0.0327848\ttotal: 574ms\tremaining: 208ms\n","734:\tlearn: 0.0327383\ttotal: 574ms\tremaining: 207ms\n","735:\tlearn: 0.0326786\ttotal: 575ms\tremaining: 206ms\n","736:\tlearn: 0.0326262\ttotal: 575ms\tremaining: 205ms\n","737:\tlearn: 0.0325857\ttotal: 576ms\tremaining: 204ms\n","738:\tlearn: 0.0325164\ttotal: 576ms\tremaining: 203ms\n","739:\tlearn: 0.0323995\ttotal: 576ms\tremaining: 203ms\n","740:\tlearn: 0.0323208\ttotal: 577ms\tremaining: 202ms\n","741:\tlearn: 0.0322304\ttotal: 577ms\tremaining: 201ms\n","742:\tlearn: 0.0321151\ttotal: 577ms\tremaining: 200ms\n","743:\tlearn: 0.0320429\ttotal: 578ms\tremaining: 199ms\n","744:\tlearn: 0.0319748\ttotal: 578ms\tremaining: 198ms\n","745:\tlearn: 0.0319106\ttotal: 579ms\tremaining: 197ms\n","746:\tlearn: 0.0318435\ttotal: 579ms\tremaining: 196ms\n","747:\tlearn: 0.0317869\ttotal: 579ms\tremaining: 195ms\n","748:\tlearn: 0.0317093\ttotal: 580ms\tremaining: 194ms\n","749:\tlearn: 0.0316501\ttotal: 580ms\tremaining: 193ms\n","750:\tlearn: 0.0315543\ttotal: 581ms\tremaining: 193ms\n","751:\tlearn: 0.0314987\ttotal: 581ms\tremaining: 192ms\n","752:\tlearn: 0.0314511\ttotal: 582ms\tremaining: 191ms\n","753:\tlearn: 0.0313676\ttotal: 582ms\tremaining: 190ms\n","754:\tlearn: 0.0313178\ttotal: 583ms\tremaining: 189ms\n","755:\tlearn: 0.0312710\ttotal: 583ms\tremaining: 188ms\n","756:\tlearn: 0.0312141\ttotal: 584ms\tremaining: 187ms\n","757:\tlearn: 0.0311530\ttotal: 584ms\tremaining: 186ms\n","758:\tlearn: 0.0310974\ttotal: 585ms\tremaining: 186ms\n","759:\tlearn: 0.0310351\ttotal: 585ms\tremaining: 185ms\n","760:\tlearn: 0.0309614\ttotal: 586ms\tremaining: 184ms\n","761:\tlearn: 0.0308933\ttotal: 586ms\tremaining: 183ms\n","762:\tlearn: 0.0308409\ttotal: 587ms\tremaining: 182ms\n","763:\tlearn: 0.0307822\ttotal: 587ms\tremaining: 181ms\n","764:\tlearn: 0.0307262\ttotal: 588ms\tremaining: 181ms\n","765:\tlearn: 0.0306781\ttotal: 588ms\tremaining: 180ms\n","766:\tlearn: 0.0306191\ttotal: 589ms\tremaining: 179ms\n","767:\tlearn: 0.0305637\ttotal: 589ms\tremaining: 178ms\n","768:\tlearn: 0.0305014\ttotal: 590ms\tremaining: 177ms\n","769:\tlearn: 0.0304454\ttotal: 591ms\tremaining: 176ms\n","770:\tlearn: 0.0303867\ttotal: 591ms\tremaining: 176ms\n","771:\tlearn: 0.0303133\ttotal: 592ms\tremaining: 175ms\n","772:\tlearn: 0.0302558\ttotal: 592ms\tremaining: 174ms\n","773:\tlearn: 0.0301759\ttotal: 593ms\tremaining: 173ms\n","774:\tlearn: 0.0301210\ttotal: 593ms\tremaining: 172ms\n","775:\tlearn: 0.0300498\ttotal: 594ms\tremaining: 171ms\n","776:\tlearn: 0.0299955\ttotal: 594ms\tremaining: 171ms\n","777:\tlearn: 0.0298920\ttotal: 595ms\tremaining: 170ms\n","778:\tlearn: 0.0298403\ttotal: 595ms\tremaining: 169ms\n","779:\tlearn: 0.0297563\ttotal: 596ms\tremaining: 168ms\n","780:\tlearn: 0.0296984\ttotal: 597ms\tremaining: 167ms\n","781:\tlearn: 0.0296309\ttotal: 597ms\tremaining: 166ms\n","782:\tlearn: 0.0295668\ttotal: 601ms\tremaining: 167ms\n","783:\tlearn: 0.0295046\ttotal: 602ms\tremaining: 166ms\n","784:\tlearn: 0.0294597\ttotal: 602ms\tremaining: 165ms\n","785:\tlearn: 0.0293923\ttotal: 603ms\tremaining: 164ms\n","786:\tlearn: 0.0293295\ttotal: 604ms\tremaining: 163ms\n","787:\tlearn: 0.0292704\ttotal: 604ms\tremaining: 163ms\n","788:\tlearn: 0.0292191\ttotal: 605ms\tremaining: 162ms\n","789:\tlearn: 0.0291622\ttotal: 606ms\tremaining: 161ms\n","790:\tlearn: 0.0290860\ttotal: 606ms\tremaining: 160ms\n","791:\tlearn: 0.0290353\ttotal: 607ms\tremaining: 160ms\n","792:\tlearn: 0.0289569\ttotal: 608ms\tremaining: 159ms\n","793:\tlearn: 0.0288943\ttotal: 609ms\tremaining: 158ms\n","794:\tlearn: 0.0288225\ttotal: 614ms\tremaining: 158ms\n","795:\tlearn: 0.0287731\ttotal: 615ms\tremaining: 158ms\n","796:\tlearn: 0.0287111\ttotal: 616ms\tremaining: 157ms\n","797:\tlearn: 0.0286572\ttotal: 617ms\tremaining: 156ms\n","798:\tlearn: 0.0285998\ttotal: 617ms\tremaining: 155ms\n","799:\tlearn: 0.0285305\ttotal: 618ms\tremaining: 154ms\n","800:\tlearn: 0.0284954\ttotal: 618ms\tremaining: 154ms\n","801:\tlearn: 0.0284436\ttotal: 619ms\tremaining: 153ms\n","802:\tlearn: 0.0284064\ttotal: 620ms\tremaining: 152ms\n","803:\tlearn: 0.0283453\ttotal: 620ms\tremaining: 151ms\n","804:\tlearn: 0.0283048\ttotal: 621ms\tremaining: 150ms\n","805:\tlearn: 0.0282672\ttotal: 622ms\tremaining: 150ms\n","806:\tlearn: 0.0282330\ttotal: 622ms\tremaining: 149ms\n","807:\tlearn: 0.0281772\ttotal: 623ms\tremaining: 148ms\n","808:\tlearn: 0.0281152\ttotal: 624ms\tremaining: 147ms\n","809:\tlearn: 0.0280748\ttotal: 624ms\tremaining: 146ms\n","810:\tlearn: 0.0280094\ttotal: 625ms\tremaining: 146ms\n","811:\tlearn: 0.0279463\ttotal: 625ms\tremaining: 145ms\n","812:\tlearn: 0.0278873\ttotal: 626ms\tremaining: 144ms\n","813:\tlearn: 0.0278359\ttotal: 627ms\tremaining: 143ms\n","814:\tlearn: 0.0277729\ttotal: 628ms\tremaining: 142ms\n","815:\tlearn: 0.0277211\ttotal: 628ms\tremaining: 142ms\n","816:\tlearn: 0.0276725\ttotal: 629ms\tremaining: 141ms\n","817:\tlearn: 0.0276278\ttotal: 630ms\tremaining: 140ms\n","818:\tlearn: 0.0275752\ttotal: 630ms\tremaining: 139ms\n","819:\tlearn: 0.0275342\ttotal: 631ms\tremaining: 139ms\n","820:\tlearn: 0.0274814\ttotal: 631ms\tremaining: 138ms\n","821:\tlearn: 0.0274232\ttotal: 632ms\tremaining: 137ms\n","822:\tlearn: 0.0273798\ttotal: 633ms\tremaining: 136ms\n","823:\tlearn: 0.0273293\ttotal: 633ms\tremaining: 135ms\n","824:\tlearn: 0.0272985\ttotal: 634ms\tremaining: 134ms\n","825:\tlearn: 0.0272426\ttotal: 634ms\tremaining: 134ms\n","826:\tlearn: 0.0271963\ttotal: 635ms\tremaining: 133ms\n","827:\tlearn: 0.0271287\ttotal: 636ms\tremaining: 132ms\n","828:\tlearn: 0.0270676\ttotal: 636ms\tremaining: 131ms\n","829:\tlearn: 0.0269995\ttotal: 638ms\tremaining: 131ms\n","830:\tlearn: 0.0269436\ttotal: 638ms\tremaining: 130ms\n","831:\tlearn: 0.0268790\ttotal: 639ms\tremaining: 129ms\n","832:\tlearn: 0.0268363\ttotal: 639ms\tremaining: 128ms\n","833:\tlearn: 0.0267826\ttotal: 640ms\tremaining: 127ms\n","834:\tlearn: 0.0267398\ttotal: 640ms\tremaining: 127ms\n","835:\tlearn: 0.0267004\ttotal: 641ms\tremaining: 126ms\n","836:\tlearn: 0.0266669\ttotal: 641ms\tremaining: 125ms\n","837:\tlearn: 0.0266129\ttotal: 642ms\tremaining: 124ms\n","838:\tlearn: 0.0265661\ttotal: 642ms\tremaining: 123ms\n","839:\tlearn: 0.0265109\ttotal: 643ms\tremaining: 122ms\n","840:\tlearn: 0.0264643\ttotal: 644ms\tremaining: 122ms\n","841:\tlearn: 0.0264223\ttotal: 644ms\tremaining: 121ms\n","842:\tlearn: 0.0263772\ttotal: 645ms\tremaining: 120ms\n","843:\tlearn: 0.0263333\ttotal: 646ms\tremaining: 119ms\n","844:\tlearn: 0.0262863\ttotal: 646ms\tremaining: 119ms\n","845:\tlearn: 0.0262420\ttotal: 647ms\tremaining: 118ms\n","846:\tlearn: 0.0261857\ttotal: 647ms\tremaining: 117ms\n","847:\tlearn: 0.0261447\ttotal: 648ms\tremaining: 116ms\n","848:\tlearn: 0.0261067\ttotal: 648ms\tremaining: 115ms\n","849:\tlearn: 0.0260696\ttotal: 649ms\tremaining: 115ms\n","850:\tlearn: 0.0260305\ttotal: 650ms\tremaining: 114ms\n","851:\tlearn: 0.0259704\ttotal: 650ms\tremaining: 113ms\n","852:\tlearn: 0.0259312\ttotal: 651ms\tremaining: 112ms\n","853:\tlearn: 0.0258808\ttotal: 651ms\tremaining: 111ms\n","854:\tlearn: 0.0258350\ttotal: 652ms\tremaining: 111ms\n","855:\tlearn: 0.0257913\ttotal: 653ms\tremaining: 110ms\n","856:\tlearn: 0.0257449\ttotal: 653ms\tremaining: 109ms\n","857:\tlearn: 0.0256954\ttotal: 654ms\tremaining: 108ms\n","858:\tlearn: 0.0256422\ttotal: 654ms\tremaining: 107ms\n","859:\tlearn: 0.0255727\ttotal: 655ms\tremaining: 107ms\n","860:\tlearn: 0.0255429\ttotal: 655ms\tremaining: 106ms\n","861:\tlearn: 0.0254977\ttotal: 656ms\tremaining: 105ms\n","862:\tlearn: 0.0254590\ttotal: 656ms\tremaining: 104ms\n","863:\tlearn: 0.0254201\ttotal: 657ms\tremaining: 103ms\n","864:\tlearn: 0.0253765\ttotal: 657ms\tremaining: 103ms\n","865:\tlearn: 0.0253341\ttotal: 658ms\tremaining: 102ms\n","866:\tlearn: 0.0252948\ttotal: 658ms\tremaining: 101ms\n","867:\tlearn: 0.0252618\ttotal: 659ms\tremaining: 100ms\n","868:\tlearn: 0.0252084\ttotal: 659ms\tremaining: 99.4ms\n","869:\tlearn: 0.0251665\ttotal: 660ms\tremaining: 98.7ms\n","870:\tlearn: 0.0251147\ttotal: 661ms\tremaining: 97.9ms\n","871:\tlearn: 0.0250734\ttotal: 662ms\tremaining: 97.2ms\n","872:\tlearn: 0.0250121\ttotal: 662ms\tremaining: 96.4ms\n","873:\tlearn: 0.0249757\ttotal: 663ms\tremaining: 95.6ms\n","874:\tlearn: 0.0249300\ttotal: 667ms\tremaining: 95.3ms\n","875:\tlearn: 0.0248932\ttotal: 670ms\tremaining: 94.8ms\n","876:\tlearn: 0.0248474\ttotal: 672ms\tremaining: 94.3ms\n","877:\tlearn: 0.0248063\ttotal: 675ms\tremaining: 93.8ms\n","878:\tlearn: 0.0247782\ttotal: 677ms\tremaining: 93.2ms\n","879:\tlearn: 0.0247352\ttotal: 679ms\tremaining: 92.6ms\n","880:\tlearn: 0.0246786\ttotal: 680ms\tremaining: 91.8ms\n","881:\tlearn: 0.0246400\ttotal: 680ms\tremaining: 91ms\n","882:\tlearn: 0.0246091\ttotal: 680ms\tremaining: 90.1ms\n","883:\tlearn: 0.0245571\ttotal: 681ms\tremaining: 89.3ms\n","884:\tlearn: 0.0245191\ttotal: 681ms\tremaining: 88.5ms\n","885:\tlearn: 0.0244853\ttotal: 681ms\tremaining: 87.7ms\n","886:\tlearn: 0.0244275\ttotal: 682ms\tremaining: 86.9ms\n","887:\tlearn: 0.0243870\ttotal: 682ms\tremaining: 86.1ms\n","888:\tlearn: 0.0243470\ttotal: 683ms\tremaining: 85.3ms\n","889:\tlearn: 0.0242948\ttotal: 683ms\tremaining: 84.5ms\n","890:\tlearn: 0.0242447\ttotal: 684ms\tremaining: 83.6ms\n","891:\tlearn: 0.0242055\ttotal: 684ms\tremaining: 82.8ms\n","892:\tlearn: 0.0241652\ttotal: 685ms\tremaining: 82ms\n","893:\tlearn: 0.0241262\ttotal: 685ms\tremaining: 81.2ms\n","894:\tlearn: 0.0240837\ttotal: 686ms\tremaining: 80.4ms\n","895:\tlearn: 0.0240462\ttotal: 686ms\tremaining: 79.6ms\n","896:\tlearn: 0.0239921\ttotal: 687ms\tremaining: 78.8ms\n","897:\tlearn: 0.0239478\ttotal: 687ms\tremaining: 78ms\n","898:\tlearn: 0.0239147\ttotal: 688ms\tremaining: 77.2ms\n","899:\tlearn: 0.0238673\ttotal: 688ms\tremaining: 76.4ms\n","900:\tlearn: 0.0238311\ttotal: 688ms\tremaining: 75.6ms\n","901:\tlearn: 0.0237931\ttotal: 689ms\tremaining: 74.8ms\n","902:\tlearn: 0.0237584\ttotal: 689ms\tremaining: 74ms\n","903:\tlearn: 0.0237299\ttotal: 690ms\tremaining: 73.3ms\n","904:\tlearn: 0.0236955\ttotal: 690ms\tremaining: 72.5ms\n","905:\tlearn: 0.0236436\ttotal: 691ms\tremaining: 71.7ms\n","906:\tlearn: 0.0236197\ttotal: 691ms\tremaining: 70.9ms\n","907:\tlearn: 0.0235591\ttotal: 692ms\tremaining: 70.1ms\n","908:\tlearn: 0.0235243\ttotal: 692ms\tremaining: 69.3ms\n","909:\tlearn: 0.0234893\ttotal: 692ms\tremaining: 68.5ms\n","910:\tlearn: 0.0234563\ttotal: 693ms\tremaining: 67.7ms\n","911:\tlearn: 0.0233964\ttotal: 693ms\tremaining: 66.9ms\n","912:\tlearn: 0.0233505\ttotal: 698ms\tremaining: 66.5ms\n","913:\tlearn: 0.0233169\ttotal: 700ms\tremaining: 65.9ms\n","914:\tlearn: 0.0232775\ttotal: 702ms\tremaining: 65.3ms\n","915:\tlearn: 0.0232372\ttotal: 704ms\tremaining: 64.6ms\n","916:\tlearn: 0.0232094\ttotal: 707ms\tremaining: 64ms\n","917:\tlearn: 0.0231742\ttotal: 708ms\tremaining: 63.2ms\n","918:\tlearn: 0.0231412\ttotal: 709ms\tremaining: 62.5ms\n","919:\tlearn: 0.0231142\ttotal: 710ms\tremaining: 61.7ms\n","920:\tlearn: 0.0230823\ttotal: 710ms\tremaining: 60.9ms\n","921:\tlearn: 0.0230495\ttotal: 711ms\tremaining: 60.2ms\n","922:\tlearn: 0.0230050\ttotal: 712ms\tremaining: 59.4ms\n","923:\tlearn: 0.0229634\ttotal: 713ms\tremaining: 58.7ms\n","924:\tlearn: 0.0229243\ttotal: 715ms\tremaining: 57.9ms\n","925:\tlearn: 0.0228945\ttotal: 716ms\tremaining: 57.2ms\n","926:\tlearn: 0.0228554\ttotal: 718ms\tremaining: 56.5ms\n","927:\tlearn: 0.0228219\ttotal: 719ms\tremaining: 55.8ms\n","928:\tlearn: 0.0227866\ttotal: 721ms\tremaining: 55.1ms\n","929:\tlearn: 0.0227502\ttotal: 722ms\tremaining: 54.4ms\n","930:\tlearn: 0.0227071\ttotal: 724ms\tremaining: 53.6ms\n","931:\tlearn: 0.0226740\ttotal: 725ms\tremaining: 52.9ms\n","932:\tlearn: 0.0226136\ttotal: 726ms\tremaining: 52.2ms\n","933:\tlearn: 0.0225748\ttotal: 727ms\tremaining: 51.4ms\n","934:\tlearn: 0.0225224\ttotal: 729ms\tremaining: 50.6ms\n","935:\tlearn: 0.0224807\ttotal: 730ms\tremaining: 49.9ms\n","936:\tlearn: 0.0224486\ttotal: 734ms\tremaining: 49.4ms\n","937:\tlearn: 0.0224113\ttotal: 736ms\tremaining: 48.6ms\n","938:\tlearn: 0.0223781\ttotal: 737ms\tremaining: 47.9ms\n","939:\tlearn: 0.0223495\ttotal: 747ms\tremaining: 47.7ms\n","940:\tlearn: 0.0223092\ttotal: 752ms\tremaining: 47.1ms\n","941:\tlearn: 0.0222788\ttotal: 756ms\tremaining: 46.6ms\n","942:\tlearn: 0.0222403\ttotal: 757ms\tremaining: 45.8ms\n","943:\tlearn: 0.0222048\ttotal: 759ms\tremaining: 45ms\n","944:\tlearn: 0.0221412\ttotal: 760ms\tremaining: 44.2ms\n","945:\tlearn: 0.0220894\ttotal: 761ms\tremaining: 43.4ms\n","946:\tlearn: 0.0220520\ttotal: 764ms\tremaining: 42.7ms\n","947:\tlearn: 0.0220181\ttotal: 765ms\tremaining: 42ms\n","948:\tlearn: 0.0219824\ttotal: 766ms\tremaining: 41.2ms\n","949:\tlearn: 0.0219522\ttotal: 768ms\tremaining: 40.4ms\n","950:\tlearn: 0.0219304\ttotal: 769ms\tremaining: 39.6ms\n","951:\tlearn: 0.0219006\ttotal: 771ms\tremaining: 38.9ms\n","952:\tlearn: 0.0218696\ttotal: 772ms\tremaining: 38.1ms\n","953:\tlearn: 0.0218305\ttotal: 779ms\tremaining: 37.6ms\n","954:\tlearn: 0.0217961\ttotal: 779ms\tremaining: 36.7ms\n","955:\tlearn: 0.0217661\ttotal: 780ms\tremaining: 35.9ms\n","956:\tlearn: 0.0217295\ttotal: 781ms\tremaining: 35.1ms\n","957:\tlearn: 0.0216907\ttotal: 781ms\tremaining: 34.3ms\n","958:\tlearn: 0.0216614\ttotal: 782ms\tremaining: 33.4ms\n","959:\tlearn: 0.0216419\ttotal: 782ms\tremaining: 32.6ms\n","960:\tlearn: 0.0216186\ttotal: 783ms\tremaining: 31.8ms\n","961:\tlearn: 0.0215654\ttotal: 783ms\tremaining: 30.9ms\n","962:\tlearn: 0.0215362\ttotal: 784ms\tremaining: 30.1ms\n","963:\tlearn: 0.0214982\ttotal: 784ms\tremaining: 29.3ms\n","964:\tlearn: 0.0214671\ttotal: 785ms\tremaining: 28.5ms\n","965:\tlearn: 0.0214133\ttotal: 785ms\tremaining: 27.6ms\n","966:\tlearn: 0.0213779\ttotal: 786ms\tremaining: 26.8ms\n","967:\tlearn: 0.0213488\ttotal: 786ms\tremaining: 26ms\n","968:\tlearn: 0.0213130\ttotal: 787ms\tremaining: 25.2ms\n","969:\tlearn: 0.0212893\ttotal: 787ms\tremaining: 24.4ms\n","970:\tlearn: 0.0212571\ttotal: 788ms\tremaining: 23.5ms\n","971:\tlearn: 0.0212259\ttotal: 789ms\tremaining: 22.7ms\n","972:\tlearn: 0.0212043\ttotal: 789ms\tremaining: 21.9ms\n","973:\tlearn: 0.0211756\ttotal: 791ms\tremaining: 21.1ms\n","974:\tlearn: 0.0211438\ttotal: 792ms\tremaining: 20.3ms\n","975:\tlearn: 0.0210976\ttotal: 792ms\tremaining: 19.5ms\n","976:\tlearn: 0.0210673\ttotal: 793ms\tremaining: 18.7ms\n","977:\tlearn: 0.0210414\ttotal: 795ms\tremaining: 17.9ms\n","978:\tlearn: 0.0210119\ttotal: 795ms\tremaining: 17.1ms\n","979:\tlearn: 0.0209920\ttotal: 795ms\tremaining: 16.2ms\n","980:\tlearn: 0.0209643\ttotal: 796ms\tremaining: 15.4ms\n","981:\tlearn: 0.0209214\ttotal: 796ms\tremaining: 14.6ms\n","982:\tlearn: 0.0208782\ttotal: 796ms\tremaining: 13.8ms\n","983:\tlearn: 0.0208558\ttotal: 797ms\tremaining: 13ms\n","984:\tlearn: 0.0208244\ttotal: 797ms\tremaining: 12.1ms\n","985:\tlearn: 0.0207892\ttotal: 798ms\tremaining: 11.3ms\n","986:\tlearn: 0.0207746\ttotal: 798ms\tremaining: 10.5ms\n","987:\tlearn: 0.0207352\ttotal: 799ms\tremaining: 9.7ms\n","988:\tlearn: 0.0206797\ttotal: 799ms\tremaining: 8.88ms\n","989:\tlearn: 0.0206459\ttotal: 799ms\tremaining: 8.07ms\n","990:\tlearn: 0.0206147\ttotal: 800ms\tremaining: 7.26ms\n","991:\tlearn: 0.0205864\ttotal: 800ms\tremaining: 6.45ms\n","992:\tlearn: 0.0205385\ttotal: 800ms\tremaining: 5.64ms\n","993:\tlearn: 0.0205071\ttotal: 801ms\tremaining: 4.83ms\n","994:\tlearn: 0.0204693\ttotal: 801ms\tremaining: 4.03ms\n","995:\tlearn: 0.0204377\ttotal: 802ms\tremaining: 3.22ms\n","996:\tlearn: 0.0204009\ttotal: 802ms\tremaining: 2.41ms\n","997:\tlearn: 0.0203751\ttotal: 802ms\tremaining: 1.61ms\n","998:\tlearn: 0.0203450\ttotal: 803ms\tremaining: 803us\n","999:\tlearn: 0.0203120\ttotal: 803ms\tremaining: 0us\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:05<00:00,  7.73it/s]\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1302: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  dtype=np.object)\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1489: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  sample_mask = np.ones((n_samples, ), dtype=np.bool)\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n"]},{"output_type":"stream","name":"stdout","text":["0.3 1.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n","/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/gradient_boosting.py:1162: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  assert sample_mask.dtype == np.bool\n"]},{"output_type":"stream","name":"stdout","text":["Learning rate set to 0.003504\n","0:\tlearn: 0.6872509\ttotal: 1.89ms\tremaining: 1.89s\n","1:\tlearn: 0.6820281\ttotal: 2.35ms\tremaining: 1.17s\n","2:\tlearn: 0.6765236\ttotal: 3.37ms\tremaining: 1.12s\n","3:\tlearn: 0.6716571\ttotal: 4.32ms\tremaining: 1.07s\n","4:\tlearn: 0.6662642\ttotal: 5.18ms\tremaining: 1.03s\n","5:\tlearn: 0.6624546\ttotal: 6ms\tremaining: 995ms\n","6:\tlearn: 0.6573688\ttotal: 6.82ms\tremaining: 967ms\n","7:\tlearn: 0.6526367\ttotal: 7.59ms\tremaining: 942ms\n","8:\tlearn: 0.6478181\ttotal: 8.14ms\tremaining: 896ms\n","9:\tlearn: 0.6428728\ttotal: 8.79ms\tremaining: 870ms\n","10:\tlearn: 0.6380595\ttotal: 9.41ms\tremaining: 846ms\n","11:\tlearn: 0.6326962\ttotal: 10.1ms\tremaining: 830ms\n","12:\tlearn: 0.6274643\ttotal: 10.8ms\tremaining: 819ms\n","13:\tlearn: 0.6221397\ttotal: 11.5ms\tremaining: 808ms\n","14:\tlearn: 0.6170258\ttotal: 12.2ms\tremaining: 800ms\n","15:\tlearn: 0.6126683\ttotal: 12.9ms\tremaining: 791ms\n","16:\tlearn: 0.6079449\ttotal: 13.6ms\tremaining: 785ms\n","17:\tlearn: 0.6032049\ttotal: 18ms\tremaining: 981ms\n","18:\tlearn: 0.5987289\ttotal: 18.5ms\tremaining: 954ms\n","19:\tlearn: 0.5936055\ttotal: 20.1ms\tremaining: 984ms\n","20:\tlearn: 0.5889071\ttotal: 20.6ms\tremaining: 959ms\n","21:\tlearn: 0.5844200\ttotal: 21.2ms\tremaining: 945ms\n","22:\tlearn: 0.5801950\ttotal: 21.9ms\tremaining: 931ms\n","23:\tlearn: 0.5751461\ttotal: 22.5ms\tremaining: 914ms\n","24:\tlearn: 0.5708670\ttotal: 22.9ms\tremaining: 893ms\n","25:\tlearn: 0.5663074\ttotal: 23.3ms\tremaining: 873ms\n","26:\tlearn: 0.5617777\ttotal: 23.7ms\tremaining: 856ms\n","27:\tlearn: 0.5575577\ttotal: 24.6ms\tremaining: 854ms\n","28:\tlearn: 0.5539024\ttotal: 25ms\tremaining: 838ms\n","29:\tlearn: 0.5490081\ttotal: 25.3ms\tremaining: 818ms\n","30:\tlearn: 0.5450974\ttotal: 25.7ms\tremaining: 804ms\n","31:\tlearn: 0.5416382\ttotal: 26.1ms\tremaining: 791ms\n","32:\tlearn: 0.5378149\ttotal: 26.6ms\tremaining: 778ms\n","33:\tlearn: 0.5329383\ttotal: 27.2ms\tremaining: 773ms\n","34:\tlearn: 0.5284423\ttotal: 27.6ms\tremaining: 761ms\n","35:\tlearn: 0.5240468\ttotal: 28ms\tremaining: 749ms\n","36:\tlearn: 0.5207795\ttotal: 28.4ms\tremaining: 738ms\n","37:\tlearn: 0.5178010\ttotal: 28.7ms\tremaining: 728ms\n","38:\tlearn: 0.5141089\ttotal: 29.2ms\tremaining: 720ms\n","39:\tlearn: 0.5103324\ttotal: 29.6ms\tremaining: 711ms\n","40:\tlearn: 0.5070826\ttotal: 30.4ms\tremaining: 711ms\n","41:\tlearn: 0.5032556\ttotal: 30.8ms\tremaining: 702ms\n","42:\tlearn: 0.4992318\ttotal: 31.2ms\tremaining: 694ms\n","43:\tlearn: 0.4952427\ttotal: 32ms\tremaining: 694ms\n","44:\tlearn: 0.4913688\ttotal: 32.8ms\tremaining: 695ms\n","45:\tlearn: 0.4874550\ttotal: 33.9ms\tremaining: 703ms\n","46:\tlearn: 0.4836085\ttotal: 34.4ms\tremaining: 697ms\n","47:\tlearn: 0.4806756\ttotal: 35ms\tremaining: 695ms\n","48:\tlearn: 0.4772519\ttotal: 35.3ms\tremaining: 685ms\n","49:\tlearn: 0.4742456\ttotal: 35.9ms\tremaining: 681ms\n","50:\tlearn: 0.4709414\ttotal: 36.4ms\tremaining: 677ms\n","51:\tlearn: 0.4674689\ttotal: 36.9ms\tremaining: 673ms\n","52:\tlearn: 0.4637435\ttotal: 37.4ms\tremaining: 668ms\n","53:\tlearn: 0.4608033\ttotal: 37.8ms\tremaining: 662ms\n","54:\tlearn: 0.4575192\ttotal: 38.5ms\tremaining: 662ms\n","55:\tlearn: 0.4538197\ttotal: 39ms\tremaining: 657ms\n","56:\tlearn: 0.4502594\ttotal: 39.4ms\tremaining: 652ms\n","57:\tlearn: 0.4466105\ttotal: 39.8ms\tremaining: 646ms\n","58:\tlearn: 0.4439483\ttotal: 40.3ms\tremaining: 642ms\n","59:\tlearn: 0.4413994\ttotal: 40.6ms\tremaining: 635ms\n","60:\tlearn: 0.4387311\ttotal: 41ms\tremaining: 631ms\n","61:\tlearn: 0.4347435\ttotal: 41.3ms\tremaining: 624ms\n","62:\tlearn: 0.4317918\ttotal: 41.8ms\tremaining: 622ms\n","63:\tlearn: 0.4291893\ttotal: 42.2ms\tremaining: 618ms\n","64:\tlearn: 0.4265306\ttotal: 42.7ms\tremaining: 614ms\n","65:\tlearn: 0.4236933\ttotal: 43.1ms\tremaining: 610ms\n","66:\tlearn: 0.4208259\ttotal: 43.5ms\tremaining: 606ms\n","67:\tlearn: 0.4186365\ttotal: 43.9ms\tremaining: 602ms\n","68:\tlearn: 0.4148036\ttotal: 44.2ms\tremaining: 597ms\n","69:\tlearn: 0.4123862\ttotal: 44.7ms\tremaining: 594ms\n","70:\tlearn: 0.4093546\ttotal: 45.1ms\tremaining: 590ms\n","71:\tlearn: 0.4064559\ttotal: 45.5ms\tremaining: 587ms\n","72:\tlearn: 0.4040967\ttotal: 45.9ms\tremaining: 583ms\n","73:\tlearn: 0.4008302\ttotal: 46.3ms\tremaining: 579ms\n","74:\tlearn: 0.3977888\ttotal: 47.5ms\tremaining: 586ms\n","75:\tlearn: 0.3953874\ttotal: 48.2ms\tremaining: 586ms\n","76:\tlearn: 0.3920324\ttotal: 48.9ms\tremaining: 586ms\n","77:\tlearn: 0.3898056\ttotal: 49.3ms\tremaining: 583ms\n","78:\tlearn: 0.3867264\ttotal: 49.8ms\tremaining: 580ms\n","79:\tlearn: 0.3837317\ttotal: 50.2ms\tremaining: 577ms\n","80:\tlearn: 0.3807024\ttotal: 51.2ms\tremaining: 581ms\n","81:\tlearn: 0.3781345\ttotal: 51.6ms\tremaining: 578ms\n","82:\tlearn: 0.3754320\ttotal: 52ms\tremaining: 575ms\n","83:\tlearn: 0.3730355\ttotal: 52.4ms\tremaining: 572ms\n","84:\tlearn: 0.3700533\ttotal: 52.8ms\tremaining: 568ms\n","85:\tlearn: 0.3680275\ttotal: 54.8ms\tremaining: 582ms\n","86:\tlearn: 0.3657013\ttotal: 55.4ms\tremaining: 581ms\n","87:\tlearn: 0.3626005\ttotal: 55.7ms\tremaining: 578ms\n","88:\tlearn: 0.3606679\ttotal: 60.9ms\tremaining: 623ms\n","89:\tlearn: 0.3586215\ttotal: 61.7ms\tremaining: 624ms\n","90:\tlearn: 0.3559647\ttotal: 63.8ms\tremaining: 638ms\n","91:\tlearn: 0.3533094\ttotal: 65.3ms\tremaining: 645ms\n","92:\tlearn: 0.3510687\ttotal: 66.5ms\tremaining: 649ms\n","93:\tlearn: 0.3488994\ttotal: 69.8ms\tremaining: 673ms\n","94:\tlearn: 0.3466414\ttotal: 70.7ms\tremaining: 673ms\n","95:\tlearn: 0.3441499\ttotal: 71.4ms\tremaining: 672ms\n","96:\tlearn: 0.3418122\ttotal: 72.2ms\tremaining: 672ms\n","97:\tlearn: 0.3393993\ttotal: 72.8ms\tremaining: 670ms\n","98:\tlearn: 0.3363999\ttotal: 73.3ms\tremaining: 667ms\n","99:\tlearn: 0.3342760\ttotal: 74.1ms\tremaining: 667ms\n","100:\tlearn: 0.3320604\ttotal: 75.8ms\tremaining: 675ms\n","101:\tlearn: 0.3299857\ttotal: 77.7ms\tremaining: 685ms\n","102:\tlearn: 0.3280606\ttotal: 79.3ms\tremaining: 691ms\n","103:\tlearn: 0.3261661\ttotal: 80.9ms\tremaining: 697ms\n","104:\tlearn: 0.3241730\ttotal: 81.5ms\tremaining: 695ms\n","105:\tlearn: 0.3218620\ttotal: 82ms\tremaining: 692ms\n","106:\tlearn: 0.3197202\ttotal: 82.3ms\tremaining: 687ms\n","107:\tlearn: 0.3176493\ttotal: 82.8ms\tremaining: 684ms\n","108:\tlearn: 0.3150595\ttotal: 83.3ms\tremaining: 681ms\n","109:\tlearn: 0.3130469\ttotal: 83.7ms\tremaining: 677ms\n","110:\tlearn: 0.3108692\ttotal: 84.1ms\tremaining: 674ms\n","111:\tlearn: 0.3087721\ttotal: 84.4ms\tremaining: 669ms\n","112:\tlearn: 0.3070178\ttotal: 84.9ms\tremaining: 666ms\n","113:\tlearn: 0.3046675\ttotal: 85.2ms\tremaining: 662ms\n","114:\tlearn: 0.3028117\ttotal: 85.4ms\tremaining: 657ms\n","115:\tlearn: 0.3004932\ttotal: 85.7ms\tremaining: 653ms\n","116:\tlearn: 0.2984926\ttotal: 86.1ms\tremaining: 650ms\n","117:\tlearn: 0.2963558\ttotal: 86.5ms\tremaining: 646ms\n","118:\tlearn: 0.2944385\ttotal: 86.9ms\tremaining: 643ms\n","119:\tlearn: 0.2926631\ttotal: 87.1ms\tremaining: 639ms\n","120:\tlearn: 0.2910756\ttotal: 87.5ms\tremaining: 636ms\n","121:\tlearn: 0.2888946\ttotal: 87.9ms\tremaining: 633ms\n","122:\tlearn: 0.2870110\ttotal: 88.4ms\tremaining: 630ms\n","123:\tlearn: 0.2853142\ttotal: 88.7ms\tremaining: 627ms\n","124:\tlearn: 0.2838622\ttotal: 89.3ms\tremaining: 625ms\n","125:\tlearn: 0.2817721\ttotal: 89.8ms\tremaining: 623ms\n","126:\tlearn: 0.2799132\ttotal: 90.3ms\tremaining: 620ms\n","127:\tlearn: 0.2787327\ttotal: 95.7ms\tremaining: 652ms\n","128:\tlearn: 0.2773417\ttotal: 96.1ms\tremaining: 649ms\n","129:\tlearn: 0.2754084\ttotal: 96.5ms\tremaining: 645ms\n","130:\tlearn: 0.2736096\ttotal: 96.9ms\tremaining: 643ms\n","131:\tlearn: 0.2719664\ttotal: 97.3ms\tremaining: 640ms\n","132:\tlearn: 0.2697943\ttotal: 97.6ms\tremaining: 636ms\n","133:\tlearn: 0.2682670\ttotal: 99.4ms\tremaining: 642ms\n","134:\tlearn: 0.2669624\ttotal: 99.8ms\tremaining: 640ms\n","135:\tlearn: 0.2650164\ttotal: 101ms\tremaining: 643ms\n","136:\tlearn: 0.2634249\ttotal: 102ms\tremaining: 640ms\n","137:\tlearn: 0.2611359\ttotal: 102ms\tremaining: 637ms\n","138:\tlearn: 0.2601736\ttotal: 102ms\tremaining: 633ms\n","139:\tlearn: 0.2588569\ttotal: 106ms\tremaining: 650ms\n","140:\tlearn: 0.2573914\ttotal: 107ms\tremaining: 654ms\n","141:\tlearn: 0.2557655\ttotal: 108ms\tremaining: 651ms\n","142:\tlearn: 0.2541986\ttotal: 109ms\tremaining: 656ms\n","143:\tlearn: 0.2524779\ttotal: 110ms\tremaining: 653ms\n","144:\tlearn: 0.2510007\ttotal: 110ms\tremaining: 650ms\n","145:\tlearn: 0.2493638\ttotal: 111ms\tremaining: 648ms\n","146:\tlearn: 0.2475605\ttotal: 111ms\tremaining: 644ms\n","147:\tlearn: 0.2459018\ttotal: 111ms\tremaining: 642ms\n","148:\tlearn: 0.2445546\ttotal: 112ms\tremaining: 639ms\n","149:\tlearn: 0.2429558\ttotal: 112ms\tremaining: 636ms\n","150:\tlearn: 0.2414760\ttotal: 113ms\tremaining: 634ms\n","151:\tlearn: 0.2398046\ttotal: 113ms\tremaining: 632ms\n","152:\tlearn: 0.2386139\ttotal: 119ms\tremaining: 661ms\n","153:\tlearn: 0.2373220\ttotal: 120ms\tremaining: 658ms\n","154:\tlearn: 0.2358481\ttotal: 120ms\tremaining: 656ms\n","155:\tlearn: 0.2342222\ttotal: 121ms\tremaining: 653ms\n","156:\tlearn: 0.2330380\ttotal: 121ms\tremaining: 651ms\n","157:\tlearn: 0.2318366\ttotal: 122ms\tremaining: 648ms\n","158:\tlearn: 0.2304596\ttotal: 122ms\tremaining: 645ms\n","159:\tlearn: 0.2290344\ttotal: 122ms\tremaining: 643ms\n","160:\tlearn: 0.2277898\ttotal: 123ms\tremaining: 640ms\n","161:\tlearn: 0.2265165\ttotal: 123ms\tremaining: 637ms\n","162:\tlearn: 0.2251982\ttotal: 125ms\tremaining: 644ms\n","163:\tlearn: 0.2239285\ttotal: 126ms\tremaining: 642ms\n","164:\tlearn: 0.2227415\ttotal: 130ms\tremaining: 658ms\n","165:\tlearn: 0.2211969\ttotal: 130ms\tremaining: 655ms\n","166:\tlearn: 0.2196333\ttotal: 131ms\tremaining: 652ms\n","167:\tlearn: 0.2184808\ttotal: 131ms\tremaining: 649ms\n","168:\tlearn: 0.2169374\ttotal: 131ms\tremaining: 646ms\n","169:\tlearn: 0.2158458\ttotal: 132ms\tremaining: 643ms\n","170:\tlearn: 0.2144994\ttotal: 132ms\tremaining: 642ms\n","171:\tlearn: 0.2132406\ttotal: 135ms\tremaining: 652ms\n","172:\tlearn: 0.2117747\ttotal: 136ms\tremaining: 650ms\n","173:\tlearn: 0.2104856\ttotal: 140ms\tremaining: 665ms\n","174:\tlearn: 0.2090058\ttotal: 142ms\tremaining: 668ms\n","175:\tlearn: 0.2080559\ttotal: 143ms\tremaining: 668ms\n","176:\tlearn: 0.2068224\ttotal: 150ms\tremaining: 695ms\n","177:\tlearn: 0.2059394\ttotal: 151ms\tremaining: 696ms\n","178:\tlearn: 0.2046719\ttotal: 152ms\tremaining: 695ms\n","179:\tlearn: 0.2035511\ttotal: 152ms\tremaining: 694ms\n","180:\tlearn: 0.2022971\ttotal: 153ms\tremaining: 694ms\n","181:\tlearn: 0.2010523\ttotal: 155ms\tremaining: 695ms\n","182:\tlearn: 0.1997334\ttotal: 156ms\tremaining: 698ms\n","183:\tlearn: 0.1986813\ttotal: 158ms\tremaining: 702ms\n","184:\tlearn: 0.1978271\ttotal: 165ms\tremaining: 727ms\n","185:\tlearn: 0.1968238\ttotal: 166ms\tremaining: 727ms\n","186:\tlearn: 0.1956278\ttotal: 167ms\tremaining: 726ms\n","187:\tlearn: 0.1944911\ttotal: 168ms\tremaining: 726ms\n","188:\tlearn: 0.1932167\ttotal: 169ms\tremaining: 724ms\n","189:\tlearn: 0.1919969\ttotal: 170ms\tremaining: 725ms\n","190:\tlearn: 0.1906545\ttotal: 172ms\tremaining: 726ms\n","191:\tlearn: 0.1897078\ttotal: 173ms\tremaining: 728ms\n","192:\tlearn: 0.1884546\ttotal: 174ms\tremaining: 730ms\n","193:\tlearn: 0.1872928\ttotal: 181ms\tremaining: 750ms\n","194:\tlearn: 0.1862869\ttotal: 182ms\tremaining: 750ms\n","195:\tlearn: 0.1853658\ttotal: 182ms\tremaining: 748ms\n","196:\tlearn: 0.1843554\ttotal: 183ms\tremaining: 747ms\n","197:\tlearn: 0.1835190\ttotal: 184ms\tremaining: 746ms\n","198:\tlearn: 0.1825712\ttotal: 185ms\tremaining: 746ms\n","199:\tlearn: 0.1816762\ttotal: 187ms\tremaining: 747ms\n","200:\tlearn: 0.1809781\ttotal: 189ms\tremaining: 749ms\n","201:\tlearn: 0.1799797\ttotal: 190ms\tremaining: 751ms\n","202:\tlearn: 0.1791227\ttotal: 193ms\tremaining: 760ms\n","203:\tlearn: 0.1781043\ttotal: 195ms\tremaining: 762ms\n","204:\tlearn: 0.1770662\ttotal: 197ms\tremaining: 763ms\n","205:\tlearn: 0.1760263\ttotal: 198ms\tremaining: 764ms\n","206:\tlearn: 0.1750798\ttotal: 200ms\tremaining: 765ms\n","207:\tlearn: 0.1741398\ttotal: 202ms\tremaining: 768ms\n","208:\tlearn: 0.1732100\ttotal: 203ms\tremaining: 768ms\n","209:\tlearn: 0.1723618\ttotal: 204ms\tremaining: 769ms\n","210:\tlearn: 0.1713769\ttotal: 206ms\tremaining: 771ms\n","211:\tlearn: 0.1703674\ttotal: 208ms\tremaining: 772ms\n","212:\tlearn: 0.1697164\ttotal: 209ms\tremaining: 774ms\n","213:\tlearn: 0.1690502\ttotal: 210ms\tremaining: 771ms\n","214:\tlearn: 0.1681571\ttotal: 210ms\tremaining: 768ms\n","215:\tlearn: 0.1672038\ttotal: 211ms\tremaining: 766ms\n","216:\tlearn: 0.1663801\ttotal: 212ms\tremaining: 763ms\n","217:\tlearn: 0.1655013\ttotal: 212ms\tremaining: 760ms\n","218:\tlearn: 0.1645780\ttotal: 212ms\tremaining: 757ms\n","219:\tlearn: 0.1636487\ttotal: 213ms\tremaining: 754ms\n","220:\tlearn: 0.1628130\ttotal: 213ms\tremaining: 751ms\n","221:\tlearn: 0.1618926\ttotal: 213ms\tremaining: 748ms\n","222:\tlearn: 0.1610201\ttotal: 214ms\tremaining: 745ms\n","223:\tlearn: 0.1602423\ttotal: 219ms\tremaining: 760ms\n","224:\tlearn: 0.1594016\ttotal: 220ms\tremaining: 757ms\n","225:\tlearn: 0.1586340\ttotal: 220ms\tremaining: 754ms\n","226:\tlearn: 0.1577671\ttotal: 221ms\tremaining: 751ms\n","227:\tlearn: 0.1567842\ttotal: 221ms\tremaining: 748ms\n","228:\tlearn: 0.1557333\ttotal: 221ms\tremaining: 745ms\n","229:\tlearn: 0.1549539\ttotal: 222ms\tremaining: 742ms\n","230:\tlearn: 0.1542385\ttotal: 222ms\tremaining: 739ms\n","231:\tlearn: 0.1535145\ttotal: 222ms\tremaining: 736ms\n","232:\tlearn: 0.1525370\ttotal: 223ms\tremaining: 733ms\n","233:\tlearn: 0.1520477\ttotal: 223ms\tremaining: 730ms\n","234:\tlearn: 0.1513812\ttotal: 224ms\tremaining: 728ms\n","235:\tlearn: 0.1505378\ttotal: 224ms\tremaining: 726ms\n","236:\tlearn: 0.1497759\ttotal: 225ms\tremaining: 725ms\n","237:\tlearn: 0.1491117\ttotal: 226ms\tremaining: 724ms\n","238:\tlearn: 0.1483761\ttotal: 227ms\tremaining: 722ms\n","239:\tlearn: 0.1475702\ttotal: 227ms\tremaining: 719ms\n","240:\tlearn: 0.1467738\ttotal: 228ms\tremaining: 717ms\n","241:\tlearn: 0.1461662\ttotal: 228ms\tremaining: 715ms\n","242:\tlearn: 0.1455568\ttotal: 229ms\tremaining: 712ms\n","243:\tlearn: 0.1449175\ttotal: 229ms\tremaining: 710ms\n","244:\tlearn: 0.1442641\ttotal: 229ms\tremaining: 707ms\n","245:\tlearn: 0.1432701\ttotal: 230ms\tremaining: 704ms\n","246:\tlearn: 0.1426502\ttotal: 230ms\tremaining: 702ms\n","247:\tlearn: 0.1420796\ttotal: 230ms\tremaining: 699ms\n","248:\tlearn: 0.1414639\ttotal: 231ms\tremaining: 696ms\n","249:\tlearn: 0.1407829\ttotal: 231ms\tremaining: 693ms\n","250:\tlearn: 0.1401687\ttotal: 234ms\tremaining: 698ms\n","251:\tlearn: 0.1393116\ttotal: 234ms\tremaining: 696ms\n","252:\tlearn: 0.1387720\ttotal: 235ms\tremaining: 693ms\n","253:\tlearn: 0.1381725\ttotal: 235ms\tremaining: 691ms\n","254:\tlearn: 0.1375456\ttotal: 236ms\tremaining: 689ms\n","255:\tlearn: 0.1367700\ttotal: 237ms\tremaining: 688ms\n","256:\tlearn: 0.1360553\ttotal: 238ms\tremaining: 687ms\n","257:\tlearn: 0.1353763\ttotal: 238ms\tremaining: 685ms\n","258:\tlearn: 0.1344383\ttotal: 239ms\tremaining: 683ms\n","259:\tlearn: 0.1339340\ttotal: 239ms\tremaining: 681ms\n","260:\tlearn: 0.1332637\ttotal: 240ms\tremaining: 679ms\n","261:\tlearn: 0.1327621\ttotal: 240ms\tremaining: 677ms\n","262:\tlearn: 0.1320930\ttotal: 241ms\tremaining: 676ms\n","263:\tlearn: 0.1313754\ttotal: 242ms\tremaining: 674ms\n","264:\tlearn: 0.1308918\ttotal: 242ms\tremaining: 672ms\n","265:\tlearn: 0.1301215\ttotal: 243ms\tremaining: 669ms\n","266:\tlearn: 0.1293874\ttotal: 243ms\tremaining: 667ms\n","267:\tlearn: 0.1289855\ttotal: 244ms\tremaining: 668ms\n","268:\tlearn: 0.1282795\ttotal: 246ms\tremaining: 668ms\n","269:\tlearn: 0.1275310\ttotal: 247ms\tremaining: 668ms\n","270:\tlearn: 0.1270492\ttotal: 250ms\tremaining: 674ms\n","271:\tlearn: 0.1264837\ttotal: 251ms\tremaining: 672ms\n","272:\tlearn: 0.1258629\ttotal: 251ms\tremaining: 670ms\n","273:\tlearn: 0.1249699\ttotal: 252ms\tremaining: 667ms\n","274:\tlearn: 0.1245521\ttotal: 252ms\tremaining: 665ms\n","275:\tlearn: 0.1242012\ttotal: 252ms\tremaining: 662ms\n","276:\tlearn: 0.1235348\ttotal: 253ms\tremaining: 660ms\n","277:\tlearn: 0.1229095\ttotal: 253ms\tremaining: 658ms\n","278:\tlearn: 0.1222661\ttotal: 254ms\tremaining: 656ms\n","279:\tlearn: 0.1214854\ttotal: 254ms\tremaining: 654ms\n","280:\tlearn: 0.1209538\ttotal: 255ms\tremaining: 651ms\n","281:\tlearn: 0.1205432\ttotal: 255ms\tremaining: 649ms\n","282:\tlearn: 0.1200395\ttotal: 255ms\tremaining: 647ms\n","283:\tlearn: 0.1195565\ttotal: 256ms\tremaining: 645ms\n","284:\tlearn: 0.1189932\ttotal: 256ms\tremaining: 643ms\n","285:\tlearn: 0.1185343\ttotal: 257ms\tremaining: 641ms\n","286:\tlearn: 0.1178930\ttotal: 257ms\tremaining: 639ms\n","287:\tlearn: 0.1174261\ttotal: 258ms\tremaining: 637ms\n","288:\tlearn: 0.1168648\ttotal: 258ms\tremaining: 635ms\n","289:\tlearn: 0.1163328\ttotal: 258ms\tremaining: 633ms\n","290:\tlearn: 0.1157705\ttotal: 259ms\tremaining: 631ms\n","291:\tlearn: 0.1151397\ttotal: 259ms\tremaining: 629ms\n","292:\tlearn: 0.1146335\ttotal: 260ms\tremaining: 627ms\n","293:\tlearn: 0.1143201\ttotal: 265ms\tremaining: 635ms\n","294:\tlearn: 0.1138426\ttotal: 265ms\tremaining: 634ms\n","295:\tlearn: 0.1131938\ttotal: 266ms\tremaining: 632ms\n","296:\tlearn: 0.1126657\ttotal: 266ms\tremaining: 629ms\n","297:\tlearn: 0.1121927\ttotal: 269ms\tremaining: 633ms\n","298:\tlearn: 0.1116228\ttotal: 269ms\tremaining: 631ms\n","299:\tlearn: 0.1110844\ttotal: 272ms\tremaining: 634ms\n","300:\tlearn: 0.1108276\ttotal: 272ms\tremaining: 632ms\n","301:\tlearn: 0.1102714\ttotal: 272ms\tremaining: 630ms\n","302:\tlearn: 0.1098305\ttotal: 273ms\tremaining: 628ms\n","303:\tlearn: 0.1094743\ttotal: 273ms\tremaining: 625ms\n","304:\tlearn: 0.1090798\ttotal: 274ms\tremaining: 623ms\n","305:\tlearn: 0.1086301\ttotal: 274ms\tremaining: 621ms\n","306:\tlearn: 0.1081352\ttotal: 274ms\tremaining: 619ms\n","307:\tlearn: 0.1076778\ttotal: 275ms\tremaining: 617ms\n","308:\tlearn: 0.1070273\ttotal: 275ms\tremaining: 615ms\n","309:\tlearn: 0.1065263\ttotal: 276ms\tremaining: 613ms\n","310:\tlearn: 0.1060089\ttotal: 276ms\tremaining: 611ms\n","311:\tlearn: 0.1055679\ttotal: 276ms\tremaining: 609ms\n","312:\tlearn: 0.1051812\ttotal: 282ms\tremaining: 618ms\n","313:\tlearn: 0.1046225\ttotal: 291ms\tremaining: 636ms\n","314:\tlearn: 0.1041983\ttotal: 292ms\tremaining: 636ms\n","315:\tlearn: 0.1036829\ttotal: 294ms\tremaining: 635ms\n","316:\tlearn: 0.1032248\ttotal: 295ms\tremaining: 635ms\n","317:\tlearn: 0.1027852\ttotal: 297ms\tremaining: 636ms\n","318:\tlearn: 0.1023300\ttotal: 301ms\tremaining: 642ms\n","319:\tlearn: 0.1019666\ttotal: 304ms\tremaining: 646ms\n","320:\tlearn: 0.1015301\ttotal: 306ms\tremaining: 648ms\n","321:\tlearn: 0.1011655\ttotal: 309ms\tremaining: 650ms\n","322:\tlearn: 0.1008279\ttotal: 311ms\tremaining: 651ms\n","323:\tlearn: 0.1004310\ttotal: 312ms\tremaining: 652ms\n","324:\tlearn: 0.0999773\ttotal: 314ms\tremaining: 653ms\n","325:\tlearn: 0.0996377\ttotal: 316ms\tremaining: 654ms\n","326:\tlearn: 0.0989904\ttotal: 318ms\tremaining: 654ms\n","327:\tlearn: 0.0986391\ttotal: 319ms\tremaining: 653ms\n","328:\tlearn: 0.0981243\ttotal: 320ms\tremaining: 653ms\n","329:\tlearn: 0.0977120\ttotal: 322ms\tremaining: 654ms\n","330:\tlearn: 0.0973374\ttotal: 323ms\tremaining: 652ms\n","331:\tlearn: 0.0969076\ttotal: 323ms\tremaining: 650ms\n","332:\tlearn: 0.0964918\ttotal: 324ms\tremaining: 648ms\n","333:\tlearn: 0.0960516\ttotal: 324ms\tremaining: 646ms\n","334:\tlearn: 0.0957844\ttotal: 324ms\tremaining: 644ms\n","335:\tlearn: 0.0953373\ttotal: 325ms\tremaining: 642ms\n","336:\tlearn: 0.0950035\ttotal: 325ms\tremaining: 640ms\n","337:\tlearn: 0.0946075\ttotal: 326ms\tremaining: 638ms\n","338:\tlearn: 0.0942696\ttotal: 326ms\tremaining: 636ms\n","339:\tlearn: 0.0938542\ttotal: 327ms\tremaining: 634ms\n","340:\tlearn: 0.0934273\ttotal: 327ms\tremaining: 633ms\n","341:\tlearn: 0.0931103\ttotal: 328ms\tremaining: 631ms\n","342:\tlearn: 0.0928178\ttotal: 328ms\tremaining: 629ms\n","343:\tlearn: 0.0923715\ttotal: 329ms\tremaining: 627ms\n","344:\tlearn: 0.0920825\ttotal: 329ms\tremaining: 625ms\n","345:\tlearn: 0.0916260\ttotal: 330ms\tremaining: 623ms\n","346:\tlearn: 0.0913012\ttotal: 330ms\tremaining: 621ms\n","347:\tlearn: 0.0908552\ttotal: 331ms\tremaining: 620ms\n","348:\tlearn: 0.0904874\ttotal: 332ms\tremaining: 619ms\n","349:\tlearn: 0.0900736\ttotal: 332ms\tremaining: 617ms\n","350:\tlearn: 0.0897298\ttotal: 333ms\tremaining: 615ms\n","351:\tlearn: 0.0894493\ttotal: 333ms\tremaining: 613ms\n","352:\tlearn: 0.0891435\ttotal: 334ms\tremaining: 611ms\n","353:\tlearn: 0.0886732\ttotal: 338ms\tremaining: 617ms\n","354:\tlearn: 0.0883668\ttotal: 340ms\tremaining: 618ms\n","355:\tlearn: 0.0880310\ttotal: 342ms\tremaining: 618ms\n","356:\tlearn: 0.0875780\ttotal: 342ms\tremaining: 616ms\n","357:\tlearn: 0.0871820\ttotal: 343ms\tremaining: 615ms\n","358:\tlearn: 0.0867608\ttotal: 343ms\tremaining: 613ms\n","359:\tlearn: 0.0864424\ttotal: 344ms\tremaining: 611ms\n","360:\tlearn: 0.0862249\ttotal: 344ms\tremaining: 609ms\n","361:\tlearn: 0.0858283\ttotal: 344ms\tremaining: 607ms\n","362:\tlearn: 0.0854835\ttotal: 345ms\tremaining: 605ms\n","363:\tlearn: 0.0850520\ttotal: 345ms\tremaining: 603ms\n","364:\tlearn: 0.0847196\ttotal: 346ms\tremaining: 601ms\n","365:\tlearn: 0.0843517\ttotal: 346ms\tremaining: 599ms\n","366:\tlearn: 0.0840400\ttotal: 346ms\tremaining: 597ms\n","367:\tlearn: 0.0837467\ttotal: 347ms\tremaining: 596ms\n","368:\tlearn: 0.0834494\ttotal: 347ms\tremaining: 594ms\n","369:\tlearn: 0.0831970\ttotal: 348ms\tremaining: 592ms\n","370:\tlearn: 0.0828007\ttotal: 348ms\tremaining: 590ms\n","371:\tlearn: 0.0824711\ttotal: 349ms\tremaining: 588ms\n","372:\tlearn: 0.0821771\ttotal: 349ms\tremaining: 587ms\n","373:\tlearn: 0.0818068\ttotal: 349ms\tremaining: 585ms\n","374:\tlearn: 0.0814747\ttotal: 350ms\tremaining: 583ms\n","375:\tlearn: 0.0811967\ttotal: 350ms\tremaining: 581ms\n","376:\tlearn: 0.0808842\ttotal: 351ms\tremaining: 580ms\n","377:\tlearn: 0.0806295\ttotal: 351ms\tremaining: 578ms\n","378:\tlearn: 0.0804112\ttotal: 352ms\tremaining: 577ms\n","379:\tlearn: 0.0801735\ttotal: 352ms\tremaining: 575ms\n","380:\tlearn: 0.0798490\ttotal: 353ms\tremaining: 573ms\n","381:\tlearn: 0.0795899\ttotal: 353ms\tremaining: 572ms\n","382:\tlearn: 0.0793788\ttotal: 354ms\tremaining: 570ms\n","383:\tlearn: 0.0790900\ttotal: 354ms\tremaining: 568ms\n","384:\tlearn: 0.0787950\ttotal: 355ms\tremaining: 566ms\n","385:\tlearn: 0.0785654\ttotal: 355ms\tremaining: 564ms\n","386:\tlearn: 0.0782210\ttotal: 355ms\tremaining: 563ms\n","387:\tlearn: 0.0778938\ttotal: 356ms\tremaining: 561ms\n","388:\tlearn: 0.0775292\ttotal: 356ms\tremaining: 559ms\n","389:\tlearn: 0.0771616\ttotal: 358ms\tremaining: 559ms\n","390:\tlearn: 0.0768341\ttotal: 358ms\tremaining: 558ms\n","391:\tlearn: 0.0765105\ttotal: 358ms\tremaining: 556ms\n","392:\tlearn: 0.0762448\ttotal: 359ms\tremaining: 554ms\n","393:\tlearn: 0.0759307\ttotal: 359ms\tremaining: 552ms\n","394:\tlearn: 0.0756318\ttotal: 360ms\tremaining: 551ms\n","395:\tlearn: 0.0754022\ttotal: 360ms\tremaining: 549ms\n","396:\tlearn: 0.0751883\ttotal: 360ms\tremaining: 547ms\n","397:\tlearn: 0.0749354\ttotal: 361ms\tremaining: 546ms\n","398:\tlearn: 0.0746846\ttotal: 361ms\tremaining: 544ms\n","399:\tlearn: 0.0743525\ttotal: 362ms\tremaining: 542ms\n","400:\tlearn: 0.0740276\ttotal: 362ms\tremaining: 541ms\n","401:\tlearn: 0.0736928\ttotal: 365ms\tremaining: 543ms\n","402:\tlearn: 0.0733400\ttotal: 365ms\tremaining: 541ms\n","403:\tlearn: 0.0730195\ttotal: 366ms\tremaining: 539ms\n","404:\tlearn: 0.0728377\ttotal: 366ms\tremaining: 538ms\n","405:\tlearn: 0.0726162\ttotal: 372ms\tremaining: 544ms\n","406:\tlearn: 0.0723633\ttotal: 373ms\tremaining: 544ms\n","407:\tlearn: 0.0721472\ttotal: 375ms\tremaining: 544ms\n","408:\tlearn: 0.0718354\ttotal: 377ms\tremaining: 544ms\n","409:\tlearn: 0.0716219\ttotal: 378ms\tremaining: 544ms\n","410:\tlearn: 0.0714044\ttotal: 380ms\tremaining: 544ms\n","411:\tlearn: 0.0711673\ttotal: 380ms\tremaining: 543ms\n","412:\tlearn: 0.0709256\ttotal: 381ms\tremaining: 541ms\n","413:\tlearn: 0.0706054\ttotal: 381ms\tremaining: 540ms\n","414:\tlearn: 0.0703471\ttotal: 382ms\tremaining: 538ms\n","415:\tlearn: 0.0700926\ttotal: 382ms\tremaining: 536ms\n","416:\tlearn: 0.0698605\ttotal: 383ms\tremaining: 535ms\n","417:\tlearn: 0.0695914\ttotal: 383ms\tremaining: 533ms\n","418:\tlearn: 0.0694548\ttotal: 383ms\tremaining: 532ms\n","419:\tlearn: 0.0692895\ttotal: 384ms\tremaining: 530ms\n","420:\tlearn: 0.0690810\ttotal: 384ms\tremaining: 528ms\n","421:\tlearn: 0.0688172\ttotal: 385ms\tremaining: 527ms\n","422:\tlearn: 0.0685081\ttotal: 385ms\tremaining: 525ms\n","423:\tlearn: 0.0682502\ttotal: 385ms\tremaining: 524ms\n","424:\tlearn: 0.0680810\ttotal: 386ms\tremaining: 522ms\n","425:\tlearn: 0.0679252\ttotal: 390ms\tremaining: 526ms\n","426:\tlearn: 0.0676799\ttotal: 392ms\tremaining: 526ms\n","427:\tlearn: 0.0674467\ttotal: 394ms\tremaining: 526ms\n","428:\tlearn: 0.0672553\ttotal: 396ms\tremaining: 527ms\n","429:\tlearn: 0.0670019\ttotal: 398ms\tremaining: 527ms\n","430:\tlearn: 0.0668495\ttotal: 400ms\tremaining: 528ms\n","431:\tlearn: 0.0666595\ttotal: 402ms\tremaining: 529ms\n","432:\tlearn: 0.0664525\ttotal: 404ms\tremaining: 529ms\n","433:\tlearn: 0.0661974\ttotal: 406ms\tremaining: 529ms\n","434:\tlearn: 0.0659306\ttotal: 407ms\tremaining: 529ms\n","435:\tlearn: 0.0657661\ttotal: 409ms\tremaining: 529ms\n","436:\tlearn: 0.0655364\ttotal: 410ms\tremaining: 529ms\n","437:\tlearn: 0.0653692\ttotal: 412ms\tremaining: 529ms\n","438:\tlearn: 0.0651229\ttotal: 414ms\tremaining: 529ms\n","439:\tlearn: 0.0648764\ttotal: 416ms\tremaining: 529ms\n","440:\tlearn: 0.0646435\ttotal: 417ms\tremaining: 528ms\n","441:\tlearn: 0.0644978\ttotal: 419ms\tremaining: 529ms\n","442:\tlearn: 0.0642007\ttotal: 422ms\tremaining: 530ms\n","443:\tlearn: 0.0639757\ttotal: 425ms\tremaining: 532ms\n","444:\tlearn: 0.0637484\ttotal: 427ms\tremaining: 533ms\n","445:\tlearn: 0.0634659\ttotal: 429ms\tremaining: 533ms\n","446:\tlearn: 0.0632441\ttotal: 430ms\tremaining: 532ms\n","447:\tlearn: 0.0630166\ttotal: 439ms\tremaining: 541ms\n","448:\tlearn: 0.0628005\ttotal: 440ms\tremaining: 540ms\n","449:\tlearn: 0.0626505\ttotal: 441ms\tremaining: 538ms\n","450:\tlearn: 0.0624163\ttotal: 441ms\tremaining: 537ms\n","451:\tlearn: 0.0622208\ttotal: 442ms\tremaining: 536ms\n","452:\tlearn: 0.0619800\ttotal: 443ms\tremaining: 535ms\n","453:\tlearn: 0.0617791\ttotal: 444ms\tremaining: 533ms\n","454:\tlearn: 0.0615573\ttotal: 444ms\tremaining: 532ms\n","455:\tlearn: 0.0613792\ttotal: 445ms\tremaining: 531ms\n","456:\tlearn: 0.0612114\ttotal: 446ms\tremaining: 530ms\n","457:\tlearn: 0.0609784\ttotal: 446ms\tremaining: 528ms\n","458:\tlearn: 0.0607738\ttotal: 447ms\tremaining: 527ms\n","459:\tlearn: 0.0606333\ttotal: 448ms\tremaining: 526ms\n","460:\tlearn: 0.0604588\ttotal: 449ms\tremaining: 525ms\n","461:\tlearn: 0.0602215\ttotal: 454ms\tremaining: 529ms\n","462:\tlearn: 0.0599963\ttotal: 455ms\tremaining: 528ms\n","463:\tlearn: 0.0598804\ttotal: 456ms\tremaining: 527ms\n","464:\tlearn: 0.0596878\ttotal: 457ms\tremaining: 526ms\n","465:\tlearn: 0.0595320\ttotal: 457ms\tremaining: 524ms\n","466:\tlearn: 0.0594038\ttotal: 458ms\tremaining: 523ms\n","467:\tlearn: 0.0591993\ttotal: 459ms\tremaining: 521ms\n","468:\tlearn: 0.0589910\ttotal: 459ms\tremaining: 520ms\n","469:\tlearn: 0.0587305\ttotal: 460ms\tremaining: 519ms\n","470:\tlearn: 0.0586187\ttotal: 461ms\tremaining: 518ms\n","471:\tlearn: 0.0584227\ttotal: 462ms\tremaining: 517ms\n","472:\tlearn: 0.0582439\ttotal: 463ms\tremaining: 515ms\n","473:\tlearn: 0.0580266\ttotal: 464ms\tremaining: 514ms\n","474:\tlearn: 0.0578307\ttotal: 465ms\tremaining: 514ms\n","475:\tlearn: 0.0576451\ttotal: 471ms\tremaining: 519ms\n","476:\tlearn: 0.0574362\ttotal: 472ms\tremaining: 518ms\n","477:\tlearn: 0.0573198\ttotal: 473ms\tremaining: 517ms\n","478:\tlearn: 0.0571617\ttotal: 474ms\tremaining: 516ms\n","479:\tlearn: 0.0570144\ttotal: 475ms\tremaining: 515ms\n","480:\tlearn: 0.0568967\ttotal: 476ms\tremaining: 513ms\n","481:\tlearn: 0.0566922\ttotal: 477ms\tremaining: 512ms\n","482:\tlearn: 0.0565289\ttotal: 477ms\tremaining: 511ms\n","483:\tlearn: 0.0563698\ttotal: 478ms\tremaining: 510ms\n","484:\tlearn: 0.0562036\ttotal: 479ms\tremaining: 509ms\n","485:\tlearn: 0.0560091\ttotal: 480ms\tremaining: 507ms\n","486:\tlearn: 0.0558322\ttotal: 480ms\tremaining: 506ms\n","487:\tlearn: 0.0556233\ttotal: 481ms\tremaining: 505ms\n","488:\tlearn: 0.0554878\ttotal: 482ms\tremaining: 503ms\n","489:\tlearn: 0.0553021\ttotal: 483ms\tremaining: 502ms\n","490:\tlearn: 0.0550870\ttotal: 489ms\tremaining: 507ms\n","491:\tlearn: 0.0549373\ttotal: 490ms\tremaining: 506ms\n","492:\tlearn: 0.0548165\ttotal: 491ms\tremaining: 505ms\n","493:\tlearn: 0.0546219\ttotal: 491ms\tremaining: 503ms\n","494:\tlearn: 0.0544635\ttotal: 492ms\tremaining: 502ms\n","495:\tlearn: 0.0543386\ttotal: 493ms\tremaining: 501ms\n","496:\tlearn: 0.0542240\ttotal: 494ms\tremaining: 500ms\n","497:\tlearn: 0.0540822\ttotal: 495ms\tremaining: 499ms\n","498:\tlearn: 0.0538895\ttotal: 496ms\tremaining: 498ms\n","499:\tlearn: 0.0537556\ttotal: 501ms\tremaining: 501ms\n","500:\tlearn: 0.0535933\ttotal: 502ms\tremaining: 500ms\n","501:\tlearn: 0.0534250\ttotal: 503ms\tremaining: 499ms\n","502:\tlearn: 0.0532725\ttotal: 504ms\tremaining: 498ms\n","503:\tlearn: 0.0531228\ttotal: 505ms\tremaining: 497ms\n","504:\tlearn: 0.0529407\ttotal: 505ms\tremaining: 495ms\n","505:\tlearn: 0.0528012\ttotal: 506ms\tremaining: 494ms\n","506:\tlearn: 0.0525976\ttotal: 507ms\tremaining: 493ms\n","507:\tlearn: 0.0524209\ttotal: 508ms\tremaining: 492ms\n","508:\tlearn: 0.0522411\ttotal: 508ms\tremaining: 490ms\n","509:\tlearn: 0.0520936\ttotal: 518ms\tremaining: 497ms\n","510:\tlearn: 0.0519198\ttotal: 518ms\tremaining: 496ms\n","511:\tlearn: 0.0517270\ttotal: 519ms\tremaining: 495ms\n","512:\tlearn: 0.0515796\ttotal: 520ms\tremaining: 494ms\n","513:\tlearn: 0.0513504\ttotal: 520ms\tremaining: 492ms\n","514:\tlearn: 0.0511958\ttotal: 521ms\tremaining: 491ms\n","515:\tlearn: 0.0510172\ttotal: 522ms\tremaining: 489ms\n","516:\tlearn: 0.0508520\ttotal: 523ms\tremaining: 488ms\n","517:\tlearn: 0.0506586\ttotal: 526ms\tremaining: 489ms\n","518:\tlearn: 0.0504661\ttotal: 528ms\tremaining: 489ms\n","519:\tlearn: 0.0502739\ttotal: 529ms\tremaining: 489ms\n","520:\tlearn: 0.0501604\ttotal: 530ms\tremaining: 487ms\n","521:\tlearn: 0.0500422\ttotal: 531ms\tremaining: 486ms\n","522:\tlearn: 0.0499306\ttotal: 533ms\tremaining: 486ms\n","523:\tlearn: 0.0497835\ttotal: 533ms\tremaining: 484ms\n","524:\tlearn: 0.0496492\ttotal: 534ms\tremaining: 483ms\n","525:\tlearn: 0.0494988\ttotal: 535ms\tremaining: 482ms\n","526:\tlearn: 0.0492702\ttotal: 540ms\tremaining: 484ms\n","527:\tlearn: 0.0490902\ttotal: 541ms\tremaining: 483ms\n","528:\tlearn: 0.0489796\ttotal: 541ms\tremaining: 482ms\n","529:\tlearn: 0.0488150\ttotal: 542ms\tremaining: 481ms\n","530:\tlearn: 0.0486884\ttotal: 543ms\tremaining: 480ms\n","531:\tlearn: 0.0485467\ttotal: 544ms\tremaining: 478ms\n","532:\tlearn: 0.0483845\ttotal: 545ms\tremaining: 477ms\n","533:\tlearn: 0.0481769\ttotal: 545ms\tremaining: 476ms\n","534:\tlearn: 0.0480678\ttotal: 546ms\tremaining: 475ms\n","535:\tlearn: 0.0479234\ttotal: 547ms\tremaining: 473ms\n","536:\tlearn: 0.0478298\ttotal: 548ms\tremaining: 473ms\n","537:\tlearn: 0.0476901\ttotal: 549ms\tremaining: 472ms\n","538:\tlearn: 0.0475561\ttotal: 550ms\tremaining: 470ms\n","539:\tlearn: 0.0474184\ttotal: 553ms\tremaining: 471ms\n","540:\tlearn: 0.0472763\ttotal: 561ms\tremaining: 476ms\n","541:\tlearn: 0.0471624\ttotal: 561ms\tremaining: 474ms\n","542:\tlearn: 0.0470298\ttotal: 562ms\tremaining: 473ms\n","543:\tlearn: 0.0469377\ttotal: 563ms\tremaining: 472ms\n","544:\tlearn: 0.0467959\ttotal: 564ms\tremaining: 471ms\n","545:\tlearn: 0.0466605\ttotal: 565ms\tremaining: 469ms\n","546:\tlearn: 0.0465156\ttotal: 565ms\tremaining: 468ms\n","547:\tlearn: 0.0463842\ttotal: 567ms\tremaining: 468ms\n","548:\tlearn: 0.0462714\ttotal: 568ms\tremaining: 467ms\n","549:\tlearn: 0.0461930\ttotal: 568ms\tremaining: 465ms\n","550:\tlearn: 0.0460278\ttotal: 570ms\tremaining: 464ms\n","551:\tlearn: 0.0458953\ttotal: 570ms\tremaining: 463ms\n","552:\tlearn: 0.0457733\ttotal: 573ms\tremaining: 463ms\n","553:\tlearn: 0.0456757\ttotal: 573ms\tremaining: 462ms\n","554:\tlearn: 0.0455394\ttotal: 574ms\tremaining: 460ms\n","555:\tlearn: 0.0454111\ttotal: 575ms\tremaining: 459ms\n","556:\tlearn: 0.0453091\ttotal: 578ms\tremaining: 460ms\n","557:\tlearn: 0.0451921\ttotal: 579ms\tremaining: 459ms\n","558:\tlearn: 0.0450659\ttotal: 580ms\tremaining: 458ms\n","559:\tlearn: 0.0449606\ttotal: 581ms\tremaining: 457ms\n","560:\tlearn: 0.0448450\ttotal: 582ms\tremaining: 455ms\n","561:\tlearn: 0.0447215\ttotal: 583ms\tremaining: 454ms\n","562:\tlearn: 0.0446004\ttotal: 583ms\tremaining: 453ms\n","563:\tlearn: 0.0445036\ttotal: 584ms\tremaining: 452ms\n","564:\tlearn: 0.0443544\ttotal: 585ms\tremaining: 450ms\n","565:\tlearn: 0.0442251\ttotal: 586ms\tremaining: 449ms\n","566:\tlearn: 0.0441002\ttotal: 587ms\tremaining: 448ms\n","567:\tlearn: 0.0439886\ttotal: 588ms\tremaining: 447ms\n","568:\tlearn: 0.0438581\ttotal: 589ms\tremaining: 446ms\n","569:\tlearn: 0.0437263\ttotal: 589ms\tremaining: 445ms\n","570:\tlearn: 0.0435875\ttotal: 590ms\tremaining: 443ms\n","571:\tlearn: 0.0435047\ttotal: 594ms\tremaining: 444ms\n","572:\tlearn: 0.0433995\ttotal: 596ms\tremaining: 444ms\n","573:\tlearn: 0.0432729\ttotal: 597ms\tremaining: 443ms\n","574:\tlearn: 0.0431555\ttotal: 599ms\tremaining: 442ms\n","575:\tlearn: 0.0430542\ttotal: 602ms\tremaining: 443ms\n","576:\tlearn: 0.0429133\ttotal: 605ms\tremaining: 444ms\n","577:\tlearn: 0.0428486\ttotal: 606ms\tremaining: 443ms\n","578:\tlearn: 0.0427173\ttotal: 608ms\tremaining: 442ms\n","579:\tlearn: 0.0426517\ttotal: 610ms\tremaining: 442ms\n","580:\tlearn: 0.0425271\ttotal: 611ms\tremaining: 441ms\n","581:\tlearn: 0.0423835\ttotal: 611ms\tremaining: 439ms\n","582:\tlearn: 0.0422335\ttotal: 612ms\tremaining: 438ms\n","583:\tlearn: 0.0421389\ttotal: 612ms\tremaining: 436ms\n","584:\tlearn: 0.0420410\ttotal: 614ms\tremaining: 436ms\n","585:\tlearn: 0.0419263\ttotal: 616ms\tremaining: 435ms\n","586:\tlearn: 0.0418075\ttotal: 618ms\tremaining: 435ms\n","587:\tlearn: 0.0417060\ttotal: 620ms\tremaining: 434ms\n","588:\tlearn: 0.0415746\ttotal: 622ms\tremaining: 434ms\n","589:\tlearn: 0.0414645\ttotal: 624ms\tremaining: 433ms\n","590:\tlearn: 0.0413660\ttotal: 625ms\tremaining: 433ms\n","591:\tlearn: 0.0412581\ttotal: 626ms\tremaining: 431ms\n","592:\tlearn: 0.0411146\ttotal: 629ms\tremaining: 432ms\n","593:\tlearn: 0.0410069\ttotal: 630ms\tremaining: 431ms\n","594:\tlearn: 0.0409087\ttotal: 632ms\tremaining: 430ms\n","595:\tlearn: 0.0407906\ttotal: 633ms\tremaining: 429ms\n","596:\tlearn: 0.0406774\ttotal: 635ms\tremaining: 428ms\n","597:\tlearn: 0.0405922\ttotal: 637ms\tremaining: 428ms\n","598:\tlearn: 0.0404702\ttotal: 638ms\tremaining: 427ms\n","599:\tlearn: 0.0403776\ttotal: 639ms\tremaining: 426ms\n","600:\tlearn: 0.0402723\ttotal: 640ms\tremaining: 425ms\n","601:\tlearn: 0.0401759\ttotal: 641ms\tremaining: 424ms\n","602:\tlearn: 0.0400703\ttotal: 642ms\tremaining: 423ms\n","603:\tlearn: 0.0400023\ttotal: 643ms\tremaining: 422ms\n","604:\tlearn: 0.0398864\ttotal: 645ms\tremaining: 421ms\n","605:\tlearn: 0.0397828\ttotal: 646ms\tremaining: 420ms\n","606:\tlearn: 0.0396915\ttotal: 648ms\tremaining: 419ms\n","607:\tlearn: 0.0396246\ttotal: 649ms\tremaining: 419ms\n","608:\tlearn: 0.0394763\ttotal: 651ms\tremaining: 418ms\n","609:\tlearn: 0.0393921\ttotal: 652ms\tremaining: 417ms\n","610:\tlearn: 0.0392940\ttotal: 654ms\tremaining: 416ms\n","611:\tlearn: 0.0391810\ttotal: 655ms\tremaining: 416ms\n","612:\tlearn: 0.0390693\ttotal: 657ms\tremaining: 415ms\n","613:\tlearn: 0.0389873\ttotal: 658ms\tremaining: 414ms\n","614:\tlearn: 0.0388573\ttotal: 659ms\tremaining: 413ms\n","615:\tlearn: 0.0387703\ttotal: 661ms\tremaining: 412ms\n","616:\tlearn: 0.0386923\ttotal: 662ms\tremaining: 411ms\n","617:\tlearn: 0.0385832\ttotal: 663ms\tremaining: 410ms\n","618:\tlearn: 0.0384866\ttotal: 664ms\tremaining: 409ms\n","619:\tlearn: 0.0383832\ttotal: 665ms\tremaining: 408ms\n","620:\tlearn: 0.0383271\ttotal: 667ms\tremaining: 407ms\n","621:\tlearn: 0.0382069\ttotal: 668ms\tremaining: 406ms\n","622:\tlearn: 0.0381214\ttotal: 669ms\tremaining: 405ms\n","623:\tlearn: 0.0380198\ttotal: 671ms\tremaining: 404ms\n","624:\tlearn: 0.0379092\ttotal: 672ms\tremaining: 403ms\n","625:\tlearn: 0.0377996\ttotal: 673ms\tremaining: 402ms\n","626:\tlearn: 0.0377241\ttotal: 674ms\tremaining: 401ms\n","627:\tlearn: 0.0376053\ttotal: 676ms\tremaining: 400ms\n","628:\tlearn: 0.0375257\ttotal: 677ms\tremaining: 399ms\n","629:\tlearn: 0.0374122\ttotal: 678ms\tremaining: 398ms\n","630:\tlearn: 0.0373017\ttotal: 678ms\tremaining: 397ms\n","631:\tlearn: 0.0371798\ttotal: 679ms\tremaining: 395ms\n","632:\tlearn: 0.0371029\ttotal: 679ms\tremaining: 394ms\n","633:\tlearn: 0.0370205\ttotal: 679ms\tremaining: 392ms\n","634:\tlearn: 0.0369261\ttotal: 680ms\tremaining: 391ms\n","635:\tlearn: 0.0368353\ttotal: 680ms\tremaining: 389ms\n","636:\tlearn: 0.0367312\ttotal: 681ms\tremaining: 388ms\n","637:\tlearn: 0.0366495\ttotal: 681ms\tremaining: 386ms\n","638:\tlearn: 0.0365523\ttotal: 682ms\tremaining: 385ms\n","639:\tlearn: 0.0364683\ttotal: 682ms\tremaining: 384ms\n","640:\tlearn: 0.0363352\ttotal: 682ms\tremaining: 382ms\n","641:\tlearn: 0.0362344\ttotal: 683ms\tremaining: 381ms\n","642:\tlearn: 0.0361585\ttotal: 683ms\tremaining: 379ms\n","643:\tlearn: 0.0360846\ttotal: 683ms\tremaining: 378ms\n","644:\tlearn: 0.0359657\ttotal: 684ms\tremaining: 376ms\n","645:\tlearn: 0.0358915\ttotal: 684ms\tremaining: 375ms\n","646:\tlearn: 0.0357779\ttotal: 684ms\tremaining: 373ms\n","647:\tlearn: 0.0357022\ttotal: 685ms\tremaining: 372ms\n","648:\tlearn: 0.0356294\ttotal: 685ms\tremaining: 371ms\n","649:\tlearn: 0.0355385\ttotal: 686ms\tremaining: 369ms\n","650:\tlearn: 0.0354533\ttotal: 686ms\tremaining: 368ms\n","651:\tlearn: 0.0353430\ttotal: 686ms\tremaining: 366ms\n","652:\tlearn: 0.0352470\ttotal: 687ms\tremaining: 365ms\n","653:\tlearn: 0.0351444\ttotal: 687ms\tremaining: 364ms\n","654:\tlearn: 0.0350457\ttotal: 688ms\tremaining: 362ms\n","655:\tlearn: 0.0349646\ttotal: 688ms\tremaining: 361ms\n","656:\tlearn: 0.0348938\ttotal: 688ms\tremaining: 359ms\n","657:\tlearn: 0.0348103\ttotal: 689ms\tremaining: 358ms\n","658:\tlearn: 0.0347357\ttotal: 689ms\tremaining: 357ms\n","659:\tlearn: 0.0346339\ttotal: 690ms\tremaining: 355ms\n","660:\tlearn: 0.0345520\ttotal: 690ms\tremaining: 354ms\n","661:\tlearn: 0.0344555\ttotal: 691ms\tremaining: 353ms\n","662:\tlearn: 0.0344015\ttotal: 691ms\tremaining: 351ms\n","663:\tlearn: 0.0343101\ttotal: 691ms\tremaining: 350ms\n","664:\tlearn: 0.0341996\ttotal: 692ms\tremaining: 348ms\n","665:\tlearn: 0.0341294\ttotal: 692ms\tremaining: 347ms\n","666:\tlearn: 0.0340603\ttotal: 693ms\tremaining: 346ms\n","667:\tlearn: 0.0339753\ttotal: 693ms\tremaining: 345ms\n","668:\tlearn: 0.0339105\ttotal: 694ms\tremaining: 343ms\n","669:\tlearn: 0.0338457\ttotal: 694ms\tremaining: 342ms\n","670:\tlearn: 0.0337271\ttotal: 694ms\tremaining: 341ms\n","671:\tlearn: 0.0336609\ttotal: 695ms\tremaining: 339ms\n","672:\tlearn: 0.0335689\ttotal: 695ms\tremaining: 338ms\n","673:\tlearn: 0.0334729\ttotal: 696ms\tremaining: 336ms\n","674:\tlearn: 0.0334039\ttotal: 696ms\tremaining: 335ms\n","675:\tlearn: 0.0333174\ttotal: 697ms\tremaining: 334ms\n","676:\tlearn: 0.0332291\ttotal: 697ms\tremaining: 333ms\n","677:\tlearn: 0.0331709\ttotal: 698ms\tremaining: 331ms\n","678:\tlearn: 0.0330877\ttotal: 698ms\tremaining: 330ms\n","679:\tlearn: 0.0329746\ttotal: 699ms\tremaining: 329ms\n","680:\tlearn: 0.0328788\ttotal: 699ms\tremaining: 327ms\n","681:\tlearn: 0.0328323\ttotal: 699ms\tremaining: 326ms\n","682:\tlearn: 0.0327844\ttotal: 700ms\tremaining: 325ms\n","683:\tlearn: 0.0327054\ttotal: 700ms\tremaining: 324ms\n","684:\tlearn: 0.0326363\ttotal: 701ms\tremaining: 322ms\n","685:\tlearn: 0.0325611\ttotal: 702ms\tremaining: 321ms\n","686:\tlearn: 0.0324921\ttotal: 702ms\tremaining: 320ms\n","687:\tlearn: 0.0324255\ttotal: 702ms\tremaining: 319ms\n","688:\tlearn: 0.0323390\ttotal: 703ms\tremaining: 317ms\n","689:\tlearn: 0.0322880\ttotal: 703ms\tremaining: 316ms\n","690:\tlearn: 0.0322440\ttotal: 704ms\tremaining: 315ms\n","691:\tlearn: 0.0321817\ttotal: 704ms\tremaining: 313ms\n","692:\tlearn: 0.0321282\ttotal: 705ms\tremaining: 312ms\n","693:\tlearn: 0.0320624\ttotal: 705ms\tremaining: 311ms\n","694:\tlearn: 0.0319907\ttotal: 706ms\tremaining: 310ms\n","695:\tlearn: 0.0319291\ttotal: 706ms\tremaining: 308ms\n","696:\tlearn: 0.0318772\ttotal: 707ms\tremaining: 307ms\n","697:\tlearn: 0.0318081\ttotal: 707ms\tremaining: 306ms\n","698:\tlearn: 0.0317424\ttotal: 708ms\tremaining: 305ms\n","699:\tlearn: 0.0316655\ttotal: 713ms\tremaining: 306ms\n","700:\tlearn: 0.0315534\ttotal: 716ms\tremaining: 305ms\n","701:\tlearn: 0.0314779\ttotal: 718ms\tremaining: 305ms\n","702:\tlearn: 0.0314146\ttotal: 720ms\tremaining: 304ms\n","703:\tlearn: 0.0313864\ttotal: 723ms\tremaining: 304ms\n","704:\tlearn: 0.0313077\ttotal: 724ms\tremaining: 303ms\n","705:\tlearn: 0.0312415\ttotal: 725ms\tremaining: 302ms\n","706:\tlearn: 0.0311723\ttotal: 727ms\tremaining: 301ms\n","707:\tlearn: 0.0311152\ttotal: 728ms\tremaining: 300ms\n","708:\tlearn: 0.0310418\ttotal: 729ms\tremaining: 299ms\n","709:\tlearn: 0.0309775\ttotal: 731ms\tremaining: 299ms\n","710:\tlearn: 0.0308543\ttotal: 732ms\tremaining: 298ms\n","711:\tlearn: 0.0307919\ttotal: 733ms\tremaining: 296ms\n","712:\tlearn: 0.0307128\ttotal: 734ms\tremaining: 296ms\n","713:\tlearn: 0.0306766\ttotal: 736ms\tremaining: 295ms\n","714:\tlearn: 0.0306113\ttotal: 737ms\tremaining: 294ms\n","715:\tlearn: 0.0305330\ttotal: 739ms\tremaining: 293ms\n","716:\tlearn: 0.0304700\ttotal: 741ms\tremaining: 292ms\n","717:\tlearn: 0.0303959\ttotal: 743ms\tremaining: 292ms\n","718:\tlearn: 0.0303352\ttotal: 745ms\tremaining: 291ms\n","719:\tlearn: 0.0302563\ttotal: 746ms\tremaining: 290ms\n","720:\tlearn: 0.0301930\ttotal: 748ms\tremaining: 289ms\n","721:\tlearn: 0.0301370\ttotal: 749ms\tremaining: 289ms\n","722:\tlearn: 0.0300796\ttotal: 751ms\tremaining: 288ms\n","723:\tlearn: 0.0300098\ttotal: 753ms\tremaining: 287ms\n","724:\tlearn: 0.0299518\ttotal: 754ms\tremaining: 286ms\n","725:\tlearn: 0.0298780\ttotal: 755ms\tremaining: 285ms\n","726:\tlearn: 0.0298133\ttotal: 757ms\tremaining: 284ms\n","727:\tlearn: 0.0297288\ttotal: 758ms\tremaining: 283ms\n","728:\tlearn: 0.0296138\ttotal: 759ms\tremaining: 282ms\n","729:\tlearn: 0.0295417\ttotal: 761ms\tremaining: 281ms\n","730:\tlearn: 0.0294706\ttotal: 762ms\tremaining: 281ms\n","731:\tlearn: 0.0294089\ttotal: 764ms\tremaining: 280ms\n","732:\tlearn: 0.0293517\ttotal: 766ms\tremaining: 279ms\n","733:\tlearn: 0.0293088\ttotal: 768ms\tremaining: 278ms\n","734:\tlearn: 0.0292442\ttotal: 770ms\tremaining: 278ms\n","735:\tlearn: 0.0291868\ttotal: 771ms\tremaining: 277ms\n","736:\tlearn: 0.0291037\ttotal: 773ms\tremaining: 276ms\n","737:\tlearn: 0.0290491\ttotal: 774ms\tremaining: 275ms\n","738:\tlearn: 0.0289652\ttotal: 776ms\tremaining: 274ms\n","739:\tlearn: 0.0289128\ttotal: 778ms\tremaining: 273ms\n","740:\tlearn: 0.0288453\ttotal: 779ms\tremaining: 272ms\n","741:\tlearn: 0.0287892\ttotal: 781ms\tremaining: 271ms\n","742:\tlearn: 0.0287291\ttotal: 782ms\tremaining: 271ms\n","743:\tlearn: 0.0286750\ttotal: 784ms\tremaining: 270ms\n","744:\tlearn: 0.0286146\ttotal: 785ms\tremaining: 269ms\n","745:\tlearn: 0.0285473\ttotal: 787ms\tremaining: 268ms\n","746:\tlearn: 0.0284513\ttotal: 788ms\tremaining: 267ms\n","747:\tlearn: 0.0283994\ttotal: 789ms\tremaining: 266ms\n","748:\tlearn: 0.0283485\ttotal: 791ms\tremaining: 265ms\n","749:\tlearn: 0.0283069\ttotal: 793ms\tremaining: 264ms\n","750:\tlearn: 0.0282415\ttotal: 795ms\tremaining: 263ms\n","751:\tlearn: 0.0281896\ttotal: 796ms\tremaining: 263ms\n","752:\tlearn: 0.0281335\ttotal: 798ms\tremaining: 262ms\n","753:\tlearn: 0.0280798\ttotal: 799ms\tremaining: 261ms\n","754:\tlearn: 0.0280240\ttotal: 801ms\tremaining: 260ms\n","755:\tlearn: 0.0279387\ttotal: 802ms\tremaining: 259ms\n","756:\tlearn: 0.0278977\ttotal: 803ms\tremaining: 258ms\n","757:\tlearn: 0.0278354\ttotal: 805ms\tremaining: 257ms\n","758:\tlearn: 0.0277641\ttotal: 806ms\tremaining: 256ms\n","759:\tlearn: 0.0276739\ttotal: 807ms\tremaining: 255ms\n","760:\tlearn: 0.0276294\ttotal: 829ms\tremaining: 260ms\n","761:\tlearn: 0.0275753\ttotal: 830ms\tremaining: 259ms\n","762:\tlearn: 0.0275085\ttotal: 832ms\tremaining: 258ms\n","763:\tlearn: 0.0274772\ttotal: 834ms\tremaining: 258ms\n","764:\tlearn: 0.0274261\ttotal: 837ms\tremaining: 257ms\n","765:\tlearn: 0.0273800\ttotal: 837ms\tremaining: 256ms\n","766:\tlearn: 0.0273160\ttotal: 839ms\tremaining: 255ms\n","767:\tlearn: 0.0272707\ttotal: 840ms\tremaining: 254ms\n","768:\tlearn: 0.0272097\ttotal: 842ms\tremaining: 253ms\n","769:\tlearn: 0.0271587\ttotal: 843ms\tremaining: 252ms\n","770:\tlearn: 0.0271170\ttotal: 844ms\tremaining: 251ms\n","771:\tlearn: 0.0270765\ttotal: 846ms\tremaining: 250ms\n","772:\tlearn: 0.0269887\ttotal: 847ms\tremaining: 249ms\n","773:\tlearn: 0.0269285\ttotal: 848ms\tremaining: 248ms\n","774:\tlearn: 0.0268399\ttotal: 849ms\tremaining: 246ms\n","775:\tlearn: 0.0267730\ttotal: 850ms\tremaining: 245ms\n","776:\tlearn: 0.0267010\ttotal: 851ms\tremaining: 244ms\n","777:\tlearn: 0.0266555\ttotal: 852ms\tremaining: 243ms\n","778:\tlearn: 0.0266027\ttotal: 854ms\tremaining: 242ms\n","779:\tlearn: 0.0265621\ttotal: 855ms\tremaining: 241ms\n","780:\tlearn: 0.0265129\ttotal: 856ms\tremaining: 240ms\n","781:\tlearn: 0.0264535\ttotal: 858ms\tremaining: 239ms\n","782:\tlearn: 0.0264077\ttotal: 858ms\tremaining: 238ms\n","783:\tlearn: 0.0263409\ttotal: 859ms\tremaining: 237ms\n","784:\tlearn: 0.0262747\ttotal: 859ms\tremaining: 235ms\n","785:\tlearn: 0.0262184\ttotal: 862ms\tremaining: 235ms\n","786:\tlearn: 0.0261772\ttotal: 863ms\tremaining: 234ms\n","787:\tlearn: 0.0261239\ttotal: 864ms\tremaining: 233ms\n","788:\tlearn: 0.0260764\ttotal: 865ms\tremaining: 231ms\n","789:\tlearn: 0.0260102\ttotal: 867ms\tremaining: 230ms\n","790:\tlearn: 0.0259552\ttotal: 868ms\tremaining: 229ms\n","791:\tlearn: 0.0259019\ttotal: 869ms\tremaining: 228ms\n","792:\tlearn: 0.0258495\ttotal: 871ms\tremaining: 227ms\n","793:\tlearn: 0.0258007\ttotal: 873ms\tremaining: 226ms\n","794:\tlearn: 0.0257657\ttotal: 874ms\tremaining: 225ms\n","795:\tlearn: 0.0257086\ttotal: 876ms\tremaining: 224ms\n","796:\tlearn: 0.0256478\ttotal: 877ms\tremaining: 223ms\n","797:\tlearn: 0.0255841\ttotal: 878ms\tremaining: 222ms\n","798:\tlearn: 0.0255405\ttotal: 880ms\tremaining: 221ms\n","799:\tlearn: 0.0254972\ttotal: 881ms\tremaining: 220ms\n","800:\tlearn: 0.0254175\ttotal: 882ms\tremaining: 219ms\n","801:\tlearn: 0.0253687\ttotal: 884ms\tremaining: 218ms\n","802:\tlearn: 0.0253152\ttotal: 885ms\tremaining: 217ms\n","803:\tlearn: 0.0252584\ttotal: 886ms\tremaining: 216ms\n","804:\tlearn: 0.0252075\ttotal: 888ms\tremaining: 215ms\n","805:\tlearn: 0.0251637\ttotal: 890ms\tremaining: 214ms\n","806:\tlearn: 0.0251185\ttotal: 890ms\tremaining: 213ms\n","807:\tlearn: 0.0250864\ttotal: 891ms\tremaining: 212ms\n","808:\tlearn: 0.0250362\ttotal: 891ms\tremaining: 210ms\n","809:\tlearn: 0.0249979\ttotal: 892ms\tremaining: 209ms\n","810:\tlearn: 0.0249494\ttotal: 892ms\tremaining: 208ms\n","811:\tlearn: 0.0249027\ttotal: 893ms\tremaining: 207ms\n","812:\tlearn: 0.0248628\ttotal: 893ms\tremaining: 205ms\n","813:\tlearn: 0.0248160\ttotal: 893ms\tremaining: 204ms\n","814:\tlearn: 0.0247860\ttotal: 894ms\tremaining: 203ms\n","815:\tlearn: 0.0247432\ttotal: 895ms\tremaining: 202ms\n","816:\tlearn: 0.0247065\ttotal: 895ms\tremaining: 201ms\n","817:\tlearn: 0.0246521\ttotal: 896ms\tremaining: 199ms\n","818:\tlearn: 0.0246141\ttotal: 896ms\tremaining: 198ms\n","819:\tlearn: 0.0245738\ttotal: 897ms\tremaining: 197ms\n","820:\tlearn: 0.0245091\ttotal: 897ms\tremaining: 196ms\n","821:\tlearn: 0.0244601\ttotal: 897ms\tremaining: 194ms\n","822:\tlearn: 0.0244187\ttotal: 898ms\tremaining: 193ms\n","823:\tlearn: 0.0243761\ttotal: 901ms\tremaining: 192ms\n","824:\tlearn: 0.0243260\ttotal: 908ms\tremaining: 193ms\n","825:\tlearn: 0.0242445\ttotal: 909ms\tremaining: 192ms\n","826:\tlearn: 0.0241935\ttotal: 910ms\tremaining: 190ms\n","827:\tlearn: 0.0241637\ttotal: 911ms\tremaining: 189ms\n","828:\tlearn: 0.0241117\ttotal: 911ms\tremaining: 188ms\n","829:\tlearn: 0.0240678\ttotal: 912ms\tremaining: 187ms\n","830:\tlearn: 0.0240419\ttotal: 913ms\tremaining: 186ms\n","831:\tlearn: 0.0240028\ttotal: 915ms\tremaining: 185ms\n","832:\tlearn: 0.0239526\ttotal: 915ms\tremaining: 183ms\n","833:\tlearn: 0.0239012\ttotal: 916ms\tremaining: 182ms\n","834:\tlearn: 0.0238514\ttotal: 917ms\tremaining: 181ms\n","835:\tlearn: 0.0238115\ttotal: 918ms\tremaining: 180ms\n","836:\tlearn: 0.0237549\ttotal: 918ms\tremaining: 179ms\n","837:\tlearn: 0.0237029\ttotal: 924ms\tremaining: 179ms\n","838:\tlearn: 0.0236596\ttotal: 926ms\tremaining: 178ms\n","839:\tlearn: 0.0236154\ttotal: 927ms\tremaining: 177ms\n","840:\tlearn: 0.0235596\ttotal: 929ms\tremaining: 176ms\n","841:\tlearn: 0.0235205\ttotal: 931ms\tremaining: 175ms\n","842:\tlearn: 0.0234803\ttotal: 932ms\tremaining: 174ms\n","843:\tlearn: 0.0234365\ttotal: 934ms\tremaining: 173ms\n","844:\tlearn: 0.0233703\ttotal: 935ms\tremaining: 172ms\n","845:\tlearn: 0.0233274\ttotal: 936ms\tremaining: 170ms\n","846:\tlearn: 0.0232946\ttotal: 937ms\tremaining: 169ms\n","847:\tlearn: 0.0232504\ttotal: 938ms\tremaining: 168ms\n","848:\tlearn: 0.0232151\ttotal: 941ms\tremaining: 167ms\n","849:\tlearn: 0.0231656\ttotal: 942ms\tremaining: 166ms\n","850:\tlearn: 0.0231220\ttotal: 944ms\tremaining: 165ms\n","851:\tlearn: 0.0230801\ttotal: 946ms\tremaining: 164ms\n","852:\tlearn: 0.0230374\ttotal: 947ms\tremaining: 163ms\n","853:\tlearn: 0.0230027\ttotal: 950ms\tremaining: 162ms\n","854:\tlearn: 0.0229614\ttotal: 951ms\tremaining: 161ms\n","855:\tlearn: 0.0229124\ttotal: 953ms\tremaining: 160ms\n","856:\tlearn: 0.0228657\ttotal: 954ms\tremaining: 159ms\n","857:\tlearn: 0.0228242\ttotal: 955ms\tremaining: 158ms\n","858:\tlearn: 0.0227903\ttotal: 957ms\tremaining: 157ms\n","859:\tlearn: 0.0227419\ttotal: 958ms\tremaining: 156ms\n","860:\tlearn: 0.0226994\ttotal: 959ms\tremaining: 155ms\n","861:\tlearn: 0.0226478\ttotal: 960ms\tremaining: 154ms\n","862:\tlearn: 0.0225917\ttotal: 961ms\tremaining: 153ms\n","863:\tlearn: 0.0225501\ttotal: 962ms\tremaining: 151ms\n","864:\tlearn: 0.0225083\ttotal: 963ms\tremaining: 150ms\n","865:\tlearn: 0.0224559\ttotal: 965ms\tremaining: 149ms\n","866:\tlearn: 0.0224223\ttotal: 966ms\tremaining: 148ms\n","867:\tlearn: 0.0223895\ttotal: 967ms\tremaining: 147ms\n","868:\tlearn: 0.0223437\ttotal: 969ms\tremaining: 146ms\n","869:\tlearn: 0.0223006\ttotal: 970ms\tremaining: 145ms\n","870:\tlearn: 0.0222688\ttotal: 972ms\tremaining: 144ms\n","871:\tlearn: 0.0222371\ttotal: 973ms\tremaining: 143ms\n","872:\tlearn: 0.0222036\ttotal: 975ms\tremaining: 142ms\n","873:\tlearn: 0.0221445\ttotal: 976ms\tremaining: 141ms\n","874:\tlearn: 0.0221061\ttotal: 984ms\tremaining: 141ms\n","875:\tlearn: 0.0220341\ttotal: 989ms\tremaining: 140ms\n","876:\tlearn: 0.0220003\ttotal: 994ms\tremaining: 139ms\n","877:\tlearn: 0.0219583\ttotal: 999ms\tremaining: 139ms\n","878:\tlearn: 0.0219329\ttotal: 1s\tremaining: 138ms\n","879:\tlearn: 0.0219014\ttotal: 1.01s\tremaining: 137ms\n","880:\tlearn: 0.0218730\ttotal: 1.01s\tremaining: 136ms\n","881:\tlearn: 0.0218282\ttotal: 1.01s\tremaining: 135ms\n","882:\tlearn: 0.0218046\ttotal: 1.02s\tremaining: 135ms\n","883:\tlearn: 0.0217725\ttotal: 1.02s\tremaining: 134ms\n","884:\tlearn: 0.0217336\ttotal: 1.02s\tremaining: 133ms\n","885:\tlearn: 0.0217065\ttotal: 1.02s\tremaining: 132ms\n","886:\tlearn: 0.0216635\ttotal: 1.02s\tremaining: 131ms\n","887:\tlearn: 0.0216171\ttotal: 1.02s\tremaining: 129ms\n","888:\tlearn: 0.0215744\ttotal: 1.03s\tremaining: 128ms\n","889:\tlearn: 0.0215401\ttotal: 1.03s\tremaining: 127ms\n","890:\tlearn: 0.0215028\ttotal: 1.03s\tremaining: 126ms\n","891:\tlearn: 0.0214604\ttotal: 1.03s\tremaining: 125ms\n","892:\tlearn: 0.0214139\ttotal: 1.03s\tremaining: 123ms\n","893:\tlearn: 0.0213790\ttotal: 1.03s\tremaining: 122ms\n","894:\tlearn: 0.0213439\ttotal: 1.03s\tremaining: 121ms\n","895:\tlearn: 0.0213036\ttotal: 1.03s\tremaining: 120ms\n","896:\tlearn: 0.0212649\ttotal: 1.03s\tremaining: 119ms\n","897:\tlearn: 0.0212247\ttotal: 1.03s\tremaining: 117ms\n","898:\tlearn: 0.0211911\ttotal: 1.04s\tremaining: 116ms\n","899:\tlearn: 0.0211576\ttotal: 1.04s\tremaining: 115ms\n","900:\tlearn: 0.0211173\ttotal: 1.04s\tremaining: 114ms\n","901:\tlearn: 0.0210841\ttotal: 1.04s\tremaining: 113ms\n","902:\tlearn: 0.0210550\ttotal: 1.04s\tremaining: 112ms\n","903:\tlearn: 0.0210180\ttotal: 1.05s\tremaining: 111ms\n","904:\tlearn: 0.0209863\ttotal: 1.05s\tremaining: 110ms\n","905:\tlearn: 0.0209533\ttotal: 1.05s\tremaining: 109ms\n","906:\tlearn: 0.0209250\ttotal: 1.05s\tremaining: 108ms\n","907:\tlearn: 0.0209023\ttotal: 1.05s\tremaining: 107ms\n","908:\tlearn: 0.0208721\ttotal: 1.05s\tremaining: 106ms\n","909:\tlearn: 0.0208463\ttotal: 1.06s\tremaining: 104ms\n","910:\tlearn: 0.0208146\ttotal: 1.06s\tremaining: 103ms\n","911:\tlearn: 0.0207883\ttotal: 1.06s\tremaining: 102ms\n","912:\tlearn: 0.0207509\ttotal: 1.06s\tremaining: 101ms\n","913:\tlearn: 0.0207166\ttotal: 1.06s\tremaining: 99.8ms\n","914:\tlearn: 0.0206841\ttotal: 1.06s\tremaining: 98.7ms\n","915:\tlearn: 0.0206582\ttotal: 1.06s\tremaining: 97.6ms\n","916:\tlearn: 0.0206204\ttotal: 1.06s\tremaining: 96.4ms\n","917:\tlearn: 0.0205822\ttotal: 1.07s\tremaining: 95.3ms\n","918:\tlearn: 0.0205504\ttotal: 1.07s\tremaining: 94.1ms\n","919:\tlearn: 0.0205158\ttotal: 1.07s\tremaining: 93ms\n","920:\tlearn: 0.0204843\ttotal: 1.07s\tremaining: 91.9ms\n","921:\tlearn: 0.0204524\ttotal: 1.07s\tremaining: 90.7ms\n","922:\tlearn: 0.0204167\ttotal: 1.07s\tremaining: 89.6ms\n","923:\tlearn: 0.0203821\ttotal: 1.07s\tremaining: 88.4ms\n","924:\tlearn: 0.0203421\ttotal: 1.08s\tremaining: 87.3ms\n","925:\tlearn: 0.0203185\ttotal: 1.08s\tremaining: 86.1ms\n","926:\tlearn: 0.0202962\ttotal: 1.08s\tremaining: 84.9ms\n","927:\tlearn: 0.0202678\ttotal: 1.08s\tremaining: 83.7ms\n","928:\tlearn: 0.0202328\ttotal: 1.08s\tremaining: 82.6ms\n","929:\tlearn: 0.0201937\ttotal: 1.08s\tremaining: 81.4ms\n","930:\tlearn: 0.0201567\ttotal: 1.08s\tremaining: 80.3ms\n","931:\tlearn: 0.0201302\ttotal: 1.08s\tremaining: 79.1ms\n","932:\tlearn: 0.0201020\ttotal: 1.08s\tremaining: 78ms\n","933:\tlearn: 0.0200551\ttotal: 1.09s\tremaining: 76.8ms\n","934:\tlearn: 0.0199975\ttotal: 1.09s\tremaining: 75.6ms\n","935:\tlearn: 0.0199658\ttotal: 1.09s\tremaining: 74.4ms\n","936:\tlearn: 0.0199313\ttotal: 1.09s\tremaining: 73.3ms\n","937:\tlearn: 0.0198938\ttotal: 1.09s\tremaining: 72.1ms\n","938:\tlearn: 0.0198630\ttotal: 1.09s\tremaining: 71ms\n","939:\tlearn: 0.0198244\ttotal: 1.09s\tremaining: 69.8ms\n","940:\tlearn: 0.0197940\ttotal: 1.09s\tremaining: 68.7ms\n","941:\tlearn: 0.0197643\ttotal: 1.1s\tremaining: 67.5ms\n","942:\tlearn: 0.0197432\ttotal: 1.1s\tremaining: 66.4ms\n","943:\tlearn: 0.0197104\ttotal: 1.1s\tremaining: 65.2ms\n","944:\tlearn: 0.0196683\ttotal: 1.1s\tremaining: 64.1ms\n","945:\tlearn: 0.0196408\ttotal: 1.1s\tremaining: 62.9ms\n","946:\tlearn: 0.0196122\ttotal: 1.1s\tremaining: 61.7ms\n","947:\tlearn: 0.0195783\ttotal: 1.1s\tremaining: 60.6ms\n","948:\tlearn: 0.0195402\ttotal: 1.11s\tremaining: 59.4ms\n","949:\tlearn: 0.0195070\ttotal: 1.11s\tremaining: 58.3ms\n","950:\tlearn: 0.0194790\ttotal: 1.11s\tremaining: 57.1ms\n","951:\tlearn: 0.0194492\ttotal: 1.11s\tremaining: 55.9ms\n","952:\tlearn: 0.0194100\ttotal: 1.11s\tremaining: 54.8ms\n","953:\tlearn: 0.0193672\ttotal: 1.11s\tremaining: 53.7ms\n","954:\tlearn: 0.0193269\ttotal: 1.11s\tremaining: 52.6ms\n","955:\tlearn: 0.0192915\ttotal: 1.12s\tremaining: 51.4ms\n","956:\tlearn: 0.0192338\ttotal: 1.12s\tremaining: 50.2ms\n","957:\tlearn: 0.0191917\ttotal: 1.12s\tremaining: 49ms\n","958:\tlearn: 0.0191607\ttotal: 1.12s\tremaining: 47.9ms\n","959:\tlearn: 0.0191313\ttotal: 1.12s\tremaining: 46.7ms\n","960:\tlearn: 0.0191037\ttotal: 1.12s\tremaining: 45.6ms\n","961:\tlearn: 0.0190772\ttotal: 1.12s\tremaining: 44.4ms\n","962:\tlearn: 0.0190485\ttotal: 1.13s\tremaining: 43.2ms\n","963:\tlearn: 0.0190078\ttotal: 1.13s\tremaining: 42.1ms\n","964:\tlearn: 0.0189791\ttotal: 1.13s\tremaining: 40.9ms\n","965:\tlearn: 0.0189578\ttotal: 1.13s\tremaining: 39.7ms\n","966:\tlearn: 0.0189295\ttotal: 1.13s\tremaining: 38.6ms\n","967:\tlearn: 0.0189051\ttotal: 1.13s\tremaining: 37.4ms\n","968:\tlearn: 0.0188794\ttotal: 1.13s\tremaining: 36.2ms\n","969:\tlearn: 0.0188352\ttotal: 1.13s\tremaining: 35.1ms\n","970:\tlearn: 0.0188110\ttotal: 1.14s\tremaining: 33.9ms\n","971:\tlearn: 0.0187865\ttotal: 1.14s\tremaining: 32.7ms\n","972:\tlearn: 0.0187563\ttotal: 1.14s\tremaining: 31.6ms\n","973:\tlearn: 0.0187214\ttotal: 1.14s\tremaining: 30.4ms\n","974:\tlearn: 0.0187067\ttotal: 1.14s\tremaining: 29.2ms\n","975:\tlearn: 0.0186787\ttotal: 1.14s\tremaining: 28.1ms\n","976:\tlearn: 0.0186480\ttotal: 1.14s\tremaining: 26.9ms\n","977:\tlearn: 0.0186211\ttotal: 1.14s\tremaining: 25.8ms\n","978:\tlearn: 0.0185889\ttotal: 1.15s\tremaining: 24.6ms\n","979:\tlearn: 0.0185548\ttotal: 1.15s\tremaining: 23.4ms\n","980:\tlearn: 0.0185280\ttotal: 1.15s\tremaining: 22.2ms\n","981:\tlearn: 0.0185081\ttotal: 1.15s\tremaining: 21.1ms\n","982:\tlearn: 0.0184731\ttotal: 1.15s\tremaining: 19.9ms\n","983:\tlearn: 0.0184327\ttotal: 1.15s\tremaining: 18.8ms\n","984:\tlearn: 0.0184109\ttotal: 1.15s\tremaining: 17.6ms\n","985:\tlearn: 0.0183822\ttotal: 1.16s\tremaining: 16.4ms\n","986:\tlearn: 0.0183564\ttotal: 1.16s\tremaining: 15.3ms\n","987:\tlearn: 0.0183253\ttotal: 1.16s\tremaining: 14.1ms\n","988:\tlearn: 0.0182984\ttotal: 1.16s\tremaining: 12.9ms\n","989:\tlearn: 0.0182728\ttotal: 1.16s\tremaining: 11.7ms\n","990:\tlearn: 0.0182483\ttotal: 1.16s\tremaining: 10.6ms\n","991:\tlearn: 0.0182214\ttotal: 1.17s\tremaining: 9.4ms\n","992:\tlearn: 0.0181944\ttotal: 1.17s\tremaining: 8.22ms\n","993:\tlearn: 0.0181695\ttotal: 1.17s\tremaining: 7.05ms\n","994:\tlearn: 0.0181450\ttotal: 1.17s\tremaining: 5.88ms\n","995:\tlearn: 0.0181201\ttotal: 1.17s\tremaining: 4.7ms\n","996:\tlearn: 0.0180952\ttotal: 1.17s\tremaining: 3.53ms\n","997:\tlearn: 0.0180742\ttotal: 1.17s\tremaining: 2.35ms\n","998:\tlearn: 0.0180410\ttotal: 1.18s\tremaining: 1.18ms\n","999:\tlearn: 0.0180070\ttotal: 1.18s\tremaining: 0us\n"]}],"source":["# Initializing Classifiers\n","clf1 = XGBClassifier()\n","clf2 = GradientBoostingClassifier()\n","clf3 = CatBoostClassifier()\n","clf4 = LGBMClassifier()\n","eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3, clf4], weights=[2, 1, 1, 1], voting='soft')\n","\n","eclf.fit(X_train, y_train)\n","\n","gc.collect()\n","\n","oof_prob = eclf.predict_proba(X_valid)[:, 1]\n","\n","\n","def find_best_threshold(y_valid, oof_prob):\n","    best_auc = 0\n","\n","    for th in tqdm([i / 100 for i in range(30, 70)]):\n","        oof_prob_copy = oof_prob.copy()\n","        oof_prob_copy[oof_prob_copy >= th] = 1\n","        oof_prob_copy[oof_prob_copy < th] = 0\n","\n","        auc = sklearn.metrics.roc_auc_score(y_valid, oof_prob)\n","\n","        if auc > best_auc:\n","            best_th = th\n","            best_auc = auc\n","\n","        gc.collect()\n","\n","    return best_th, best_auc\n","\n","\n","best_th, best_auc = find_best_threshold(y_valid, oof_prob)\n","print(best_th, best_auc)\n","\n","eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[2, 1, 1], voting='soft')\n","eclf1.fit(X, y)\n","y_pre = eclf.predict_proba(X_test)[:, 1]\n","# test=pd.read_csv(data_path_3+'x_test_B.csv')\n","# res = test[['id']]\n","# res['y'] = y_pre\n","# res.loc[res['y'] >= best_th, 'y'] = 1\n","# res.loc[res['y'] < best_th, 'y'] = 0\n","# res.to_csv('0214_bagging_lgb.csv',index = False)\n","# print('saved')"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"5lB9xHiO-PCy","executionInfo":{"status":"ok","timestamp":1657193949003,"user_tz":-480,"elapsed":11874,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"cdc2f918-82c8-4fbd-e16b-811a01557124"}},{"cell_type":"markdown","source":[""],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"QzKsCfrN-PCy"}},{"cell_type":"code","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["saved\n"]}],"source":["high = pd.DataFrame()\n","res = Y_test[['用户编码']]\n","res['y'] = y_pre\n","for row in res.iterrows():\n","    user_id = row[1][0]\n","    p = row[1][1]\n","    if p > best_th:\n","        high = high.append({'用户编码': user_id, '高价值概率': p}, ignore_index=True)\n","\n","high.sort_values(\"高价值概率\", ascending=False, inplace=True)\n","high_5 = high[:5].reset_index(drop=True)\n","high_5\n","saved(high_5, 'Task3.csv')"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0IH31hL-PCy","executionInfo":{"status":"ok","timestamp":1657193949681,"user_tz":-480,"elapsed":681,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"bbf876fd-f28f-47b4-e87a-42ae5c9be850"}},{"cell_type":"code","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          用户编码 是否为高价值\n","5   1000000006      1\n","34  1000000035      1\n","16  1000000017      1\n","4   1000000005      1\n","60  1000000061      0\n","69  1000000070      0\n","49  1000000050      0\n","98  1000000099      0\n","33  1000000034      1\n","93  1000000094      0\n","36  1000000037      0\n","10  1000000011      1\n","20  1000000021      1\n","30  1000000031      0\n","32  1000000033      0\n","76  1000000077      1\n","99  1000000100      1\n","22  1000000023      0\n","52  1000000053      0\n","3   1000000004      0"],"text/html":["\n","  <div id=\"df-29c2a3b6-b71f-49d0-8e48-77a4a11a6f43\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>用户编码</th>\n","      <th>是否为高价值</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>1000000006</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>1000000035</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1000000017</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000000005</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>1000000061</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>1000000070</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>1000000050</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>1000000099</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>1000000034</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>93</th>\n","      <td>1000000094</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>1000000037</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1000000011</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>1000000021</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>1000000031</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>1000000033</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>1000000077</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>1000000100</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1000000023</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>1000000053</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000000004</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29c2a3b6-b71f-49d0-8e48-77a4a11a6f43')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-29c2a3b6-b71f-49d0-8e48-77a4a11a6f43 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-29c2a3b6-b71f-49d0-8e48-77a4a11a6f43');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}],"source":["Y_test"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":676},"id":"54R8sshS-PCy","executionInfo":{"status":"ok","timestamp":1657193949682,"user_tz":-480,"elapsed":14,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"20523e15-988f-455a-a8a2-7f64ffd8dcb2"}},{"cell_type":"code","execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":15}],"source":["high.shape[0]"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"reNjFJn2-PCy","executionInfo":{"status":"ok","timestamp":1657193949682,"user_tz":-480,"elapsed":13,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"4243e441-7064-4df2-f12d-67097dc8ac17"}},{"cell_type":"code","execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.97655277, 0.9876536 , 0.98768685, 0.97579264, 0.00876287,\n","       0.01206484, 0.00874365, 0.67714949, 0.78416082, 0.00779979,\n","       0.0081564 , 0.98909549, 0.9876035 , 0.00849193, 0.00672442,\n","       0.78416082, 0.97660674, 0.00924135, 0.00984169, 0.01140367])"]},"metadata":{},"execution_count":16}],"source":["y_pre"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"6OPfNMXt-PCy","executionInfo":{"status":"ok","timestamp":1657193949683,"user_tz":-480,"elapsed":12,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"fceed075-4432-48a1-ada2-b80e18aa3f3a"}},{"cell_type":"code","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9\n"]}],"source":["# y_pre[y_pre >= best_th] = 1\n","# y_pre[y_pre < best_th] = 0\n","recall = sklearn.metrics.recall_score(y_test, np.round(y_pre))\n","precision = sklearn.metrics.precision_score(y_test, np.round(y_pre))\n","f2 = 5 * recall * precision / (4 * precision + recall)\n","print(precision)"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"JEgCEm9i-PCy","executionInfo":{"status":"ok","timestamp":1657193949683,"user_tz":-480,"elapsed":10,"user":{"displayName":"陈成","userId":"07346209167866063288"}},"outputId":"773c6b34-1e14-4451-dc78-e3868e5de329"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"power.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}